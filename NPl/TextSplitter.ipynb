{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7a3ae290",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1'}, page_content='Attention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser ∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring signiﬁcantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature.\\n1 Introduction\\nRecurrent neural networks, long short-term memory [12] and gated recurrent [7] neural networks\\nin particular, have been ﬁrmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 29, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [31, 21, 13].\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the ﬁrst Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefﬁcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2'}, page_content='Recurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsigniﬁcant improvements in computational efﬁciency through factorization tricks [18] and conditional\\ncomputation [26], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 16]. In all but a few cases [22], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difﬁcult to learn dependencies between distant positions [ 11]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 22, 23, 19].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [28].\\nTo the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [14, 15] and [8].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 29].\\nHere, the encoder maps an input sequence of symbol representations (x1,...,x n) to a sequence\\nof continuous representations z = (z1,...,z n). Given z, the decoder then generates an output\\nsequence (y1,...,y m) of symbols one element at a time. At each step the model is auto-regressive\\n[9], consuming the previously generated symbols as additional input when generating the next.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The ﬁrst is a multi-head self-attention mechanism, and the second is a simple, position-\\n2'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3'}, page_content='Figure 1: The Transformer - model architecture.\\nwise fully connected feed-forward network. We employ a residual connection [10] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm(x+ Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position ican depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\n3'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4'}, page_content='Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices Kand V. We compute\\nthe matrix of outputs as:\\nAttention(Q,K,V ) = softmax(QKT\\n√dk\\n)V (1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof 1√dk\\n. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efﬁcient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by 1√dk\\n.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneﬁcial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\noutput values. These are concatenated and once again projected, resulting in the ﬁnal values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = ∑dk\\ni=1 qiki, has mean 0 and variance dk.\\n4'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5'}, page_content='MultiHead(Q,K,V ) = Concat(head1,..., headh)WO\\nwhere headi = Attention(QWQ\\ni ,KW K\\ni ,VW V\\ni )\\nWhere the projections are parameter matricesWQ\\ni ∈Rdmodel×dk , WK\\ni ∈Rdmodel×dk , WV\\ni ∈Rdmodel×dv\\nand WO ∈Rhdv×dmodel .\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h= 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[31, 2, 8].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation ﬂow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0,xW1 + b1)W2 + b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [24]. In the embedding layers, we multiply those weights by √dmodel.\\n3.5 Positional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\n5'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6'}, page_content='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2 ·d) O(1) O(1)\\nRecurrent O(n·d2) O(n) O(n)\\nConvolutional O(k·n·d2) O(1) O(logk(n))\\nSelf-Attention (restricted) O(r·n·d) O(1) O(n/r)\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and ﬁxed [8].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i) = sin(pos/100002i/dmodel )\\nPE(pos,2i+1) = cos(pos/100002i/dmodel )\\nwhere posis the position and iis the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2πto 10000 ·2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any ﬁxed offset k, PEpos+k can be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [8] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1,...,x n) to another sequence of equal length (z1,...,z n), with xi,zi ∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [11]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\nlength n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[31] and byte-pair [25] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\\n6'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7'}, page_content='the input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k<n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [ 15], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\nconsiderably, to O(k·n·d+ n·d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side beneﬁt, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the signiﬁcantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [31]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2 Hardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3 Optimizer\\nWe used the Adam optimizer [17] with β1 = 0.9, β2 = 0.98 and ϵ= 10−9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate= d−0.5\\nmodel ·min(step_num−0.5,step_num·warmup_steps−1.5) (3)\\nThis corresponds to increasing the learning rate linearly for the ﬁrst warmup_stepstraining steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps= 4000.\\n5.4 Regularization\\nWe employ three types of regularization during training:\\nResidual Dropout We apply dropout [27] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\n7'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8'}, page_content='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU Training Cost (FLOPs)\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet [15] 23.75\\nDeep-Att + PosUnk [32] 39.2 1.0 ·1020\\nGNMT + RL [31] 24.6 39.92 2.3 ·1019 1.4 ·1020\\nConvS2S [8] 25.16 40.46 9.6 ·1018 1.5 ·1020\\nMoE [26] 26.03 40.56 2.0 ·1019 1.2 ·1020\\nDeep-Att + PosUnk Ensemble [32] 40.4 8.0 ·1020\\nGNMT + RL Ensemble [31] 26.30 41.16 1.8 ·1020 1.1 ·1021\\nConvS2S Ensemble [8] 26.36 41.29 7.7 ·1019 1.2 ·1021\\nTransformer (base model) 27.3 38.1 3.3 · 1018\\nTransformer (big) 28.4 41.0 2.3 ·1019\\nLabel Smoothing During training, we employed label smoothing of value ϵls = 0.1 [30]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6 Results\\n6.1 Machine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The conﬁguration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty α= 0.6 [31]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [31].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of ﬂoating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision ﬂoating-point capacity of each GPU 5.\\n6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9'}, page_content='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d model dff h d k dv Pdrop ϵls\\ntrain PPL BLEU params\\nsteps (dev) (dev) ×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)\\n1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B) 16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)\\n2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)\\n0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-ﬁtting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [8], and observe nearly identical\\nresults to the base model.\\n7 Conclusion\\nIn this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\n9'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10'}, page_content='References\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[9] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850, 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770–778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient ﬂow in\\nrecurrent nets: the difﬁculty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735–1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[14] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time.arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[20] Samy Bengio Łukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n10'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 10, 'page_label': '11'}, page_content='[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n[22] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[24] Oﬁr Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n11')]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "doc=PyPDFLoader(r\"D:\\btch 16\\NPl\\NIPS-2017-attention-is-all-you-need-Paper.pdf\")\n",
    "fin_doc=doc.load()\n",
    "fin_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fe232821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1'}, page_content='Attention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1'}, page_content='Ashish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1'}, page_content='Google Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1'}, page_content='Noam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗†'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1'}, page_content='noam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗†\\nUniversity of Toronto'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1'}, page_content='Niki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.edu'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1'}, page_content='nikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser ∗\\nGoogle Brain'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1'}, page_content='Google Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser ∗\\nGoogle Brain\\nlukaszkaiser@google.com'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1'}, page_content='usz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser ∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1'}, page_content='Google Research\\nllion@google.com\\nAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser ∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1'}, page_content='llion@google.com\\nAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser ∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1'}, page_content='Łukasz Kaiser ∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1'}, page_content='illia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1'}, page_content='convolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1'}, page_content='performing models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1'}, page_content='mechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1'}, page_content='based solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1'}, page_content='entirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring signiﬁcantly'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1'}, page_content='be superior in quality while being more parallelizable and requiring signiﬁcantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1'}, page_content='less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1'}, page_content='to-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1'}, page_content='ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1'}, page_content='our model establishes a new single-model state-of-the-art BLEU score of 41.0 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1'}, page_content='training for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature.\\n1 Introduction'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1'}, page_content='best models from the literature.\\n1 Introduction\\nRecurrent neural networks, long short-term memory [12] and gated recurrent [7] neural networks'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1'}, page_content='Recurrent neural networks, long short-term memory [12] and gated recurrent [7] neural networks\\nin particular, have been ﬁrmly established as state of the art approaches in sequence modeling and'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1'}, page_content='in particular, have been ﬁrmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 29, 2, 5]. Numerous'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1'}, page_content='transduction problems such as language modeling and machine translation [ 29, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1'}, page_content='efforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [31, 21, 13].'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1'}, page_content='architectures [31, 21, 13].\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1'}, page_content='the effort to evaluate this idea. Ashish, with Illia, designed and implemented the ﬁrst Transformer models and'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1'}, page_content='has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1'}, page_content='attention and the parameter-free position representation and became the other person involved in nearly every'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1'}, page_content='detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1'}, page_content='tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1'}, page_content='efﬁcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1'}, page_content='implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1'}, page_content='our research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2'}, page_content='Recurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2'}, page_content='states ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2'}, page_content='sequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2'}, page_content='sequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsigniﬁcant improvements in computational efﬁciency through factorization tricks [18] and conditional'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2'}, page_content='signiﬁcant improvements in computational efﬁciency through factorization tricks [18] and conditional\\ncomputation [26], while also improving model performance in case of the latter. The fundamental'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2'}, page_content='computation [26], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2'}, page_content='constraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2'}, page_content='Attention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2'}, page_content='the input or output sequences [2, 16]. In all but a few cases [22], however, such attention mechanisms\\nare used in conjunction with a recurrent network.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2'}, page_content='are used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2'}, page_content='In this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2'}, page_content='relying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2'}, page_content='The Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2'}, page_content='translation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2'}, page_content='The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2'}, page_content='block, computing hidden representations in parallel for all input and output positions. In these models,'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2'}, page_content='the number of operations required to relate signals from two arbitrary input or output positions grows'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2'}, page_content='in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difﬁcult to learn dependencies between distant positions [ 11]. In the Transformer this is'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2'}, page_content='it more difﬁcult to learn dependencies between distant positions [ 11]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2'}, page_content='reduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2'}, page_content='to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2'}, page_content='described in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2'}, page_content='of a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2'}, page_content='used successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 22, 23, 19].'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2'}, page_content='textual entailment and learning task-independent sentence representations [4, 22, 23, 19].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2'}, page_content='End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2'}, page_content='aligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [28].'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2'}, page_content='language modeling tasks [28].\\nTo the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2'}, page_content='To the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2'}, page_content='aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [14, 15] and [8].\\n3 Model Architecture'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2'}, page_content='self-attention and discuss its advantages over models such as [14, 15] and [8].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 29].'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2'}, page_content='Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 29].\\nHere, the encoder maps an input sequence of symbol representations (x1,...,x n) to a sequence'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2'}, page_content='Here, the encoder maps an input sequence of symbol representations (x1,...,x n) to a sequence\\nof continuous representations z = (z1,...,z n). Given z, the decoder then generates an output'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2'}, page_content='of continuous representations z = (z1,...,z n). Given z, the decoder then generates an output\\nsequence (y1,...,y m) of symbols one element at a time. At each step the model is auto-regressive'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2'}, page_content='sequence (y1,...,y m) of symbols one element at a time. At each step the model is auto-regressive\\n[9], consuming the previously generated symbols as additional input when generating the next.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2'}, page_content='[9], consuming the previously generated symbols as additional input when generating the next.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2'}, page_content='The Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2'}, page_content='connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2'}, page_content='respectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2'}, page_content='Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The ﬁrst is a multi-head self-attention mechanism, and the second is a simple, position-\\n2'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3'}, page_content='Figure 1: The Transformer - model architecture.\\nwise fully connected feed-forward network. We employ a residual connection [10] around each of'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3'}, page_content='wise fully connected feed-forward network. We employ a residual connection [10] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3'}, page_content='the two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm(x+ Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3'}, page_content='LayerNorm(x+ Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3'}, page_content='itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3'}, page_content='layers, produce outputs of dimension dmodel = 512.\\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3'}, page_content='Decoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3'}, page_content='attention over the output of the encoder stack. Similar to the encoder, we employ residual connections'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3'}, page_content='around each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3'}, page_content='sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3'}, page_content='masking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position ican depend only on the known outputs at positions less than i.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3'}, page_content='predictions for position ican depend only on the known outputs at positions less than i.\\n3.2 Attention'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3'}, page_content='3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3'}, page_content='where the query, keys, values, and output are all vectors. The output is computed as a weighted sum'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3'}, page_content='of the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3'}, page_content='query with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3'}, page_content='We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\n3'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4'}, page_content='Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4'}, page_content='attention layers running in parallel.\\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\\nvalues.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4'}, page_content='values.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4'}, page_content='In practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices Kand V. We compute'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4'}, page_content='into a matrix Q. The keys and values are also packed together into matrices Kand V. We compute\\nthe matrix of outputs as:\\nAttention(Q,K,V ) = softmax(QKT\\n√dk\\n)V (1)'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4'}, page_content='the matrix of outputs as:\\nAttention(Q,K,V ) = softmax(QKT\\n√dk\\n)V (1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4'}, page_content='plicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof 1√dk'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4'}, page_content='of 1√dk\\n. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4'}, page_content='much faster and more space-efﬁcient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4'}, page_content='matrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4'}, page_content='dot product attention without scaling for larger values of dk [3]. We suspect that for large values of'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4'}, page_content='dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by 1√dk\\n.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4'}, page_content='extremely small gradients 4. To counteract this effect, we scale the dot products by 1√dk\\n.\\n3.2.2 Multi-Head Attention'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4'}, page_content='.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4'}, page_content='we found it beneﬁcial to linearly project the queries, keys and values htimes with different, learned'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4'}, page_content='linear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4'}, page_content='queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\noutput values. These are concatenated and once again projected, resulting in the ﬁnal values, as'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4'}, page_content='output values. These are concatenated and once again projected, resulting in the ﬁnal values, as\\ndepicted in Figure 2.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4'}, page_content='depicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4'}, page_content='Multi-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4'}, page_content='subspaces at different positions. With a single attention head, averaging inhibits this.\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4'}, page_content='4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = ∑dk'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4'}, page_content='variables with mean 0 and variance 1. Then their dot product, q · k = ∑dk\\ni=1 qiki, has mean 0 and variance dk.\\n4'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5'}, page_content='MultiHead(Q,K,V ) = Concat(head1,..., headh)WO\\nwhere headi = Attention(QWQ\\ni ,KW K\\ni ,VW V\\ni )\\nWhere the projections are parameter matricesWQ\\ni ∈Rdmodel×dk , WK\\ni ∈Rdmodel×dk , WV\\ni ∈Rdmodel×dv'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5'}, page_content='where headi = Attention(QWQ\\ni ,KW K\\ni ,VW V\\ni )\\nWhere the projections are parameter matricesWQ\\ni ∈Rdmodel×dk , WK\\ni ∈Rdmodel×dk , WV\\ni ∈Rdmodel×dv\\nand WO ∈Rhdv×dmodel .'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5'}, page_content='i ∈Rdmodel×dk , WK\\ni ∈Rdmodel×dk , WV\\ni ∈Rdmodel×dv\\nand WO ∈Rhdv×dmodel .\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5'}, page_content='In this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h= 64. Due to the reduced dimension of each head, the total computational cost'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5'}, page_content='dk = dv = dmodel/h= 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5'}, page_content='is similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5'}, page_content='The Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5'}, page_content='• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5'}, page_content='and the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5'}, page_content='position in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[31, 2, 8].'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5'}, page_content='typical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[31, 2, 8].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5'}, page_content='[31, 2, 8].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5'}, page_content='and queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5'}, page_content='encoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5'}, page_content='all positions in the decoder up to and including that position. We need to prevent leftward\\ninformation ﬂow in the decoder to preserve the auto-regressive property. We implement this'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5'}, page_content='information ﬂow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5'}, page_content='inside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5'}, page_content='of the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5'}, page_content='3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5'}, page_content='In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5'}, page_content='connected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5'}, page_content='consists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0,xW1 + b1)W2 + b2 (2)'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5'}, page_content='FFN(x) = max(0,xW1 + b1)W2 + b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5'}, page_content='While the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5'}, page_content='from layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5'}, page_content='The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4 Embeddings and Softmax'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5'}, page_content='dff = 2048.\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5'}, page_content='Similarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5'}, page_content='mation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5'}, page_content='our model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [24]. In the embedding layers, we multiply those weights by √dmodel.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5'}, page_content='linear transformation, similar to [24]. In the embedding layers, we multiply those weights by √dmodel.\\n3.5 Positional Encoding'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5'}, page_content='3.5 Positional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5'}, page_content='order of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5'}, page_content='tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\n5'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6'}, page_content='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6'}, page_content='for different layer types. nis the sequence length, dis the representation dimension, kis the kernel\\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6'}, page_content='size of convolutions and rthe size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2 ·d) O(1) O(1)'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6'}, page_content='Layer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2 ·d) O(1) O(1)\\nRecurrent O(n·d2) O(n) O(n)\\nConvolutional O(k·n·d2) O(1) O(logk(n))'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6'}, page_content='Operations\\nSelf-Attention O(n2 ·d) O(1) O(1)\\nRecurrent O(n·d2) O(n) O(n)\\nConvolutional O(k·n·d2) O(1) O(logk(n))\\nSelf-Attention (restricted) O(r·n·d) O(1) O(n/r)'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6'}, page_content='Convolutional O(k·n·d2) O(1) O(logk(n))\\nSelf-Attention (restricted) O(r·n·d) O(1) O(n/r)\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6'}, page_content='bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6'}, page_content='as the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and ﬁxed [8].\\nIn this work, we use sine and cosine functions of different frequencies:'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6'}, page_content='learned and ﬁxed [8].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i) = sin(pos/100002i/dmodel )\\nPE(pos,2i+1) = cos(pos/100002i/dmodel )'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6'}, page_content='PE(pos,2i) = sin(pos/100002i/dmodel )\\nPE(pos,2i+1) = cos(pos/100002i/dmodel )\\nwhere posis the position and iis the dimension. That is, each dimension of the positional encoding'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6'}, page_content='where posis the position and iis the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2πto 10000 ·2π. We'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6'}, page_content='corresponds to a sinusoid. The wavelengths form a geometric progression from 2πto 10000 ·2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6'}, page_content='chose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any ﬁxed offset k, PEpos+k can be represented as a linear function of'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6'}, page_content='relative positions, since for any ﬁxed offset k, PEpos+k can be represented as a linear function of\\nPEpos.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6'}, page_content='PEpos.\\nWe also experimented with using learned positional embeddings [8] instead, and found that the two'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6'}, page_content='We also experimented with using learned positional embeddings [8] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6'}, page_content='versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6'}, page_content='because it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6'}, page_content='during training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6'}, page_content='In this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6'}, page_content='tional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1,...,x n) to another sequence of equal length (z1,...,z n), with xi,zi ∈Rd, such as a hidden'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6'}, page_content='(x1,...,x n) to another sequence of equal length (z1,...,z n), with xi,zi ∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6'}, page_content='layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6'}, page_content='consider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6'}, page_content='One is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6'}, page_content='be parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6'}, page_content='The third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6'}, page_content='dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6'}, page_content='ability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6'}, page_content='traverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [11]. Hence we also compare'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6'}, page_content='and output sequences, the easier it is to learn long-range dependencies [11]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6'}, page_content='the maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6'}, page_content='different layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6'}, page_content='executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6'}, page_content='computational complexity, self-attention layers are faster than recurrent layers when the sequence\\nlength n is smaller than the representation dimensionality d, which is most often the case with'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6'}, page_content='length n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6'}, page_content='sentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[31] and byte-pair [25] representations. To improve computational performance for tasks involving'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6'}, page_content='very long sequences, self-attention could be restricted to considering only a neighborhood of size rin\\n6'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7'}, page_content='the input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7'}, page_content='path length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k<n does not connect all pairs of input and output'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7'}, page_content='positions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7'}, page_content='or O(logk(n)) in the case of dilated convolutions [ 15], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7'}, page_content='between any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7'}, page_content='recurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\nconsiderably, to O(k·n·d+ n·d2). Even with k = n, however, the complexity of a separable'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7'}, page_content='considerably, to O(k·n·d+ n·d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7'}, page_content='convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7'}, page_content='the approach we take in our model.\\nAs side beneﬁt, self-attention could yield more interpretable models. We inspect attention distributions'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7'}, page_content='from our models and present and discuss examples in the appendix. Not only do individual attention'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7'}, page_content='heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7'}, page_content='and semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7'}, page_content='5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7'}, page_content='We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7'}, page_content='sentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the signiﬁcantly larger WMT'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7'}, page_content='target vocabulary of about 37000 tokens. For English-French, we used the signiﬁcantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7'}, page_content='2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [31]. Sentence pairs were batched together by approximate sequence length. Each training'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7'}, page_content='vocabulary [31]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7'}, page_content='batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2 Hardware and Schedule'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7'}, page_content='target tokens.\\n5.2 Hardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7'}, page_content='We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7'}, page_content='trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7'}, page_content='bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3 Optimizer'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7'}, page_content='(3.5 days).\\n5.3 Optimizer\\nWe used the Adam optimizer [17] with β1 = 0.9, β2 = 0.98 and ϵ= 10−9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate= d−0.5'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7'}, page_content='rate over the course of training, according to the formula:\\nlrate= d−0.5\\nmodel ·min(step_num−0.5,step_num·warmup_steps−1.5) (3)'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7'}, page_content='lrate= d−0.5\\nmodel ·min(step_num−0.5,step_num·warmup_steps−1.5) (3)\\nThis corresponds to increasing the learning rate linearly for the ﬁrst warmup_stepstraining steps,'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7'}, page_content='This corresponds to increasing the learning rate linearly for the ﬁrst warmup_stepstraining steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7'}, page_content='and decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps= 4000.\\n5.4 Regularization\\nWe employ three types of regularization during training:'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7'}, page_content='warmup_steps= 4000.\\n5.4 Regularization\\nWe employ three types of regularization during training:\\nResidual Dropout We apply dropout [27] to the output of each sub-layer, before it is added to the'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7'}, page_content='Residual Dropout We apply dropout [27] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7'}, page_content='sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7'}, page_content='positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\n7'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8'}, page_content='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8'}, page_content='English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU Training Cost (FLOPs)\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet [15] 23.75'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8'}, page_content='Model\\nBLEU Training Cost (FLOPs)\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet [15] 23.75\\nDeep-Att + PosUnk [32] 39.2 1.0 ·1020\\nGNMT + RL [31] 24.6 39.92 2.3 ·1019 1.4 ·1020'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8'}, page_content='BLEU Training Cost (FLOPs)\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet [15] 23.75\\nDeep-Att + PosUnk [32] 39.2 1.0 ·1020\\nGNMT + RL [31] 24.6 39.92 2.3 ·1019 1.4 ·1020\\nConvS2S [8] 25.16 40.46 9.6 ·1018 1.5 ·1020'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8'}, page_content='ByteNet [15] 23.75\\nDeep-Att + PosUnk [32] 39.2 1.0 ·1020\\nGNMT + RL [31] 24.6 39.92 2.3 ·1019 1.4 ·1020\\nConvS2S [8] 25.16 40.46 9.6 ·1018 1.5 ·1020\\nMoE [26] 26.03 40.56 2.0 ·1019 1.2 ·1020'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8'}, page_content='GNMT + RL [31] 24.6 39.92 2.3 ·1019 1.4 ·1020\\nConvS2S [8] 25.16 40.46 9.6 ·1018 1.5 ·1020\\nMoE [26] 26.03 40.56 2.0 ·1019 1.2 ·1020\\nDeep-Att + PosUnk Ensemble [32] 40.4 8.0 ·1020'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8'}, page_content='ConvS2S [8] 25.16 40.46 9.6 ·1018 1.5 ·1020\\nMoE [26] 26.03 40.56 2.0 ·1019 1.2 ·1020\\nDeep-Att + PosUnk Ensemble [32] 40.4 8.0 ·1020\\nGNMT + RL Ensemble [31] 26.30 41.16 1.8 ·1020 1.1 ·1021'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8'}, page_content='MoE [26] 26.03 40.56 2.0 ·1019 1.2 ·1020\\nDeep-Att + PosUnk Ensemble [32] 40.4 8.0 ·1020\\nGNMT + RL Ensemble [31] 26.30 41.16 1.8 ·1020 1.1 ·1021\\nConvS2S Ensemble [8] 26.36 41.29 7.7 ·1019 1.2 ·1021'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8'}, page_content='GNMT + RL Ensemble [31] 26.30 41.16 1.8 ·1020 1.1 ·1021\\nConvS2S Ensemble [8] 26.36 41.29 7.7 ·1019 1.2 ·1021\\nTransformer (base model) 27.3 38.1 3.3 · 1018\\nTransformer (big) 28.4 41.0 2.3 ·1019'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8'}, page_content='Transformer (base model) 27.3 38.1 3.3 · 1018\\nTransformer (big) 28.4 41.0 2.3 ·1019\\nLabel Smoothing During training, we employed label smoothing of value ϵls = 0.1 [30]. This'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8'}, page_content='Label Smoothing During training, we employed label smoothing of value ϵls = 0.1 [30]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6 Results'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8'}, page_content='hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6 Results\\n6.1 Machine Translation'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8'}, page_content='6 Results\\n6.1 Machine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8'}, page_content='On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8'}, page_content='in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The conﬁguration of this model is'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8'}, page_content='BLEU, establishing a new state-of-the-art BLEU score of 28.4. The conﬁguration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8'}, page_content='listed in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8'}, page_content='surpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8'}, page_content='the competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8'}, page_content='On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8'}, page_content='outperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8'}, page_content='previous state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8'}, page_content='dropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8'}, page_content='For the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8'}, page_content='were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty α= 0.6 [31]. These hyperparameters'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8'}, page_content='used beam search with a beam size of 4 and length penalty α= 0.6 [31]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8'}, page_content='were chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [31].'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8'}, page_content='inference to input length + 50, but terminate early when possible [31].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8'}, page_content='architectures from the literature. We estimate the number of ﬂoating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8'}, page_content='model by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision ﬂoating-point capacity of each GPU 5.\\n6.2 Model Variations'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8'}, page_content='single-precision ﬂoating-point capacity of each GPU 5.\\n6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8'}, page_content='To evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8'}, page_content='in different ways, measuring the change in performance on English-to-German translation on the\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8'}, page_content='development set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8'}, page_content='checkpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8'}, page_content='In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8'}, page_content='keeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8'}, page_content='attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9'}, page_content='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9'}, page_content='model. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9'}, page_content='perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d model dff h d k dv Pdrop ϵls\\ntrain PPL BLEU params\\nsteps (dev) (dev) ×106'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9'}, page_content='per-word perplexities.\\nN d model dff h d k dv Pdrop ϵls\\ntrain PPL BLEU params\\nsteps (dev) (dev) ×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)\\n1 512 512 5.29 24.9\\n4 128 128 5.00 25.5'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9'}, page_content='N d model dff h d k dv Pdrop ϵls\\ntrain PPL BLEU params\\nsteps (dev) (dev) ×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)\\n1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9'}, page_content='train PPL BLEU params\\nsteps (dev) (dev) ×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)\\n1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B) 16 5.16 25.1 58'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9'}, page_content='steps (dev) (dev) ×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)\\n1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B) 16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9'}, page_content='base 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)\\n1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B) 16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)\\n2 6.11 23.7 36'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9'}, page_content='(A)\\n1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B) 16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)\\n2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9'}, page_content='4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B) 16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)\\n2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9'}, page_content='16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B) 16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)\\n2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9'}, page_content='32 16 16 5.01 25.4\\n(B) 16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)\\n2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9'}, page_content='(B) 16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)\\n2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)\\n0.0 5.77 24.6'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9'}, page_content='32 5.01 25.4 60\\n(C)\\n2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)\\n0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9'}, page_content='(C)\\n2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)\\n0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9'}, page_content='1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)\\n0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9'}, page_content='1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)\\n0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9'}, page_content='(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9'}, page_content='In Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9'}, page_content='suggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected,'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9'}, page_content='bigger models are better, and dropout is very helpful in avoiding over-ﬁtting. In row (E) we replace our'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9'}, page_content='sinusoidal positional encoding with learned positional embeddings [8], and observe nearly identical\\nresults to the base model.\\n7 Conclusion'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9'}, page_content='results to the base model.\\n7 Conclusion\\nIn this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9'}, page_content='In this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9'}, page_content='attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9'}, page_content='multi-headed self-attention.\\nFor translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9'}, page_content='For translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9'}, page_content='on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9'}, page_content='English-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9'}, page_content='model outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9'}, page_content='plan to extend the Transformer to problems involving input and output modalities other than text and'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9'}, page_content='to investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9'}, page_content='such as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9'}, page_content='The code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9'}, page_content='tensorflow/tensor2tensor.\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\n9'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10'}, page_content='References\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10'}, page_content='arXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10'}, page_content='learning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10'}, page_content='[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10'}, page_content='machine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10'}, page_content='[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10'}, page_content='reading. arXiv preprint arXiv:1601.06733, 2016.\\n[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10'}, page_content='[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10'}, page_content='and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10'}, page_content='machine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10'}, page_content='preprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10'}, page_content='[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10'}, page_content='of gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10'}, page_content='[8] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10'}, page_content='tional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[9] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850, 2013.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10'}, page_content='[9] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850, 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10'}, page_content='arXiv:1308.0850, 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10'}, page_content='age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770–778, 2016.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10'}, page_content='Recognition, pages 770–778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient ﬂow in\\nrecurrent nets: the difﬁculty of learning long-term dependencies, 2001.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10'}, page_content='recurrent nets: the difﬁculty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735–1780, 1997.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10'}, page_content='9(8):1735–1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10'}, page_content='the limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[14] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10'}, page_content='[14] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10'}, page_content='on Learning Representations (ICLR), 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10'}, page_content='[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time.arXiv preprint arXiv:1610.10099v2,\\n2017.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10'}, page_content='ray Kavukcuoglu. Neural machine translation in linear time.arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10'}, page_content='2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10'}, page_content='In International Conference on Learning Representations, 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10'}, page_content='[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10'}, page_content='[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10'}, page_content='arXiv:1703.10722, 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10'}, page_content='[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10'}, page_content='arXiv:1703.03130, 2017.\\n[20] Samy Bengio Łukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n10'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 10, 'page_label': '11'}, page_content='[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 10, 'page_label': '11'}, page_content='based neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n[22] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 10, 'page_label': '11'}, page_content='[22] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 10, 'page_label': '11'}, page_content='model. In Empirical Methods in Natural Language Processing, 2016.\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 10, 'page_label': '11'}, page_content='[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 10, 'page_label': '11'}, page_content='summarization. arXiv preprint arXiv:1705.04304, 2017.\\n[24] Oﬁr Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 10, 'page_label': '11'}, page_content='preprint arXiv:1608.05859, 2016.\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 10, 'page_label': '11'}, page_content='with subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 10, 'page_label': '11'}, page_content='[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 10, 'page_label': '11'}, page_content='and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 10, 'page_label': '11'}, page_content='layer. arXiv preprint arXiv:1701.06538, 2017.\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 10, 'page_label': '11'}, page_content='[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 10, 'page_label': '11'}, page_content='nov. Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 10, 'page_label': '11'}, page_content='Learning Research, 15(1):1929–1958, 2014.\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 10, 'page_label': '11'}, page_content='[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 10, 'page_label': '11'}, page_content='networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\\nInc., 2015.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 10, 'page_label': '11'}, page_content='Advances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 10, 'page_label': '11'}, page_content='Inc., 2015.\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 10, 'page_label': '11'}, page_content='networks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 10, 'page_label': '11'}, page_content='[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 10, 'page_label': '11'}, page_content='Rethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 10, 'page_label': '11'}, page_content='[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 10, 'page_label': '11'}, page_content='Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 10, 'page_label': '11'}, page_content='translation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': 'D:\\\\btch 16\\\\NPl\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 10, 'page_label': '11'}, page_content='arXiv:1609.08144, 2016.\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n11')]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=200)\n",
    "final_doc=text_splitter.split_documents(fin_doc)\n",
    "final_doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6a0ba2dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"[The Tragedie of Hamlet by William Shakespeare 1599]\\n\\n\\nActus Primus. Scoena Prima.\\n\\nEnter Barnardo and Francisco two Centinels.\\n\\n  Barnardo. Who's there?\\n  Fran. Nay answer me: Stand & vnfold\\nyour selfe\\n\\n   Bar. Long liue the King\\n\\n   Fran. Barnardo?\\n  Bar. He\\n\\n   Fran. You come most carefully vpon your houre\\n\\n   Bar. 'Tis now strook twelue, get thee to bed Francisco\\n\\n   Fran. For this releefe much thankes: 'Tis bitter cold,\\nAnd I am sicke at heart\\n\\n   Barn. Haue you had quiet Guard?\\n  Fran. Not a Mouse stirring\\n\\n   Barn. Well, goodnight. If you do meet Horatio and\\nMarcellus, the Riuals of my Watch, bid them make hast.\\nEnter Horatio and Marcellus.\\n\\n  Fran. I thinke I heare them. Stand: who's there?\\n  Hor. Friends to this ground\\n\\n   Mar. And Leige-men to the Dane\\n\\n   Fran. Giue you good night\\n\\n   Mar. O farwel honest Soldier, who hath relieu'd you?\\n  Fra. Barnardo ha's my place: giue you goodnight.\\n\\nExit Fran.\\n\\n  Mar. Holla Barnardo\\n\\n   Bar. Say, what is Horatio there?\\n  Hor. A peece of him\\n\\n   Bar. Welcome Horatio, welcome good Marcellus\\n\\n   Mar. What, ha's this thing appear'd againe to night\\n\\n   Bar. I haue seene nothing\\n\\n   Mar. Horatio saies, 'tis but our Fantasie,\\nAnd will not let beleefe take hold of him\\nTouching this dreaded sight, twice seene of vs,\\nTherefore I haue intreated him along\\nWith vs, to watch the minutes of this Night,\\nThat if againe this Apparition come,\\nHe may approue our eyes, and speake to it\\n\\n   Hor. Tush, tush, 'twill not appeare\\n\\n   Bar. Sit downe a-while,\\nAnd let vs once againe assaile your eares,\\nThat are so fortified against our Story,\\nWhat we two Nights haue seene\\n\\n   Hor. Well, sit we downe,\\nAnd let vs heare Barnardo speake of this\\n\\n   Barn. Last night of all,\\nWhen yond same Starre that's Westward from the Pole\\nHad made his course t' illume that part of Heauen\\nWhere now it burnes, Marcellus and my selfe,\\nThe Bell then beating one\\n\\n   Mar. Peace, breake thee of:\\nEnter the Ghost.\\n\\nLooke where it comes againe\\n\\n   Barn. In the same figure, like the King that's dead\\n\\n   Mar. Thou art a Scholler; speake to it Horatio\\n\\n   Barn. Lookes it not like the King? Marke it Horatio\\n\\n   Hora. Most like: It harrowes me with fear & wonder\\n  Barn. It would be spoke too\\n\\n   Mar. Question it Horatio\\n\\n   Hor. What art thou that vsurp'st this time of night,\\nTogether with that Faire and Warlike forme\\nIn which the Maiesty of buried Denmarke\\nDid sometimes march: By Heauen I charge thee speake\\n\\n   Mar. It is offended\\n\\n   Barn. See, it stalkes away\\n\\n   Hor. Stay: speake; speake: I Charge thee, speake.\\n\\nExit the Ghost.\\n\\n  Mar. 'Tis gone, and will not answer\\n\\n   Barn. How now Horatio? You tremble & look pale:\\nIs not this something more then Fantasie?\\nWhat thinke you on't?\\n  Hor. Before my God, I might not this beleeue\\nWithout the sensible and true auouch\\nOf mine owne eyes\\n\\n   Mar. Is it not like the King?\\n  Hor. As thou art to thy selfe,\\nSuch was the very Armour he had on,\\nWhen th' Ambitious Norwey combatted:\\nSo frown'd he once, when in an angry parle\\nHe smot the sledded Pollax on the Ice.\\n'Tis strange\\n\\n   Mar. Thus twice before, and iust at this dead houre,\\nWith Martiall stalke, hath he gone by our Watch\\n\\n   Hor. In what particular thought to work, I know not:\\nBut in the grosse and scope of my Opinion,\\nThis boades some strange erruption to our State\\n\\n   Mar. Good now sit downe, & tell me he that knowes\\nWhy this same strict and most obseruant Watch,\\nSo nightly toyles the subiect of the Land,\\nAnd why such dayly Cast of Brazon Cannon\\nAnd Forraigne Mart for Implements of warre:\\nWhy such impresse of Ship-wrights, whose sore Taske\\nDo's not diuide the Sunday from the weeke,\\nWhat might be toward, that this sweaty hast\\nDoth make the Night ioynt-Labourer with the day:\\nWho is't that can informe me?\\n  Hor. That can I,\\nAt least the whisper goes so: Our last King,\\nWhose Image euen but now appear'd to vs,\\nWas (as you know) by Fortinbras of Norway,\\n(Thereto prick'd on by a most emulate Pride)\\nDar'd to the Combate. In which, our Valiant Hamlet,\\n(For so this side of our knowne world esteem'd him)\\nDid slay this Fortinbras: who by a Seal'd Compact,\\nWell ratified by Law, and Heraldrie,\\nDid forfeite (with his life) all those his Lands\\nWhich he stood seiz'd on, to the Conqueror:\\nAgainst the which, a Moity competent\\nWas gaged by our King: which had return'd\\nTo the Inheritance of Fortinbras,\\nHad he bin Vanquisher, as by the same Cou'nant\\nAnd carriage of the Article designe,\\nHis fell to Hamlet. Now sir, young Fortinbras,\\nOf vnimproued Mettle, hot and full,\\nHath in the skirts of Norway, heere and there,\\nShark'd vp a List of Landlesse Resolutes,\\nFor Foode and Diet, to some Enterprize\\nThat hath a stomacke in't: which is no other\\n(And it doth well appeare vnto our State)\\nBut to recouer of vs by strong hand\\nAnd termes Compulsatiue, those foresaid Lands\\nSo by his Father lost: and this (I take it)\\nIs the maine Motiue of our Preparations,\\nThe Sourse of this our Watch, and the cheefe head\\nOf this post-hast, and Romage in the Land.\\nEnter Ghost againe.\\n\\nBut soft, behold: Loe, where it comes againe:\\nIle crosse it, though it blast me. Stay Illusion:\\nIf thou hast any sound, or vse of Voyce,\\nSpeake to me. If there be any good thing to be done,\\nThat may to thee do ease, and grace to me; speak to me.\\nIf thou art priuy to thy Countries Fate\\n(Which happily foreknowing may auoyd) Oh speake.\\nOr, if thou hast vp-hoorded in thy life\\nExtorted Treasure in the wombe of Earth,\\n(For which, they say, you Spirits oft walke in death)\\nSpeake of it. Stay, and speake. Stop it Marcellus\\n\\n   Mar. Shall I strike at it with my Partizan?\\n  Hor. Do, if it will not stand\\n\\n   Barn. 'Tis heere\\n\\n   Hor. 'Tis heere\\n\\n   Mar. 'Tis gone.\\n\\nExit Ghost.\\n\\nWe do it wrong, being so Maiesticall\\nTo offer it the shew of Violence,\\nFor it is as the Ayre, invulnerable,\\nAnd our vaine blowes, malicious Mockery\\n\\n   Barn. It was about to speake, when the Cocke crew\\n\\n   Hor. And then it started, like a guilty thing\\nVpon a fearfull Summons. I haue heard,\\nThe Cocke that is the Trumpet to the day,\\nDoth with his lofty and shrill-sounding Throate\\nAwake the God of Day: and at his warning,\\nWhether in Sea, or Fire, in Earth, or Ayre,\\nTh' extrauagant, and erring Spirit, hyes\\nTo his Confine. And of the truth heerein,\\nThis present Obiect made probation\\n\\n   Mar. It faded on the crowing of the Cocke.\\nSome sayes, that euer 'gainst that Season comes\\nWherein our Sauiours Birch is celebrated,\\nThe Bird of Dawning singeth all night long:\\nAnd then (they say) no Spirit can walke abroad,\\nThe nights are wholsome, then no Planets strike,\\nNo Faiery talkes, nor Witch hath power to Charme:\\nSo hallow'd, and so gracious is the time\\n\\n   Hor. So haue I heard, and do in part beleeue it.\\nBut looke, the Morne in Russet mantle clad,\\nWalkes o're the dew of yon high Easterne Hill,\\nBreake we our Watch vp, and by my aduice\\nLet vs impart what we haue seene to night\\nVnto yong Hamlet. For vpon my life,\\nThis Spirit dumbe to vs, will speake to him:\\nDo you consent we shall acquaint him with it,\\nAs needfull in our Loues, fitting our Duty?\\n  Mar. Let do't I pray, and I this morning know\\nWhere we shall finde him most conueniently.\\n\\nExeunt.\\n\\nScena Secunda.\\n\\nEnter Claudius King of Denmarke, Gertrude the Queene, Hamlet,\\nPolonius,\\nLaertes, and his Sister Ophelia, Lords Attendant.\\n\\n  King. Though yet of Hamlet our deere Brothers death\\nThe memory be greene: and that it vs befitted\\nTo beare our hearts in greefe, and our whole Kingdome\\nTo be contracted in one brow of woe:\\nYet so farre hath Discretion fought with Nature,\\nThat we with wisest sorrow thinke on him,\\nTogether with remembrance of our selues.\\nTherefore our sometimes Sister, now our Queene,\\nTh' imperiall Ioyntresse of this warlike State,\\nHaue we, as 'twere, with a defeated ioy,\\nWith one Auspicious, and one Dropping eye,\\nWith mirth in Funerall, and with Dirge in Marriage,\\nIn equall Scale weighing Delight and Dole\\nTaken to Wife; nor haue we heerein barr'd\\nYour better Wisedomes, which haue freely gone\\nWith this affaire along, for all our Thankes.\\nNow followes, that you know young Fortinbras,\\nHolding a weake supposall of our worth;\\nOr thinking by our late deere Brothers death,\\nOur State to be disioynt, and out of Frame,\\nColleagued with the dreame of his Aduantage;\\nHe hath not fayl'd to pester vs with Message,\\nImporting the surrender of those Lands\\nLost by his Father: with all Bonds of Law\\nTo our most valiant Brother. So much for him.\\nEnter Voltemand and Cornelius.\\n\\nNow for our selfe, and for this time of meeting\\nThus much the businesse is. We haue heere writ\\nTo Norway, Vncle of young Fortinbras,\\nWho Impotent and Bedrid, scarsely heares\\nOf this his Nephewes purpose, to suppresse\\nHis further gate heerein. In that the Leuies,\\nThe Lists, and full proportions are all made\\nOut of his subiect: and we heere dispatch\\nYou good Cornelius, and you Voltemand,\\nFor bearing of this greeting to old Norway,\\nGiuing to you no further personall power\\nTo businesse with the King, more then the scope\\nOf these dilated Articles allow:\\nFarewell, and let your hast commend your duty\\n\\n   Volt. In that, and all things, will we shew our duty\\n\\n   King. We doubt it nothing, heartily farewell.\\n\\nExit Voltemand and Cornelius.\\n\\nAnd now Laertes, what's the newes with you?\\nYou told vs of some suite. What is't Laertes?\\nYou cannot speake of Reason to the Dane,\\nAnd loose your voyce. What would'st thou beg Laertes,\\nThat shall not be my Offer, not thy Asking?\\nThe Head is not more Natiue to the Heart,\\nThe Hand more instrumentall to the Mouth,\\nThen is the Throne of Denmarke to thy Father.\\nWhat would'st thou haue Laertes?\\n  Laer. Dread my Lord,\\nYour leaue and fauour to returne to France,\\nFrom whence, though willingly I came to Denmarke\\nTo shew my duty in your Coronation,\\nYet now I must confesse, that duty done,\\nMy thoughts and wishes bend againe towards France,\\nAnd bow them to your gracious leaue and pardon\\n\\n   King. Haue you your Fathers leaue?\\nWhat sayes Pollonius?\\n  Pol. He hath my Lord:\\nI do beseech you giue him leaue to go\\n\\n   King. Take thy faire houre Laertes, time be thine,\\nAnd thy best graces spend it at thy will:\\nBut now my Cosin Hamlet, and my Sonne?\\n  Ham. A little more then kin, and lesse then kinde\\n\\n   King. How is it that the Clouds still hang on you?\\n  Ham. Not so my Lord, I am too much i'th' Sun\\n\\n   Queen. Good Hamlet cast thy nightly colour off,\\nAnd let thine eye looke like a Friend on Denmarke.\\nDo not for euer with thy veyled lids\\nSeeke for thy Noble Father in the dust;\\nThou know'st 'tis common, all that liues must dye,\\nPassing through Nature, to Eternity\\n\\n   Ham. I Madam, it is common\\n\\n   Queen. If it be;\\nWhy seemes it so particular with thee\\n\\n   Ham. Seemes Madam? Nay, it is: I know not Seemes:\\n'Tis not alone my Inky Cloake (good Mother)\\nNor Customary suites of solemne Blacke,\\nNor windy suspiration of forc'd breath,\\nNo, nor the fruitfull Riuer in the Eye,\\nNor the deiected hauiour of the Visage,\\nTogether with all Formes, Moods, shewes of Griefe,\\nThat can denote me truly. These indeed Seeme,\\nFor they are actions that a man might play:\\nBut I haue that Within, which passeth show;\\nThese, but the Trappings, and the Suites of woe\\n\\n   King. 'Tis sweet and commendable\\nIn your Nature Hamlet,\\nTo giue these mourning duties to your Father:\\nBut you must know, your Father lost a Father,\\nThat Father lost, lost his, and the Suruiuer bound\\nIn filiall Obligation, for some terme\\nTo do obsequious Sorrow. But to perseuer\\nIn obstinate Condolement, is a course\\nOf impious stubbornnesse. 'Tis vnmanly greefe,\\nIt shewes a will most incorrect to Heauen,\\nA Heart vnfortified, a Minde impatient,\\nAn Vnderstanding simple, and vnschool'd:\\nFor, what we know must be, and is as common\\nAs any the most vulgar thing to sence,\\nWhy should we in our peeuish Opposition\\nTake it to heart? Fye, 'tis a fault to Heauen,\\nA fault against the Dead, a fault to Nature,\\nTo Reason most absurd, whose common Theame\\nIs death of Fathers, and who still hath cried,\\nFrom the first Coarse, till he that dyed to day,\\nThis must be so. We pray you throw to earth\\nThis vnpreuayling woe, and thinke of vs\\nAs of a Father; For let the world take note,\\nYou are the most immediate to our Throne,\\nAnd with no lesse Nobility of Loue,\\nThen that which deerest Father beares his Sonne,\\nDo I impart towards you. For your intent\\nIn going backe to Schoole in Wittenberg,\\nIt is most retrograde to our desire:\\nAnd we beseech you, bend you to remaine\\nHeere in the cheere and comfort of our eye,\\nOur cheefest Courtier Cosin, and our Sonne\\n\\n   Qu. Let not thy Mother lose her Prayers Hamlet:\\nI prythee stay with vs, go not to Wittenberg\\n\\n   Ham. I shall in all my best\\nObey you Madam\\n\\n   King. Why 'tis a louing, and a faire Reply,\\nBe as our selfe in Denmarke. Madam come,\\nThis gentle and vnforc'd accord of Hamlet\\nSits smiling to my heart; in grace whereof,\\nNo iocond health that Denmarke drinkes to day,\\nBut the great Cannon to the Clowds shall tell,\\nAnd the Kings Rouce, the Heauens shall bruite againe,\\nRespeaking earthly Thunder. Come away.\\n\\nExeunt.\\n\\nManet Hamlet.\\n\\n  Ham. Oh that this too too solid Flesh, would melt,\\nThaw, and resolue it selfe into a Dew:\\nOr that the Euerlasting had not fixt\\nHis Cannon 'gainst Selfe-slaughter. O God, O God!\\nHow weary, stale, flat, and vnprofitable\\nSeemes to me all the vses of this world?\\nFie on't? Oh fie, fie, 'tis an vnweeded Garden\\nThat growes to Seed: Things rank, and grosse in Nature\\nPossesse it meerely. That it should come to this:\\nBut two months dead: Nay, not so much; not two,\\nSo excellent a King, that was to this\\nHiperion to a Satyre: so louing to my Mother,\\nThat he might not beteene the windes of heauen\\nVisit her face too roughly. Heauen and Earth\\nMust I remember: why she would hang on him,\\nAs if encrease of Appetite had growne\\nBy what is fed on; and yet within a month?\\nLet me not thinke on't: Frailty, thy name is woman.\\nA little Month, or ere those shooes were old,\\nWith which she followed my poore Fathers body\\nLike Niobe, all teares. Why she, euen she.\\n(O Heauen! A beast that wants discourse of Reason\\nWould haue mourn'd longer) married with mine Vnkle,\\nMy Fathers Brother: but no more like my Father,\\nThen I to Hercules. Within a Moneth?\\nEre yet the salt of most vnrighteous Teares\\nHad left the flushing of her gauled eyes,\\nShe married. O most wicked speed, to post\\nWith such dexterity to Incestuous sheets:\\nIt is not, nor it cannot come to good.\\nBut breake my heart, for I must hold my tongue.\\nEnter Horatio, Barnardo, and Marcellus.\\n\\n  Hor. Haile to your Lordship\\n\\n   Ham. I am glad to see you well:\\nHoratio, or I do forget my selfe\\n\\n   Hor. The same my Lord,\\nAnd your poore Seruant euer\\n\\n   Ham. Sir my good friend,\\nIle change that name with you:\\nAnd what make you from Wittenberg Horatio?\\nMarcellus\\n\\n   Mar. My good Lord\\n\\n   Ham. I am very glad to see you: good euen Sir.\\nBut what in faith make you from Wittemberge?\\n  Hor. A truant disposition, good my Lord\\n\\n   Ham. I would not haue your Enemy say so;\\nNor shall you doe mine eare that violence,\\nTo make it truster of your owne report\\nAgainst your selfe. I know you are no Truant:\\nBut what is your affaire in Elsenour?\\nWee'l teach you to drinke deepe, ere you depart\\n\\n   Hor. My Lord, I came to see your Fathers Funerall\\n\\n   Ham. I pray thee doe not mock me (fellow Student)\\nI thinke it was to see my Mothers Wedding\\n\\n   Hor. Indeed my Lord, it followed hard vpon\\n\\n   Ham. Thrift thrift Horatio: the Funerall Bakt-meats\\nDid coldly furnish forth the Marriage Tables;\\nWould I had met my dearest foe in heauen,\\nEre I had euer seene that day Horatio.\\nMy father, me thinkes I see my father\\n\\n   Hor. Oh where my Lord?\\n  Ham. In my minds eye (Horatio)\\n  Hor. I saw him once; he was a goodly King\\n\\n   Ham. He was a man, take him for all in all:\\nI shall not look vpon his like againe\\n\\n   Hor. My Lord, I thinke I saw him yesternight\\n\\n   Ham. Saw? Who?\\n  Hor. My Lord, the King your Father\\n\\n   Ham. The King my Father?\\n  Hor. Season your admiration for a while\\nWith an attent eare; till I may deliuer\\nVpon the witnesse of these Gentlemen,\\nThis maruell to you\\n\\n   Ham. For Heauens loue let me heare\\n\\n   Hor. Two nights together, had these Gentlemen\\n(Marcellus and Barnardo) on their Watch\\nIn the dead wast and middle of the night\\nBeene thus encountred. A figure like your Father,\\nArm'd at all points exactly, Cap a Pe,\\nAppeares before them, and with sollemne march\\nGoes slow and stately: By them thrice he walkt,\\nBy their opprest and feare-surprized eyes,\\nWithin his Truncheons length; whilst they bestil'd\\nAlmost to Ielly with the Act of feare,\\nStand dumbe and speake not to him. This to me\\nIn dreadfull secrecie impart they did,\\nAnd I with them the third Night kept the Watch,\\nWhereas they had deliuer'd both in time,\\nForme of the thing; each word made true and good,\\nThe Apparition comes. I knew your Father:\\nThese hands are not more like\\n\\n   Ham. But where was this?\\n  Mar. My Lord vpon the platforme where we watcht\\n\\n   Ham. Did you not speake to it?\\n  Hor. My Lord, I did;\\nBut answere made it none: yet once me thought\\nIt lifted vp it head, and did addresse\\nIt selfe to motion, like as it would speake:\\nBut euen then, the Morning Cocke crew lowd;\\nAnd at the sound it shrunke in hast away,\\nAnd vanisht from our sight\\n\\n   Ham. Tis very strange\\n\\n   Hor. As I doe liue my honourd Lord 'tis true;\\nAnd we did thinke it writ downe in our duty\\nTo let you know of it\\n\\n   Ham. Indeed, indeed Sirs; but this troubles me.\\nHold you the watch to Night?\\n  Both. We doe my Lord\\n\\n   Ham. Arm'd, say you?\\n  Both. Arm'd, my Lord\\n\\n   Ham. From top to toe?\\n  Both. My Lord, from head to foote\\n\\n   Ham. Then saw you not his face?\\n  Hor. O yes, my Lord, he wore his Beauer vp\\n\\n   Ham. What, lookt he frowningly?\\n  Hor. A countenance more in sorrow then in anger\\n\\n   Ham. Pale, or red?\\n  Hor. Nay very pale\\n\\n   Ham. And fixt his eyes vpon you?\\n  Hor. Most constantly\\n\\n   Ham. I would I had beene there\\n\\n   Hor. It would haue much amaz'd you\\n\\n   Ham. Very like, very like: staid it long?\\n  Hor. While one with moderate hast might tell a hundred\\n\\n   All. Longer, longer\\n\\n   Hor. Not when I saw't\\n\\n   Ham. His Beard was grisly? no\\n\\n   Hor. It was, as I haue seene it in his life,\\nA Sable Siluer'd\\n\\n   Ham. Ile watch to Night; perchance 'twill wake againe\\n\\n   Hor. I warrant you it will\\n\\n   Ham. If it assume my noble Fathers person,\\nIle speake to it, though Hell it selfe should gape\\nAnd bid me hold my peace. I pray you all,\\nIf you haue hitherto conceald this sight;\\nLet it bee treble in your silence still:\\nAnd whatsoeuer els shall hap to night,\\nGiue it an vnderstanding but no tongue;\\nI will requite your loues; so fare ye well:\\nVpon the Platforme twixt eleuen and twelue,\\nIle visit you\\n\\n   All. Our duty to your Honour.\\n\\nExeunt\\n\\n   Ham. Your loue, as mine to you: farewell.\\nMy Fathers Spirit in Armes? All is not well:\\nI doubt some foule play: would the Night were come;\\nTill then sit still my soule; foule deeds will rise,\\nThough all the earth orewhelm them to mens eies.\\nEnter.\\n\\n\\nScena Tertia\\n\\n\\nEnter Laertes and Ophelia.\\n\\n  Laer. My necessaries are imbark't; Farewell:\\nAnd Sister, as the Winds giue Benefit,\\nAnd Conuoy is assistant; doe not sleepe,\\nBut let me heare from you\\n\\n   Ophel. Doe you doubt that?\\n  Laer. For Hamlet, and the trifling of his fauours,\\nHold it a fashion and a toy in Bloude;\\nA Violet in the youth of Primy Nature;\\nFroward, not permanent; sweet not lasting\\nThe suppliance of a minute? No more\\n\\n   Ophel. No more but so\\n\\n   Laer. Thinke it no more:\\nFor nature cressant does not grow alone,\\nIn thewes and Bulke: but as his Temple waxes,\\nThe inward seruice of the Minde and Soule\\nGrowes wide withall. Perhaps he loues you now,\\nAnd now no soyle nor cautell doth besmerch\\nThe vertue of his feare: but you must feare\\nHis greatnesse weigh'd, his will is not his owne;\\nFor hee himselfe is subiect to his Birth:\\nHee may not, as vnuallued persons doe,\\nCarue for himselfe; for, on his choyce depends\\nThe sanctity and health of the whole State.\\nAnd therefore must his choyce be circumscrib'd\\nVnto the voyce and yeelding of that Body,\\nWhereof he is the Head. Then if he sayes he loues you,\\nIt fits your wisedome so farre to beleeue it;\\nAs he in his peculiar Sect and force\\nMay giue his saying deed: which is no further,\\nThen the maine voyce of Denmarke goes withall.\\nThen weight what losse your Honour may sustaine,\\nIf with too credent eare you list his Songs;\\nOr lose your Heart; or your chast Treasure open\\nTo his vnmastred importunity.\\nFeare it Ophelia, feare it my deare Sister,\\nAnd keepe within the reare of your Affection;\\nOut of the shot and danger of Desire.\\nThe chariest Maid is Prodigall enough,\\nIf she vnmaske her beauty to the Moone:\\nVertue it selfe scapes not calumnious stroakes,\\nThe Canker Galls, the Infants of the Spring\\nToo oft before the buttons be disclos'd,\\nAnd in the Morne and liquid dew of Youth,\\nContagious blastments are most imminent.\\nBe wary then, best safety lies in feare;\\nYouth to it selfe rebels, though none else neere\\n\\n   Ophe. I shall th' effect of this good Lesson keepe,\\nAs watchmen to my heart: but good my Brother\\nDoe not as some vngracious Pastors doe,\\nShew me the steepe and thorny way to Heauen;\\nWhilst like a puft and recklesse Libertine\\nHimselfe, the Primrose path of dalliance treads,\\nAnd reaks not his owne reade\\n\\n   Laer. Oh, feare me not.\\nEnter Polonius.\\n\\nI stay too long; but here my Father comes:\\nA double blessing is a double grace;\\nOccasion smiles vpon a second leaue\\n\\n   Polon. Yet heere Laertes? Aboord, aboord for shame,\\nThe winde sits in the shoulder of your saile,\\nAnd you are staid for there: my blessing with you;\\nAnd these few Precepts in thy memory,\\nSee thou Character. Giue thy thoughts no tongue,\\nNor any vnproportion'd thoughts his Act:\\nBe thou familiar; but by no meanes vulgar:\\nThe friends thou hast, and their adoption tride,\\nGrapple them to thy Soule, with hoopes of Steele:\\nBut doe not dull thy palme, with entertainment\\nOf each vnhatch't, vnfledg'd Comrade. Beware\\nOf entrance to a quarrell: but being in\\nBear't that th' opposed may beware of thee.\\nGiue euery man thine eare; but few thy voyce:\\nTake each mans censure; but reserue thy iudgement:\\nCostly thy habit as thy purse can buy;\\nBut not exprest in fancie; rich, not gawdie:\\nFor the Apparell oft proclaimes the man.\\nAnd they in France of the best ranck and station,\\nAre of a most select and generous cheff in that.\\nNeither a borrower, nor a lender be;\\nFor lone oft loses both it selfe and friend:\\nAnd borrowing duls the edge of Husbandry.\\nThis aboue all; to thine owne selfe be true:\\nAnd it must follow, as the Night the Day,\\nThou canst not then be false to any man.\\nFarewell: my Blessing season this in thee\\n\\n   Laer. Most humbly doe I take my leaue, my Lord\\n\\n   Polon. The time inuites you, goe, your seruants tend\\n\\n   Laer. Farewell Ophelia, and remember well\\nWhat I haue said to you\\n\\n   Ophe. Tis in my memory lockt,\\nAnd you your selfe shall keepe the key of it\\n\\n   Laer. Farewell.\\n\\nExit Laer.\\n\\n  Polon. What ist Ophelia he hath said to you?\\n  Ophe. So please you, somthing touching the L[ord]. Hamlet\\n\\n   Polon. Marry, well bethought:\\nTis told me he hath very oft of late\\nGiuen priuate time to you; and you your selfe\\nHaue of your audience beene most free and bounteous.\\nIf it be so, as so tis put on me;\\nAnd that in way of caution: I must tell you,\\nYou doe not vnderstand your selfe so cleerely,\\nAs it behoues my Daughter, and your Honour.\\nWhat is betweene you, giue me vp the truth?\\n  Ophe. He hath my Lord of late, made many tenders\\nOf his affection to me\\n\\n   Polon. Affection, puh. You speake like a greene Girle,\\nVnsifted in such perillous Circumstance.\\nDoe you beleeue his tenders, as you call them?\\n  Ophe. I do not know, my Lord, what I should thinke\\n\\n   Polon. Marry Ile teach you; thinke your selfe a Baby,\\nThat you haue tane his tenders for true pay,\\nWhich are not starling. Tender your selfe more dearly;\\nOr not to crack the winde of the poore Phrase,\\nRoaming it thus, you'l tender me a foole\\n\\n   Ophe. My Lord, he hath importun'd me with loue,\\nIn honourable fashion\\n\\n   Polon. I, fashion you may call it, go too, go too\\n\\n   Ophe. And hath giuen countenance to his speech,\\nMy Lord, with all the vowes of Heauen\\n\\n   Polon. I, Springes to catch Woodcocks. I doe know\\nWhen the Bloud burnes, how Prodigall the Soule\\nGiues the tongue vowes: these blazes, Daughter,\\nGiuing more light then heate; extinct in both,\\nEuen in their promise, as it is a making;\\nYou must not take for fire. For this time Daughter,\\nBe somewhat scanter of your Maiden presence;\\nSet your entreatments at a higher rate,\\nThen a command to parley. For Lord Hamlet,\\nBeleeue so much in him, that he is young,\\nAnd with a larger tether may he walke,\\nThen may be giuen you. In few, Ophelia,\\nDoe not beleeue his vowes; for they are Broakers,\\nNot of the eye, which their Inuestments show:\\nBut meere implorators of vnholy Sutes,\\nBreathing like sanctified and pious bonds,\\nThe better to beguile. This is for all:\\nI would not, in plaine tearmes, from this time forth,\\nHaue you so slander any moment leisure,\\nAs to giue words or talke with the Lord Hamlet:\\nLooke too't, I charge you; come your wayes\\n\\n   Ophe. I shall obey my Lord.\\n\\nExeunt.\\n\\nEnter Hamlet, Horatio, Marcellus.\\n\\n  Ham. The Ayre bites shrewdly: is it very cold?\\n  Hor. It is a nipping and an eager ayre\\n\\n   Ham. What hower now?\\n  Hor. I thinke it lacks of twelue\\n\\n   Mar. No, it is strooke\\n\\n   Hor. Indeed I heard it not: then it drawes neere the season,\\nWherein the Spirit held his wont to walke.\\nWhat does this meane my Lord?\\n  Ham. The King doth wake to night, and takes his rouse,\\nKeepes wassels and the swaggering vpspring reeles,\\nAnd as he dreines his draughts of Renish downe,\\nThe kettle Drum and Trumpet thus bray out\\nThe triumph of his Pledge\\n\\n   Horat. Is it a custome?\\n  Ham. I marry ist;\\nAnd to my mind, though I am natiue heere,\\nAnd to the manner borne: It is a Custome\\nMore honour'd in the breach, then the obseruance.\\nEnter Ghost.\\n\\n  Hor. Looke my Lord, it comes\\n\\n   Ham. Angels and Ministers of Grace defend vs:\\nBe thou a Spirit of health, or Goblin damn'd,\\nBring with thee ayres from Heauen, or blasts from Hell,\\nBe thy euents wicked or charitable,\\nThou com'st in such a questionable shape\\nThat I will speake to thee. Ile call thee Hamlet,\\nKing, Father, Royall Dane: Oh, oh, answer me,\\nLet me not burst in Ignorance; but tell\\nWhy thy Canoniz'd bones Hearsed in death,\\nHaue burst their cerments, why the Sepulcher\\nWherein we saw thee quietly enurn'd,\\nHath op'd his ponderous and Marble iawes,\\nTo cast thee vp againe? What may this meane?\\nThat thou dead Coarse againe in compleat steele,\\nReuisits thus the glimpses of the Moone,\\nMaking Night hidious? And we fooles of Nature,\\nSo horridly to shake our disposition,\\nWith thoughts beyond thee; reaches of our Soules,\\nSay, why is this? wherefore? what should we doe?\\n\\nGhost beckens Hamlet.\\n\\n  Hor. It beckons you to goe away with it,\\nAs if it some impartment did desire\\nTo you alone\\n\\n   Mar. Looke with what courteous action\\nIt wafts you to a more remoued ground:\\nBut doe not goe with it\\n\\n   Hor. No, by no meanes\\n\\n   Ham. It will not speake: then will I follow it\\n\\n   Hor. Doe not my Lord\\n\\n   Ham. Why, what should be the feare?\\nI doe not set my life at a pins fee;\\nAnd for my Soule, what can it doe to that?\\nBeing a thing immortall as it selfe:\\nIt waues me forth againe; Ile follow it\\n\\n   Hor. What if it tempt you toward the Floud my Lord?\\nOr to the dreadfull Sonnet of the Cliffe,\\nThat beetles o're his base into the Sea,\\nAnd there assumes some other horrible forme,\\nWhich might depriue your Soueraignty of Reason,\\nAnd draw you into madnesse thinke of it?\\n  Ham. It wafts me still: goe on, Ile follow thee\\n\\n   Mar. You shall not goe my Lord\\n\\n   Ham. Hold off your hand\\n\\n   Hor. Be rul'd, you shall not goe\\n\\n   Ham. My fate cries out,\\nAnd makes each petty Artire in this body,\\nAs hardy as the Nemian Lions nerue:\\nStill am I cal'd? Vnhand me Gentlemen:\\nBy Heau'n, Ile make a Ghost of him that lets me:\\nI say away, goe on, Ile follow thee.\\n\\nExeunt. Ghost & Hamlet.\\n\\n  Hor. He waxes desperate with imagination\\n\\n   Mar. Let's follow; 'tis not fit thus to obey him\\n\\n   Hor. Haue after, to what issue will this come?\\n  Mar. Something is rotten in the State of Denmarke\\n\\n   Hor. Heauen will direct it\\n\\n   Mar. Nay, let's follow him.\\n\\nExeunt.\\n\\nEnter Ghost and Hamlet.\\n\\n  Ham. Where wilt thou lead me? speak; Ile go no further\\n\\n   Gho. Marke me\\n\\n   Ham. I will\\n\\n   Gho. My hower is almost come,\\nWhen I to sulphurous and tormenting Flames\\nMust render vp my selfe\\n\\n   Ham. Alas poore Ghost\\n\\n   Gho. Pitty me not, but lend thy serious hearing\\nTo what I shall vnfold\\n\\n   Ham. Speake, I am bound to heare\\n\\n   Gho. So art thou to reuenge, when thou shalt heare\\n\\n   Ham. What?\\n  Gho. I am thy Fathers Spirit,\\nDoom'd for a certaine terme to walke the night;\\nAnd for the day confin'd to fast in Fiers,\\nTill the foule crimes done in my dayes of Nature\\nAre burnt and purg'd away? But that I am forbid\\nTo tell the secrets of my Prison-House;\\nI could a Tale vnfold, whose lightest word\\nWould harrow vp thy soule, freeze thy young blood,\\nMake thy two eyes like Starres, start from their Spheres,\\nThy knotty and combined lockes to part,\\nAnd each particular haire to stand an end,\\nLike Quilles vpon the fretfull Porpentine:\\nBut this eternall blason must not be\\nTo eares of flesh and bloud; list Hamlet, oh list,\\nIf thou didst euer thy deare Father loue\\n\\n   Ham. Oh Heauen!\\n  Gho. Reuenge his foule and most vnnaturall Murther\\n\\n   Ham. Murther?\\n  Ghost. Murther most foule, as in the best it is;\\nBut this most foule, strange, and vnnaturall\\n\\n   Ham. Hast, hast me to know it,\\nThat with wings as swift\\nAs meditation, or the thoughts of Loue,\\nMay sweepe to my Reuenge\\n\\n   Ghost. I finde thee apt,\\nAnd duller should'st thou be then the fat weede\\nThat rots it selfe in ease, on Lethe Wharfe,\\nWould'st thou not stirre in this. Now Hamlet heare:\\nIt's giuen out, that sleeping in mine Orchard,\\nA Serpent stung me: so the whole eare of Denmarke,\\nIs by a forged processe of my death\\nRankly abus'd: But know thou Noble youth,\\nThe Serpent that did sting thy Fathers life,\\nNow weares his Crowne\\n\\n   Ham. O my Propheticke soule: mine Vncle?\\n  Ghost. I that incestuous, that adulterate Beast\\nWith witchcraft of his wits, hath Traitorous guifts.\\nOh wicked Wit, and Gifts, that haue the power\\nSo to seduce? Won to this shamefull Lust\\nThe will of my most seeming vertuous Queene:\\nOh Hamlet, what a falling off was there,\\nFrom me, whose loue was of that dignity,\\nThat it went hand in hand, euen with the Vow\\nI made to her in Marriage; and to decline\\nVpon a wretch, whose Naturall gifts were poore\\nTo those of mine. But Vertue, as it neuer wil be moued,\\nThough Lewdnesse court it in a shape of Heauen:\\nSo Lust, though to a radiant Angell link'd,\\nWill sate it selfe in a Celestiall bed, & prey on Garbage.\\nBut soft, me thinkes I sent the Mornings Ayre;\\nBriefe let me be: Sleeping within mine Orchard,\\nMy custome alwayes in the afternoone;\\nVpon my secure hower thy Vncle stole\\nWith iuyce of cursed Hebenon in a Violl,\\nAnd in the Porches of mine eares did poure\\nThe leaperous Distilment; whose effect\\nHolds such an enmity with bloud of Man,\\nThat swift as Quick-siluer, it courses through\\nThe naturall Gates and Allies of the body;\\nAnd with a sodaine vigour it doth posset\\nAnd curd, like Aygre droppings into Milke,\\nThe thin and wholsome blood: so did it mine;\\nAnd a most instant Tetter bak'd about,\\nMost Lazar-like, with vile and loathsome crust,\\nAll my smooth Body.\\nThus was I, sleeping, by a Brothers hand,\\nOf Life, of Crowne, and Queene at once dispatcht;\\nCut off euen in the Blossomes of my Sinne,\\nVnhouzzled, disappointed, vnnaneld,\\nNo reckoning made, but sent to my account\\nWith all my imperfections on my head;\\nOh horrible Oh horrible, most horrible:\\nIf thou hast nature in thee beare it not;\\nLet not the Royall Bed of Denmarke be\\nA Couch for Luxury and damned Incest.\\nBut howsoeuer thou pursuest this Act,\\nTaint not thy mind; nor let thy Soule contriue\\nAgainst thy Mother ought; leaue her to heauen,\\nAnd to those Thornes that in her bosome lodge,\\nTo pricke and sting her. Fare thee well at once;\\nThe Glow-worme showes the Matine to be neere,\\nAnd gins to pale his vneffectuall Fire:\\nAdue, adue, Hamlet: remember me.\\nEnter.\\n\\n  Ham. Oh all you host of Heauen! Oh Earth; what els?\\nAnd shall I couple Hell? Oh fie: hold my heart;\\nAnd you my sinnewes, grow not instant Old;\\nBut beare me stiffely vp: Remember thee?\\nI, thou poore Ghost, while memory holds a seate\\nIn this distracted Globe: Remember thee?\\nYea, from the Table of my Memory,\\nIle wipe away all triuiall fond Records,\\nAll sawes of Bookes, all formes, all presures past,\\nThat youth and obseruation coppied there;\\nAnd thy Commandment all alone shall liue\\nWithin the Booke and Volume of my Braine,\\nVnmixt with baser matter; yes yes, by Heauen:\\nOh most pernicious woman!\\nOh Villaine, Villaine, smiling damned Villaine!\\nMy Tables, my Tables; meet it is I set it downe,\\nThat one may smile, and smile and be a Villaine;\\nAt least I'm sure it may be so in Denmarke;\\nSo Vnckle there you are: now to my word;\\nIt is; Adue, Adue, Remember me: I haue sworn't\\n\\n   Hor. & Mar. within. My Lord, my Lord.\\nEnter Horatio and Marcellus.\\n\\n  Mar. Lord Hamlet\\n\\n   Hor. Heauen secure him\\n\\n   Mar. So be it\\n\\n   Hor. Illo, ho, ho, my Lord\\n\\n   Ham. Hillo, ho, ho, boy; come bird, come\\n\\n   Mar. How ist my Noble Lord?\\n  Hor. What newes, my Lord?\\n  Ham. Oh wonderfull!\\n  Hor. Good my Lord tell it\\n\\n   Ham. No you'l reueale it\\n\\n   Hor. Not I, my Lord, by Heauen\\n\\n   Mar. Nor I, my Lord\\n\\n   Ham. How say you then, would heart of man once think it?\\nBut you'l be secret?\\n  Both. I, by Heau'n, my Lord\\n\\n   Ham. There's nere a villaine dwelling in all Denmarke\\nBut hee's an arrant knaue\\n\\n   Hor. There needs no Ghost my Lord, come from the\\nGraue, to tell vs this\\n\\n   Ham. Why right, you are i'th' right;\\nAnd so, without more circumstance at all,\\nI hold it fit that we shake hands, and part:\\nYou, as your busines and desires shall point you:\\nFor euery man ha's businesse and desire,\\nSuch as it is: and for mine owne poore part,\\nLooke you, Ile goe pray\\n\\n   Hor. These are but wild and hurling words, my Lord\\n\\n   Ham. I'm sorry they offend you heartily:\\nYes faith, heartily\\n\\n   Hor. There's no offence my Lord\\n\\n   Ham. Yes, by Saint Patricke, but there is my Lord,\\nAnd much offence too, touching this Vision heere:\\nIt is an honest Ghost, that let me tell you:\\nFor your desire to know what is betweene vs,\\nO'remaster't as you may. And now good friends,\\nAs you are Friends, Schollers and Soldiers,\\nGiue me one poore request\\n\\n   Hor. What is't my Lord? we will\\n\\n   Ham. Neuer make known what you haue seen to night\\n\\n   Both. My Lord, we will not\\n\\n   Ham. Nay, but swear't\\n\\n   Hor. Infaith my Lord, not I\\n\\n   Mar. Nor I my Lord: in faith\\n\\n   Ham. Vpon my sword\\n\\n   Marcell. We haue sworne my Lord already\\n\\n   Ham. Indeed, vpon my sword, Indeed\\n\\n   Gho. Sweare.\\n\\nGhost cries vnder the Stage.\\n\\n  Ham. Ah ha boy, sayest thou so. Art thou there truepenny?\\nCome one you here this fellow in the selleredge\\nConsent to sweare\\n\\n   Hor. Propose the Oath my Lord\\n\\n   Ham. Neuer to speake of this that you haue seene.\\nSweare by my sword\\n\\n   Gho. Sweare\\n\\n   Ham. Hic & vbique? Then wee'l shift for grownd,\\nCome hither Gentlemen,\\nAnd lay your hands againe vpon my sword,\\nNeuer to speake of this that you haue heard:\\nSweare by my Sword\\n\\n   Gho. Sweare\\n\\n   Ham. Well said old Mole, can'st worke i'th' ground so fast?\\nA worthy Pioner, once more remoue good friends\\n\\n   Hor. Oh day and night: but this is wondrous strange\\n\\n   Ham. And therefore as a stranger giue it welcome.\\nThere are more things in Heauen and Earth, Horatio,\\nThen are dream't of in our Philosophy. But come,\\nHere as before, neuer so helpe you mercy,\\nHow strange or odde so ere I beare my selfe;\\n(As I perchance heereafter shall thinke meet\\nTo put an Anticke disposition on:)\\nThat you at such time seeing me, neuer shall\\nWith Armes encombred thus, or thus, head shake;\\nOr by pronouncing of some doubtfull Phrase;\\nAs well, we know, or we could and if we would,\\nOr if we list to speake; or there be and if there might,\\nOr such ambiguous giuing out to note,\\nThat you know ought of me; this not to doe:\\nSo grace and mercy at your most neede helpe you:\\nSweare\\n\\n   Ghost. Sweare\\n\\n   Ham. Rest, rest perturbed Spirit: so Gentlemen,\\nWith all my loue I doe commend me to you;\\nAnd what so poore a man as Hamlet is,\\nMay doe t' expresse his loue and friending to you,\\nGod willing shall not lacke: let vs goe in together,\\nAnd still your fingers on your lippes I pray,\\nThe time is out of ioynt: Oh cursed spight,\\nThat euer I was borne to set it right.\\nNay, come let's goe together.\\n\\nExeunt.\\n\\n\\nActus Secundus.\\n\\nEnter Polonius, and Reynoldo.\\n\\n  Polon. Giue him his money, and these notes Reynoldo\\n\\n   Reynol. I will my Lord\\n\\n   Polon. You shall doe maruels wisely: good Reynoldo,\\nBefore you visite him you make inquiry\\nOf his behauiour\\n\\n   Reynol. My Lord, I did intend it\\n\\n   Polon. Marry, well said;\\nVery well said. Looke you Sir,\\nEnquire me first what Danskers are in Paris;\\nAnd how, and who; what meanes; and where they keepe:\\nWhat company, at what expence: and finding\\nBy this encompassement and drift of question,\\nThat they doe know my sonne: Come you more neerer\\nThen your particular demands will touch it,\\nTake you as 'twere some distant knowledge of him,\\nAnd thus I know his father and his friends,\\nAnd in part him. Doe you marke this Reynoldo?\\n  Reynol. I, very well my Lord\\n\\n   Polon. And in part him, but you may say not well;\\nBut if't be hee I meane, hees very wilde;\\nAddicted so and so; and there put on him\\nWhat forgeries you please; marry, none so ranke,\\nAs may dishonour him; take heed of that:\\nBut Sir, such wanton, wild, and vsuall slips,\\nAs are Companions noted and most knowne\\nTo youth and liberty\\n\\n   Reynol. As gaming my Lord\\n\\n   Polon. I, or drinking, fencing, swearing,\\nQuarelling, drabbing. You may goe so farre\\n\\n   Reynol. My Lord that would dishonour him\\n\\n   Polon. Faith no, as you may season it in the charge;\\nYou must not put another scandall on him,\\nThat hee is open to Incontinencie;\\nThat's not my meaning: but breath his faults so quaintly,\\nThat they may seeme the taints of liberty;\\nThe flash and out-breake of a fiery minde,\\nA sauagenes in vnreclaim'd bloud of generall assault\\n\\n   Reynol. But my good Lord\\n\\n   Polon. Wherefore should you doe this?\\n  Reynol. I my Lord, I would know that\\n\\n   Polon. Marry Sir, heere's my drift,\\nAnd I belieue it is a fetch of warrant:\\nYou laying these slight sulleyes on my Sonne,\\nAs 'twere a thing a little soil'd i'th' working:\\nMarke you your party in conuerse; him you would sound,\\nHauing euer seene. In the prenominate crimes,\\nThe youth you breath of guilty, be assur'd\\nHe closes with you in this consequence:\\nGood sir, or so, or friend, or Gentleman.\\nAccording to the Phrase and the Addition,\\nOf man and Country\\n\\n   Reynol. Very good my Lord\\n\\n   Polon. And then Sir does he this?\\nHe does: what was I about to say?\\nI was about say somthing: where did I leaue?\\n  Reynol. At closes in the consequence:\\nAt friend, or so, and Gentleman\\n\\n   Polon. At closes in the consequence, I marry,\\nHe closes with you thus. I know the Gentleman,\\nI saw him yesterday, or tother day;\\nOr then or then, with such and such; and as you say,\\nThere was he gaming, there o'retooke in's Rouse,\\nThere falling out at Tennis; or perchance,\\nI saw him enter such a house of saile;\\nVidelicet, a Brothell, or so forth. See you now;\\nYour bait of falshood, takes this Cape of truth;\\nAnd thus doe we of wisedome and of reach\\nWith windlesses, and with assaies of Bias,\\nBy indirections finde directions out:\\nSo by my former Lecture and aduice\\nShall you my Sonne; you haue me, haue you not?\\n  Reynol. My Lord I haue\\n\\n   Polon. God buy you; fare you well\\n\\n   Reynol. Good my Lord\\n\\n   Polon. Obserue his inclination in your selfe\\n\\n   Reynol. I shall my Lord\\n\\n   Polon. And let him plye his Musicke\\n\\n   Reynol. Well, my Lord.\\nEnter.\\n\\nEnter Ophelia.\\n\\n  Polon. Farewell:\\nHow now Ophelia, what's the matter?\\n  Ophe. Alas my Lord, I haue beene so affrighted\\n\\n   Polon. With what, in the name of Heauen?\\n  Ophe. My Lord, as I was sowing in my Chamber,\\nLord Hamlet with his doublet all vnbrac'd,\\nNo hat vpon his head, his stockings foul'd,\\nVngartred, and downe giued to his Anckle,\\nPale as his shirt, his knees knocking each other,\\nAnd with a looke so pitious in purport,\\nAs if he had been loosed out of hell,\\nTo speake of horrors: he comes before me\\n\\n   Polon. Mad for thy Loue?\\n  Ophe. My Lord, I doe not know: but truly I do feare it\\n\\n   Polon. What said he?\\n  Ophe. He tooke me by the wrist, and held me hard;\\nThen goes he to the length of all his arme;\\nAnd with his other hand thus o're his brow,\\nHe fals to such perusall of my face,\\nAs he would draw it. Long staid he so,\\nAt last, a little shaking of mine Arme:\\nAnd thrice his head thus wauing vp and downe;\\nHe rais'd a sigh, so pittious and profound,\\nThat it did seeme to shatter all his bulke,\\nAnd end his being. That done, he lets me goe,\\nAnd with his head ouer his shoulders turn'd,\\nHe seem'd to finde his way without his eyes,\\nFor out adores he went without their helpe;\\nAnd to the last, bended their light on me\\n\\n   Polon. Goe with me, I will goe seeke the King,\\nThis is the very extasie of Loue,\\nWhose violent property foredoes it selfe,\\nAnd leads the will to desperate Vndertakings,\\nAs oft as any passion vnder Heauen,\\nThat does afflict our Natures. I am sorrie,\\nWhat haue you giuen him any hard words of late?\\n  Ophe. No my good Lord: but as you did command,\\nI did repell his Letters, and deny'de\\nHis accesse to me\\n\\n   Pol. That hath made him mad.\\nI am sorrie that with better speed and iudgement\\nI had not quoted him. I feare he did but trifle,\\nAnd meant to wracke thee: but beshrew my iealousie:\\nIt seemes it is as proper to our Age,\\nTo cast beyond our selues in our Opinions,\\nAs it is common for the yonger sort\\nTo lacke discretion. Come, go we to the King,\\nThis must be knowne, being kept close might moue\\nMore greefe to hide, then hate to vtter loue.\\n\\nExeunt.\\n\\n\\nScena Secunda.\\n\\nEnter King, Queene, Rosincrane, and Guildensterne Cum alijs.\\n\\n  King. Welcome deere Rosincrance and Guildensterne.\\nMoreouer, that we much did long to see you,\\nThe neede we haue to vse you, did prouoke\\nOur hastie sending. Something haue you heard\\nOf Hamlets transformation: so I call it,\\nSince not th' exterior, nor the inward man\\nResembles that it was. What it should bee\\nMore then his Fathers death, that thus hath put him\\nSo much from th' vnderstanding of himselfe,\\nI cannot deeme of. I intreat you both,\\nThat being of so young dayes brought vp with him:\\nAnd since so Neighbour'd to his youth, and humour,\\nThat you vouchsafe your rest heere in our Court\\nSome little time: so by your Companies\\nTo draw him on to pleasures, and to gather\\nSo much as from Occasions you may gleane,\\nThat open'd lies within our remedie\\n\\n   Qu. Good Gentlemen, he hath much talk'd of you,\\nAnd sure I am, two men there are not liuing,\\nTo whom he more adheres. If it will please you\\nTo shew vs so much Gentrie, and good will,\\nAs to expend your time with vs a-while,\\nFor the supply and profit of our Hope,\\nYour Visitation shall receiue such thankes\\nAs fits a Kings remembrance\\n\\n   Rosin. Both your Maiesties\\nMight by the Soueraigne power you haue of vs,\\nPut your dread pleasures, more into Command\\nThen to Entreatie\\n\\n   Guil. We both obey,\\nAnd here giue vp our selues, in the full bent,\\nTo lay our Seruices freely at your feete,\\nTo be commanded\\n\\n   King. Thankes Rosincrance, and gentle Guildensterne\\n\\n   Qu. Thankes Guildensterne and gentle Rosincrance.\\nAnd I beseech you instantly to visit\\nMy too much changed Sonne.\\nGo some of ye,\\nAnd bring the Gentlemen where Hamlet is\\n\\n   Guil. Heauens make our presence and our practises\\nPleasant and helpfull to him.\\nEnter.\\n\\n  Queene. Amen.\\nEnter Polonius.\\n\\n  Pol. Th' Ambassadors from Norwey, my good Lord,\\nAre ioyfully return'd\\n\\n   King. Thou still hast bin the father of good Newes\\n\\n   Pol. Haue I, my Lord? Assure you, my good Liege,\\nI hold my dutie, as I hold my Soule,\\nBoth to my God, one to my gracious King:\\nAnd I do thinke, or else this braine of mine\\nHunts not the traile of Policie, so sure\\nAs I haue vs'd to do: that I haue found\\nThe very cause of Hamlets Lunacie\\n\\n   King. Oh speake of that, that I do long to heare\\n\\n   Pol. Giue first admittance to th' Ambassadors,\\nMy Newes shall be the Newes to that great Feast\\n\\n   King. Thy selfe do grace to them, and bring them in.\\nHe tels me my sweet Queene, that he hath found\\nThe head and sourse of all your Sonnes distemper\\n\\n   Qu. I doubt it is no other, but the maine,\\nHis Fathers death, and our o're-hasty Marriage.\\nEnter Polonius, Voltumand, and Cornelius.\\n\\n  King. Well, we shall sift him. Welcome good Frends:\\nSay Voltumand, what from our Brother Norwey?\\n  Volt. Most faire returne of Greetings, and Desires.\\nVpon our first, he sent out to suppresse\\nHis Nephewes Leuies, which to him appear'd\\nTo be a preparation 'gainst the Poleak:\\nBut better look'd into, he truly found\\nIt was against your Highnesse, whereat greeued,\\nThat so his Sicknesse, Age, and Impotence\\nWas falsely borne in hand, sends out Arrests\\nOn Fortinbras, which he (in breefe) obeyes,\\nReceiues rebuke from Norwey: and in fine,\\nMakes Vow before his Vnkle, neuer more\\nTo giue th' assay of Armes against your Maiestie.\\nWhereon old Norwey, ouercome with ioy,\\nGiues him three thousand Crownes in Annuall Fee,\\nAnd his Commission to imploy those Soldiers\\nSo leuied as before, against the Poleak:\\nWith an intreaty heerein further shewne,\\nThat it might please you to giue quiet passe\\nThrough your Dominions, for his Enterprize,\\nOn such regards of safety and allowance,\\nAs therein are set downe\\n\\n   King. It likes vs well:\\nAnd at our more consider'd time wee'l read,\\nAnswer, and thinke vpon this Businesse.\\nMeane time we thanke you, for your well-tooke Labour.\\nGo to your rest, at night wee'l Feast together.\\nMost welcome home.\\n\\nExit Ambass.\\n\\n  Pol. This businesse is very well ended.\\nMy Liege, and Madam, to expostulate\\nWhat Maiestie should be, what Dutie is,\\nWhy day is day; night, night; and time is time,\\nWere nothing but to waste Night, Day, and Time.\\nTherefore, since Breuitie is the Soule of Wit,\\nAnd tediousnesse, the limbes and outward flourishes,\\nI will be breefe. Your Noble Sonne is mad:\\nMad call I it; for to define true Madnesse,\\nWhat is't, but to be nothing else but mad.\\nBut let that go\\n\\n   Qu. More matter, with lesse Art\\n\\n   Pol. Madam, I sweare I vse no Art at all:\\nThat he is mad, 'tis true: 'Tis true 'tis pittie,\\nAnd pittie it is true: A foolish figure,\\nBut farewell it: for I will vse no Art.\\nMad let vs grant him then: and now remaines\\nThat we finde out the cause of this effect,\\nOr rather say, the cause of this defect;\\nFor this effect defectiue, comes by cause,\\nThus it remaines, and the remainder thus. Perpend,\\nI haue a daughter: haue, whil'st she is mine,\\nWho in her Dutie and Obedience, marke,\\nHath giuen me this: now gather, and surmise.\\n\\nThe Letter.\\n\\nTo the Celestiall, and my Soules Idoll, the most beautifed Ophelia.\\nThat's an ill Phrase, a vilde Phrase, beautified is a vilde\\nPhrase: but you shall heare these in her excellent white\\nbosome, these\\n\\n   Qu. Came this from Hamlet to her\\n\\n   Pol. Good Madam stay awhile, I will be faithfull.\\nDoubt thou, the Starres are fire,\\nDoubt, that the Sunne doth moue:\\nDoubt Truth to be a Lier,\\nBut neuer Doubt, I loue.\\nO deere Ophelia, I am ill at these Numbers: I haue not Art to\\nreckon my grones; but that I loue thee best, oh most Best beleeue\\nit. Adieu.\\nThine euermore most deere Lady, whilst this\\nMachine is to him, Hamlet.\\nThis in Obedience hath my daughter shew'd me:\\nAnd more aboue hath his soliciting,\\nAs they fell out by Time, by Meanes, and Place,\\nAll giuen to mine eare\\n\\n   King. But how hath she receiu'd his Loue?\\n  Pol. What do you thinke of me?\\n  King. As of a man, faithfull and Honourable\\n\\n   Pol. I wold faine proue so. But what might you think?\\nWhen I had seene this hot loue on the wing,\\nAs I perceiued it, I must tell you that\\nBefore my Daughter told me what might you\\nOr my deere Maiestie your Queene heere, think,\\nIf I had playd the Deske or Table-booke,\\nOr giuen my heart a winking, mute and dumbe,\\nOr look'd vpon this Loue, with idle sight,\\nWhat might you thinke? No, I went round to worke,\\nAnd (my yong Mistris) thus I did bespeake\\nLord Hamlet is a Prince out of thy Starre,\\nThis must not be: and then, I Precepts gaue her,\\nThat she should locke her selfe from his Resort,\\nAdmit no Messengers, receiue no Tokens:\\nWhich done, she tooke the Fruites of my Aduice,\\nAnd he repulsed. A short Tale to make,\\nFell into a Sadnesse, then into a Fast,\\nThence to a Watch, thence into a Weaknesse,\\nThence to a Lightnesse, and by this declension\\nInto the Madnesse whereon now he raues,\\nAnd all we waile for\\n\\n   King. Do you thinke 'tis this?\\n  Qu. It may be very likely\\n\\n   Pol. Hath there bene such a time, I'de fain know that,\\nThat I haue possitiuely said, 'tis so,\\nWhen it prou'd otherwise?\\n  King. Not that I know\\n\\n   Pol. Take this from this; if this be otherwise,\\nIf Circumstances leade me, I will finde\\nWhere truth is hid, though it were hid indeede\\nWithin the Center\\n\\n   King. How may we try it further?\\n  Pol. You know sometimes\\nHe walkes foure houres together, heere\\nIn the Lobby\\n\\n   Qu. So he ha's indeed\\n\\n   Pol. At such a time Ile loose my Daughter to him,\\nBe you and I behinde an Arras then,\\nMarke the encounter: If he loue her not,\\nAnd be not from his reason falne thereon;\\nLet me be no Assistant for a State,\\nAnd keepe a Farme and Carters\\n\\n   King. We will try it.\\nEnter Hamlet reading on a Booke.\\n\\n  Qu. But looke where sadly the poore wretch\\nComes reading\\n\\n   Pol. Away I do beseech you, both away,\\nIle boord him presently.\\n\\nExit King & Queen.\\n\\nOh giue me leaue. How does my good Lord Hamlet?\\n  Ham. Well, God-a-mercy\\n\\n   Pol. Do you know me, my Lord?\\n  Ham. Excellent, excellent well: y'are a Fishmonger\\n\\n   Pol. Not I my Lord\\n\\n   Ham. Then I would you were so honest a man\\n\\n   Pol. Honest, my Lord?\\n  Ham. I sir, to be honest as this world goes, is to bee\\none man pick'd out of two thousand\\n\\n   Pol. That's very true, my Lord\\n\\n   Ham. For if the Sun breed Magots in a dead dogge,\\nbeing a good kissing Carrion-\\nHaue you a daughter?\\n  Pol. I haue my Lord\\n\\n   Ham. Let her not walke i'thSunne: Conception is a\\nblessing, but not as your daughter may conceiue. Friend\\nlooke too't\\n\\n   Pol. How say you by that? Still harping on my daughter:\\nyet he knew me not at first; he said I was a Fishmonger:\\nhe is farre gone, farre gone: and truly in my youth,\\nI suffred much extreamity for loue: very neere this. Ile\\nspeake to him againe. What do you read my Lord?\\n  Ham. Words, words, words\\n\\n   Pol. What is the matter, my Lord?\\n  Ham. Betweene who?\\n  Pol. I meane the matter you meane, my Lord\\n\\n   Ham. Slanders Sir: for the Satyricall slaue saies here,\\nthat old men haue gray Beards; that their faces are wrinkled;\\ntheir eyes purging thicke Amber, or Plum-Tree\\nGumme: and that they haue a plentifull locke of Wit,\\ntogether with weake Hammes. All which Sir, though I\\nmost powerfully, and potently beleeue; yet I holde it\\nnot Honestie to haue it thus set downe: For you your\\nselfe Sir, should be old as I am, if like a Crab you could\\ngo backward\\n\\n   Pol. Though this be madnesse,\\nYet there is Method in't: will you walke\\nOut of the ayre my Lord?\\n  Ham. Into my Graue?\\n  Pol. Indeed that is out o'th' Ayre:\\nHow pregnant (sometimes) his Replies are?\\nA happinesse,\\nThat often Madnesse hits on,\\nWhich Reason and Sanitie could not\\nSo prosperously be deliuer'd of.\\nI will leaue him,\\nAnd sodainely contriue the meanes of meeting\\nBetweene him, and my daughter.\\nMy Honourable Lord, I will most humbly\\nTake my leaue of you\\n\\n   Ham. You cannot Sir take from me any thing, that I\\nwill more willingly part withall, except my life, my\\nlife\\n\\n   Polon. Fare you well my Lord\\n\\n   Ham. These tedious old fooles\\n\\n   Polon. You goe to seeke my Lord Hamlet; there\\nhee is.\\nEnter Rosincran and Guildensterne.\\n\\n  Rosin. God saue you Sir\\n\\n   Guild. Mine honour'd Lord?\\n  Rosin. My most deare Lord?\\n  Ham. My excellent good friends? How do'st thou\\nGuildensterne? Oh, Rosincrane; good Lads: How doe ye\\nboth?\\n  Rosin. As the indifferent Children of the earth\\n\\n   Guild. Happy, in that we are not ouer-happy: on Fortunes\\nCap, we are not the very Button\\n\\n   Ham. Nor the Soales of her Shoo?\\n  Rosin. Neither my Lord\\n\\n   Ham. Then you liue about her waste, or in the middle\\nof her fauour?\\n  Guil. Faith, her priuates, we\\n\\n   Ham. In the secret parts of Fortune? Oh, most true:\\nshe is a Strumpet. What's the newes?\\n  Rosin. None my Lord; but that the World's growne\\nhonest\\n\\n   Ham. Then is Doomesday neere: But your newes is\\nnot true. Let me question more in particular: what haue\\nyou my good friends, deserued at the hands of Fortune,\\nthat she sends you to Prison hither?\\n  Guil. Prison, my Lord?\\n  Ham. Denmark's a Prison\\n\\n   Rosin. Then is the World one\\n\\n   Ham. A goodly one, in which there are many Confines,\\nWards, and Dungeons; Denmarke being one o'th'\\nworst\\n\\n   Rosin. We thinke not so my Lord\\n\\n   Ham. Why then 'tis none to you; for there is nothing\\neither good or bad, but thinking makes it so: to me it is\\na prison\\n\\n   Rosin. Why then your Ambition makes it one: 'tis\\ntoo narrow for your minde\\n\\n   Ham. O God, I could be bounded in a nutshell, and\\ncount my selfe a King of infinite space; were it not that\\nI haue bad dreames\\n\\n   Guil. Which dreames indeed are Ambition: for the\\nvery substance of the Ambitious, is meerely the shadow\\nof a Dreame\\n\\n   Ham. A dreame it selfe is but a shadow\\n\\n   Rosin. Truely, and I hold Ambition of so ayry and\\nlight a quality, that it is but a shadowes shadow\\n\\n   Ham. Then are our Beggers bodies; and our Monarchs\\nand out-stretcht Heroes the Beggers Shadowes:\\nshall wee to th' Court: for, by my fey I cannot reason?\\n  Both. Wee'l wait vpon you\\n\\n   Ham. No such matter. I will not sort you with the\\nrest of my seruants: for to speake to you like an honest\\nman: I am most dreadfully attended; but in the beaten\\nway of friendship, What make you at Elsonower?\\n  Rosin. To visit you my Lord, no other occasion\\n\\n   Ham. Begger that I am, I am euen poore in thankes;\\nbut I thanke you: and sure deare friends my thanks\\nare too deare a halfepeny; were you not sent for? Is it\\nyour owne inclining? Is it a free visitation? Come,\\ndeale iustly with me: come, come; nay speake\\n\\n   Guil. What should we say my Lord?\\n  Ham. Why any thing. But to the purpose; you were\\nsent for; and there is a kinde confession in your lookes;\\nwhich your modesties haue not craft enough to color,\\nI know the good King & Queene haue sent for you\\n\\n   Rosin. To what end my Lord?\\n  Ham. That you must teach me: but let mee coniure\\nyou by the rights of our fellowship, by the consonancy of\\nour youth, by the Obligation of our euer-preserued loue,\\nand by what more deare, a better proposer could charge\\nyou withall; be euen and direct with me, whether you\\nwere sent for or no\\n\\n   Rosin. What say you?\\n  Ham. Nay then I haue an eye of you: if you loue me\\nhold not off\\n\\n   Guil. My Lord, we were sent for\\n\\n   Ham. I will tell you why; so shall my anticipation\\npreuent your discouery of your secricie to the King and\\nQueene: moult no feather, I haue of late, but wherefore\\nI know not, lost all my mirth, forgone all custome of exercise;\\nand indeed, it goes so heauenly with my disposition;\\nthat this goodly frame the Earth, seemes to me a sterrill\\nPromontory; this most excellent Canopy the Ayre,\\nlook you, this braue ore-hanging, this Maiesticall Roofe,\\nfretted with golden fire: why, it appeares no other thing\\nto mee, then a foule and pestilent congregation of vapours.\\nWhat a piece of worke is a man! how Noble in\\nReason? how infinite in faculty? in forme and mouing\\nhow expresse and admirable? in Action, how like an Angel?\\nin apprehension, how like a God? the beauty of the\\nworld, the Parragon of Animals; and yet to me, what is\\nthis Quintessence of Dust? Man delights not me; no,\\nnor Woman neither; though by your smiling you seeme\\nto say so\\n\\n   Rosin. My Lord, there was no such stuffe in my\\nthoughts\\n\\n   Ham. Why did you laugh, when I said, Man delights\\nnot me?\\n  Rosin. To thinke, my Lord, if you delight not in Man,\\nwhat Lenton entertainment the Players shall receiue\\nfrom you: wee coated them on the way, and hither are\\nthey comming to offer you Seruice\\n\\n   Ham. He that playes the King shall be welcome; his\\nMaiesty shall haue Tribute of mee: the aduenturous\\nKnight shal vse his Foyle and Target: the Louer shall\\nnot sigh gratis, the humorous man shall end his part in\\npeace: the Clowne shall make those laugh whose lungs\\nare tickled a'th' sere: and the Lady shall say her minde\\nfreely; or the blanke Verse shall halt for't: what Players\\nare they?\\n  Rosin. Euen those you were wont to take delight in\\nthe Tragedians of the City\\n\\n   Ham. How chances it they trauaile? their residence\\nboth in reputation and profit was better both\\nwayes\\n\\n   Rosin. I thinke their Inhibition comes by the meanes\\nof the late Innouation?\\n  Ham. Doe they hold the same estimation they did\\nwhen I was in the City? Are they so follow'd?\\n  Rosin. No indeed, they are not\\n\\n   Ham. How comes it? doe they grow rusty?\\n  Rosin. Nay, their indeauour keepes in the wonted\\npace; But there is Sir an ayrie of Children, little\\nYases, that crye out on the top of question; and\\nare most tyrannically clap't for't: these are now the\\nfashion, and so be-ratled the common Stages (so they\\ncall them) that many wearing Rapiers, are affraide of\\nGoose-quils, and dare scarse come thither\\n\\n   Ham. What are they Children? Who maintains 'em?\\nHow are they escorted? Will they pursue the Quality no\\nlonger then they can sing? Will they not say afterwards\\nif they should grow themselues to common Players (as\\nit is most like if their meanes are not better) their Writers\\ndo them wrong, to make them exclaim against their\\nowne Succession\\n\\n   Rosin. Faith there ha's bene much to do on both sides:\\nand the Nation holds it no sinne, to tarre them to Controuersie.\\nThere was for a while, no mony bid for argument,\\nvnlesse the Poet and the Player went to Cuffes in\\nthe Question\\n\\n   Ham. Is't possible?\\n  Guild. Oh there ha's beene much throwing about of\\nBraines\\n\\n   Ham. Do the Boyes carry it away?\\n  Rosin. I that they do my Lord. Hercules & his load too\\n\\n   Ham. It is not strange: for mine Vnckle is King of\\nDenmarke, and those that would make mowes at him\\nwhile my Father liued; giue twenty, forty, an hundred\\nDucates a peece, for his picture in Little. There is something\\nin this more then Naturall, if Philosophie could\\nfinde it out.\\n\\nFlourish for the Players.\\n\\n  Guil. There are the Players\\n\\n   Ham. Gentlemen, you are welcom to Elsonower: your\\nhands, come: The appurtenance of Welcome, is Fashion\\nand Ceremony. Let me comply with you in the Garbe,\\nlest my extent to the Players (which I tell you must shew\\nfairely outward) should more appeare like entertainment\\nthen yours. You are welcome: but my Vnckle Father,\\nand Aunt Mother are deceiu'd\\n\\n   Guil. In what my deere Lord?\\n  Ham. I am but mad North, North-West: when the\\nWinde is Southerly, I know a Hawke from a Handsaw.\\nEnter Polonius.\\n\\n  Pol. Well be with you Gentlemen\\n\\n   Ham. Hearke you Guildensterne, and you too: at each\\neare a hearer: that great Baby you see there, is not yet\\nout of his swathing clouts\\n\\n   Rosin. Happily he's the second time come to them: for\\nthey say, an old man is twice a childe\\n\\n   Ham. I will Prophesie. Hee comes to tell me of the\\nPlayers. Mark it, you say right Sir: for a Monday morning\\n'twas so indeed\\n\\n   Pol. My Lord, I haue Newes to tell you\\n\\n   Ham. My Lord, I haue Newes to tell you.\\nWhen Rossius an Actor in Rome-\\n  Pol. The Actors are come hither my Lord\\n\\n   Ham. Buzze, buzze\\n\\n   Pol. Vpon mine Honor\\n\\n   Ham. Then can each Actor on his Asse-\\n  Polon. The best Actors in the world, either for Tragedie,\\nComedie, Historie, Pastorall:\\nPastoricall-Comicall-Historicall-Pastorall:\\nTragicall-Historicall: Tragicall-Comicall-Historicall-Pastorall:\\nScene indiuidible: or Poem\\nvnlimited. Seneca cannot be too heauy, nor Plautus\\ntoo light, for the law of Writ, and the Liberty. These are\\nthe onely men\\n\\n   Ham. O Iephta Iudge of Israel, what a Treasure had'st\\nthou?\\n  Pol. What a Treasure had he, my Lord?\\n  Ham. Why one faire Daughter, and no more,\\nThe which he loued passing well\\n\\n   Pol. Still on my Daughter\\n\\n   Ham. Am I not i'th' right old Iephta?\\n  Polon. If you call me Iephta my Lord, I haue a daughter\\nthat I loue passing well\\n\\n   Ham. Nay that followes not\\n\\n   Polon. What followes then, my Lord?\\n  Ha. Why, As by lot, God wot: and then you know, It\\ncame to passe, as most like it was: The first rowe of the\\nPons Chanson will shew you more. For looke where my\\nAbridgements come.\\nEnter foure or fiue Players.\\n\\nY'are welcome Masters, welcome all. I am glad to see\\nthee well: Welcome good Friends. Oh my olde Friend?\\nThy face is valiant since I saw thee last: Com'st thou to\\nbeard me in Denmarke? What, my yong Lady and Mistris?\\nByrlady your Ladiship is neerer Heauen then when\\nI saw you last, by the altitude of a Choppine. Pray God\\nyour voice like a peece of vncurrant Gold be not crack'd\\nwithin the ring. Masters, you are all welcome: wee'l e'ne\\nto't like French Faulconers, flie at any thing we see: wee'l\\nhaue a Speech straight. Come giue vs a tast of your quality:\\ncome, a passionate speech\\n\\n   1.Play. What speech, my Lord?\\n  Ham. I heard thee speak me a speech once, but it was\\nneuer Acted: or if it was, not aboue once, for the Play I\\nremember pleas'd not the Million, 'twas Cauiarie to the\\nGenerall: but it was (as I receiu'd it, and others, whose\\niudgement in such matters, cried in the top of mine) an\\nexcellent Play; well digested in the Scoenes, set downe\\nwith as much modestie, as cunning. I remember one said,\\nthere was no Sallets in the lines, to make the matter sauory;\\nnor no matter in the phrase, that might indite the\\nAuthor of affectation, but cal'd it an honest method. One\\ncheefe Speech in it, I cheefely lou'd, 'twas Aeneas Tale\\nto Dido, and thereabout of it especially, where he speaks\\nof Priams slaughter. If it liue in your memory, begin at\\nthis Line, let me see, let me see: The rugged Pyrrhus like\\nth'Hyrcanian Beast. It is not so: it begins with Pyrrhus\\nThe rugged Pyrrhus, he whose Sable Armes\\nBlacke as his purpose, did the night resemble\\nWhen he lay couched in the Ominous Horse,\\nHath now this dread and blacke Complexion smear'd\\nWith Heraldry more dismall: Head to foote\\nNow is he to take Geulles, horridly Trick'd\\nWith blood of Fathers, Mothers, Daughters, Sonnes,\\nBak'd and impasted with the parching streets,\\nThat lend a tyrannous, and damned light\\nTo their vilde Murthers, roasted in wrath and fire,\\nAnd thus o're-sized with coagulate gore,\\nWith eyes like Carbuncles, the hellish Pyrrhus\\nOlde Grandsire Priam seekes\\n\\n   Pol. Fore God, my Lord, well spoken, with good accent,\\nand good discretion\\n\\n   1.Player. Anon he findes him,\\nStriking too short at Greekes. His anticke Sword,\\nRebellious to his Arme, lyes where it falles\\nRepugnant to command: vnequall match,\\nPyrrhus at Priam driues, in Rage strikes wide:\\nBut with the whiffe and winde of his fell Sword,\\nTh' vnnerued Father fals. Then senselesse Illium,\\nSeeming to feele his blow, with flaming top\\nStoopes to his Bace, and with a hideous crash\\nTakes Prisoner Pyrrhus eare. For loe, his Sword\\nWhich was declining on the Milkie head\\nOf Reuerend Priam, seem'd i'th' Ayre to sticke:\\nSo as a painted Tyrant Pyrrhus stood,\\nAnd like a Newtrall to his will and matter, did nothing.\\nBut as we often see against some storme,\\nA silence in the Heauens, the Racke stand still,\\nThe bold windes speechlesse, and the Orbe below\\nAs hush as death: Anon the dreadfull Thunder\\nDoth rend the Region. So after Pyrrhus pause,\\nA rowsed Vengeance sets him new a-worke,\\nAnd neuer did the Cyclops hammers fall\\nOn Mars his Armours, forg'd for proofe Eterne,\\nWith lesse remorse then Pyrrhus bleeding sword\\nNow falles on Priam.\\nOut, out, thou Strumpet-Fortune, all you Gods,\\nIn generall Synod take away her power:\\nBreake all the Spokes and Fallies from her wheele,\\nAnd boule the round Naue downe the hill of Heauen,\\nAs low as to the Fiends\\n\\n   Pol. This is too long\\n\\n   Ham. It shall to'th Barbars, with your beard. Prythee\\nsay on: He's for a Iigge, or a tale of Baudry, or hee\\nsleepes. Say on; come to Hecuba\\n\\n   1.Play. But who, O who, had seen the inobled Queen\\n\\n   Ham. The inobled Queene?\\n  Pol. That's good: Inobled Queene is good\\n\\n   1.Play. Run bare-foot vp and downe,\\nThreatning the flame\\nWith Bisson Rheume: A clout about that head,\\nWhere late the Diadem stood, and for a Robe\\nAbout her lanke and all ore-teamed Loines,\\nA blanket in th' Alarum of feare caught vp.\\nWho this had seene, with tongue in Venome steep'd,\\n'Gainst Fortunes State, would Treason haue pronounc'd?\\nBut if the Gods themselues did see her then,\\nWhen she saw Pyrrhus make malicious sport\\nIn mincing with his Sword her Husbands limbes,\\nThe instant Burst of Clamour that she made\\n(Vnlesse things mortall moue them not at all)\\nWould haue made milche the Burning eyes of Heauen,\\nAnd passion in the Gods\\n\\n   Pol. Looke where he ha's not turn'd his colour, and\\nha's teares in's eyes. Pray you no more\\n\\n   Ham. 'Tis well, Ile haue thee speake out the rest,\\nsoone. Good my Lord, will you see the Players wel bestow'd.\\nDo ye heare, let them be well vs'd: for they are\\nthe Abstracts and breefe Chronicles of the time. After\\nyour death, you were better haue a bad Epitaph, then\\ntheir ill report while you liued\\n\\n   Pol. My Lord, I will vse them according to their desart\\n\\n   Ham. Gods bodykins man, better. Vse euerie man\\nafter his desart, and who should scape whipping: vse\\nthem after your own Honor and Dignity. The lesse they\\ndeserue, the more merit is in your bountie. Take them\\nin\\n\\n   Pol. Come sirs.\\n\\nExit Polon.\\n\\n  Ham. Follow him Friends: wee'l heare a play to morrow.\\nDost thou heare me old Friend, can you play the\\nmurther of Gonzago?\\n  Play. I my Lord\\n\\n   Ham. Wee'l ha't to morrow night. You could for a\\nneed study a speech of some dosen or sixteene lines, which\\nI would set downe, and insert in't? Could ye not?\\n  Play. I my Lord\\n\\n   Ham. Very well. Follow that Lord, and looke you\\nmock him not. My good Friends, Ile leaue you til night\\nyou are welcome to Elsonower?\\n  Rosin. Good my Lord.\\n\\nExeunt.\\n\\nManet Hamlet.\\n\\n  Ham. I so, God buy'ye: Now I am alone.\\nOh what a Rogue and Pesant slaue am I?\\nIs it not monstrous that this Player heere,\\nBut in a Fixion, in a dreame of Passion,\\nCould force his soule so to his whole conceit,\\nThat from her working, all his visage warm'd;\\nTeares in his eyes, distraction in's Aspect,\\nA broken voyce, and his whole Function suiting\\nWith Formes, to his Conceit? And all for nothing?\\nFor Hecuba?\\nWhat's Hecuba to him, or he to Hecuba,\\nThat he should weepe for her? What would he doe,\\nHad he the Motiue and the Cue for passion\\nThat I haue? He would drowne the Stage with teares,\\nAnd cleaue the generall eare with horrid speech:\\nMake mad the guilty, and apale the free,\\nConfound the ignorant, and amaze indeed,\\nThe very faculty of Eyes and Eares. Yet I,\\nA dull and muddy-metled Rascall, peake\\nLike Iohn a-dreames, vnpregnant of my cause,\\nAnd can say nothing: No, not for a King,\\nVpon whose property, and most deere life,\\nA damn'd defeate was made. Am I a Coward?\\nWho calles me Villaine? breakes my pate a-crosse?\\nPluckes off my Beard, and blowes it in my face?\\nTweakes me by'th' Nose? giues me the Lye i'th' Throate,\\nAs deepe as to the Lungs? Who does me this?\\nHa? Why I should take it: for it cannot be,\\nBut I am Pigeon-Liuer'd, and lacke Gall\\nTo make Oppression bitter, or ere this,\\nI should haue fatted all the Region Kites\\nWith this Slaues Offall, bloudy: a Bawdy villaine,\\nRemorselesse, Treacherous, Letcherous, kindles villaine!\\nOh Vengeance!\\nWho? What an Asse am I? I sure, this is most braue,\\nThat I, the Sonne of the Deere murthered,\\nPrompted to my Reuenge by Heauen, and Hell,\\nMust (like a Whore) vnpacke my heart with words,\\nAnd fall a Cursing like a very Drab.\\nA Scullion? Fye vpon't: Foh. About my Braine.\\nI haue heard, that guilty Creatures sitting at a Play,\\nHaue by the very cunning of the Scoene,\\nBene strooke so to the soule, that presently\\nThey haue proclaim'd their Malefactions.\\nFor Murther, though it haue no tongue, will speake\\nWith most myraculous Organ. Ile haue these Players,\\nPlay something like the murder of my Father,\\nBefore mine Vnkle. Ile obserue his lookes,\\nIle rent him to the quicke: If he but blench\\nI know my course. The Spirit that I haue seene\\nMay be the Diuell, and the Diuel hath power\\nT' assume a pleasing shape, yea and perhaps\\nOut of my Weaknesse, and my Melancholly,\\nAs he is very potent with such Spirits,\\nAbuses me to damne me. Ile haue grounds\\nMore Relatiue then this: The Play's the thing,\\nWherein Ile catch the Conscience of the King.\\n\\nExit\\n\\nEnter King, Queene, Polonius, Ophelia, Rosincrance,\\nGuildenstern, and\\nLords.\\n\\n  King. And can you by no drift of circumstance\\nGet from him why he puts on this Confusion:\\nGrating so harshly all his dayes of quiet\\nWith turbulent and dangerous Lunacy\\n\\n   Rosin. He does confesse he feeles himselfe distracted,\\nBut from what cause he will by no meanes speake\\n\\n   Guil. Nor do we finde him forward to be sounded,\\nBut with a crafty Madnesse keepes aloofe:\\nWhen we would bring him on to some Confession\\nOf his true state\\n\\n   Qu. Did he receiue you well?\\n  Rosin. Most like a Gentleman\\n\\n   Guild. But with much forcing of his disposition\\n\\n   Rosin. Niggard of question, but of our demands\\nMost free in his reply\\n\\n   Qu. Did you assay him to any pastime?\\n  Rosin. Madam, it so fell out, that certaine Players\\nWe ore-wrought on the way: of these we told him,\\nAnd there did seeme in him a kinde of ioy\\nTo heare of it: They are about the Court,\\nAnd (as I thinke) they haue already order\\nThis night to play before him\\n\\n   Pol. 'Tis most true:\\nAnd he beseech'd me to intreate your Maiesties\\nTo heare, and see the matter\\n\\n   King. With all my heart, and it doth much content me\\nTo heare him so inclin'd. Good Gentlemen,\\nGiue him a further edge, and driue his purpose on\\nTo these delights\\n\\n   Rosin. We shall my Lord.\\n\\nExeunt.\\n\\n  King. Sweet Gertrude leaue vs too,\\nFor we haue closely sent for Hamlet hither,\\nThat he, as 'twere by accident, may there\\nAffront Ophelia. Her Father, and my selfe (lawful espials)\\nWill so bestow our selues, that seeing vnseene\\nWe may of their encounter frankely iudge,\\nAnd gather by him, as he is behaued,\\nIf't be th' affliction of his loue, or no.\\nThat thus he suffers for\\n\\n   Qu. I shall obey you,\\nAnd for your part Ophelia, I do wish\\nThat your good Beauties be the happy cause\\nOf Hamlets wildenesse: so shall I hope your Vertues\\nWill bring him to his wonted way againe,\\nTo both your Honors\\n\\n   Ophe. Madam, I wish it may\\n\\n   Pol. Ophelia, walke you heere. Gracious so please ye\\nWe will bestow our selues: Reade on this booke,\\nThat shew of such an exercise may colour\\nYour lonelinesse. We are oft too blame in this,\\n'Tis too much prou'd, that with Deuotions visage,\\nAnd pious Action, we do surge o're\\nThe diuell himselfe\\n\\n   King. Oh 'tis true:\\nHow smart a lash that speech doth giue my Conscience?\\nThe Harlots Cheeke beautied with plaist'ring Art\\nIs not more vgly to the thing that helpes it,\\nThen is my deede, to my most painted word.\\nOh heauie burthen!\\n  Pol. I heare him comming, let's withdraw my Lord.\\n\\nExeunt.\\n\\nEnter Hamlet.\\n\\n  Ham. To be, or not to be, that is the Question:\\nWhether 'tis Nobler in the minde to suffer\\nThe Slings and Arrowes of outragious Fortune,\\nOr to take Armes against a Sea of troubles,\\nAnd by opposing end them: to dye, to sleepe\\nNo more; and by a sleepe, to say we end\\nThe Heart-ake, and the thousand Naturall shockes\\nThat Flesh is heyre too? 'Tis a consummation\\nDeuoutly to be wish'd. To dye to sleepe,\\nTo sleepe, perchance to Dreame; I, there's the rub,\\nFor in that sleepe of death, what dreames may come,\\nWhen we haue shuffel'd off this mortall coile,\\nMust giue vs pawse. There's the respect\\nThat makes Calamity of so long life:\\nFor who would beare the Whips and Scornes of time,\\nThe Oppressors wrong, the poore mans Contumely,\\nThe pangs of dispriz'd Loue, the Lawes delay,\\nThe insolence of Office, and the Spurnes\\nThat patient merit of the vnworthy takes,\\nWhen he himselfe might his Quietus make\\nWith a bare Bodkin? Who would these Fardles beare\\nTo grunt and sweat vnder a weary life,\\nBut that the dread of something after death,\\nThe vndiscouered Countrey, from whose Borne\\nNo Traueller returnes, Puzels the will,\\nAnd makes vs rather beare those illes we haue,\\nThen flye to others that we know not of.\\nThus Conscience does make Cowards of vs all,\\nAnd thus the Natiue hew of Resolution\\nIs sicklied o're, with the pale cast of Thought,\\nAnd enterprizes of great pith and moment,\\nWith this regard their Currants turne away,\\nAnd loose the name of Action. Soft you now,\\nThe faire Ophelia? Nimph, in thy Orizons\\nBe all my sinnes remembred\\n\\n   Ophe. Good my Lord,\\nHow does your Honor for this many a day?\\n  Ham. I humbly thanke you: well, well, well\\n\\n   Ophe. My Lord, I haue Remembrances of yours,\\nThat I haue longed long to re-deliuer.\\nI pray you now, receiue them\\n\\n   Ham. No, no, I neuer gaue you ought\\n\\n   Ophe. My honor'd Lord, I know right well you did,\\nAnd with them words of so sweet breath compos'd,\\nAs made the things more rich, then perfume left:\\nTake these againe, for to the Noble minde\\nRich gifts wax poore, when giuers proue vnkinde.\\nThere my Lord\\n\\n   Ham. Ha, ha: Are you honest?\\n  Ophe. My Lord\\n\\n   Ham. Are you faire?\\n  Ophe. What meanes your Lordship?\\n  Ham. That if you be honest and faire, your Honesty\\nshould admit no discourse to your Beautie\\n\\n   Ophe. Could Beautie my Lord, haue better Comerce\\nthen your Honestie?\\n  Ham. I trulie: for the power of Beautie, will sooner\\ntransforme Honestie from what is, to a Bawd, then the\\nforce of Honestie can translate Beautie into his likenesse.\\nThis was sometime a Paradox, but now the time giues it\\nproofe. I did loue you once\\n\\n   Ophe. Indeed my Lord, you made me beleeue so\\n\\n   Ham. You should not haue beleeued me. For vertue\\ncannot so innocculate our old stocke, but we shall rellish\\nof it. I loued you not\\n\\n   Ophe. I was the more deceiued\\n\\n   Ham. Get thee to a Nunnerie. Why would'st thou\\nbe a breeder of Sinners? I am my selfe indifferent honest,\\nbut yet I could accuse me of such things, that it were better\\nmy Mother had not borne me. I am very prowd, reuengefull,\\nAmbitious, with more offences at my becke,\\nthen I haue thoughts to put them in imagination, to giue\\nthem shape, or time to acte them in. What should such\\nFellowes as I do, crawling betweene Heauen and Earth.\\nWe are arrant Knaues all, beleeue none of vs. Goe thy\\nwayes to a Nunnery. Where's your Father?\\n  Ophe. At home, my Lord\\n\\n   Ham. Let the doores be shut vpon him, that he may\\nplay the Foole no way, but in's owne house. Farewell\\n\\n   Ophe. O helpe him, you sweet Heauens\\n\\n   Ham. If thou doest Marry, Ile giue thee this Plague\\nfor thy Dowrie. Be thou as chast as Ice, as pure as Snow,\\nthou shalt not escape Calumny. Get thee to a Nunnery.\\nGo, Farewell. Or if thou wilt needs Marry, marry a fool:\\nfor Wise men know well enough, what monsters you\\nmake of them. To a Nunnery go, and quickly too. Farwell\\n\\n   Ophe. O heauenly Powers, restore him\\n\\n   Ham. I haue heard of your pratlings too wel enough.\\nGod has giuen you one pace, and you make your selfe another:\\nyou gidge, you amble, and you lispe, and nickname\\nGods creatures, and make your Wantonnesse, your Ignorance.\\nGo too, Ile no more on't, it hath made me mad.\\nI say, we will haue no more Marriages. Those that are\\nmarried already, all but one shall liue, the rest shall keep\\nas they are. To a Nunnery, go.\\n\\nExit Hamlet.\\n\\n  Ophe. O what a Noble minde is heere o're-throwne?\\nThe Courtiers, Soldiers, Schollers: Eye, tongue, sword,\\nTh' expectansie and Rose of the faire State,\\nThe glasse of Fashion, and the mould of Forme,\\nTh' obseru'd of all Obseruers, quite, quite downe.\\nHaue I of Ladies most deiect and wretched,\\nThat suck'd the Honie of his Musicke Vowes:\\nNow see that Noble, and most Soueraigne Reason,\\nLike sweet Bels iangled out of tune, and harsh,\\nThat vnmatch'd Forme and Feature of blowne youth,\\nBlasted with extasie. Oh woe is me,\\nT'haue seene what I haue seene: see what I see.\\nEnter King, and Polonius.\\n\\n  King. Loue? His affections do not that way tend,\\nNor what he spake, though it lack'd Forme a little,\\nWas not like Madnesse. There's something in his soule?\\nO're which his Melancholly sits on brood,\\nAnd I do doubt the hatch, and the disclose\\nWill be some danger, which to preuent\\nI haue in quicke determination\\nThus set it downe. He shall with speed to England\\nFor the demand of our neglected Tribute:\\nHaply the Seas and Countries different\\nWith variable Obiects, shall expell\\nThis something setled matter in his heart:\\nWhereon his Braines still beating, puts him thus\\nFrom fashion of himselfe. What thinke you on't?\\n  Pol. It shall do well. But yet do I beleeue\\nThe Origin and Commencement of this greefe\\nSprung from neglected loue. How now Ophelia?\\nYou neede not tell vs, what Lord Hamlet saide,\\nWe heard it all. My Lord, do as you please,\\nBut if you hold it fit after the Play,\\nLet his Queene Mother all alone intreat him\\nTo shew his Greefes: let her be round with him,\\nAnd Ile be plac'd so, please you in the eare\\nOf all their Conference. If she finde him not,\\nTo England send him: Or confine him where\\nYour wisedome best shall thinke\\n\\n   King. It shall be so:\\nMadnesse in great Ones, must not vnwatch'd go.\\n\\nExeunt.\\n\\nEnter Hamlet, and two or three of the Players.\\n\\n  Ham. Speake the Speech I pray you, as I pronounc'd\\nit to you trippingly on the Tongue: But if you mouth it,\\nas many of your Players do, I had as liue the Town-Cryer\\nhad spoke my Lines: Nor do not saw the Ayre too much\\nyour hand thus, but vse all gently; for in the verie Torrent,\\nTempest, and (as I say) the Whirle-winde of\\nPassion, you must acquire and beget a Temperance that\\nmay giue it Smoothnesse. O it offends mee to the Soule,\\nto see a robustious Pery-wig-pated Fellow, teare a Passion\\nto tatters, to verie ragges, to split the eares of the\\nGroundlings: who (for the most part) are capeable of\\nnothing, but inexplicable dumbe shewes, & noise: I could\\nhaue such a Fellow whipt for o're-doing Termagant: it\\noutHerod's Herod. Pray you auoid it\\n\\n   Player. I warrant your Honor\\n\\n   Ham. Be not too tame neyther: but let your owne\\nDiscretion be your Tutor. Sute the Action to the Word,\\nthe Word to the Action, with this speciall obseruance:\\nThat you ore-stop not the modestie of Nature; for any\\nthing so ouer-done, is fro[m] the purpose of Playing, whose\\nend both at the first and now, was and is, to hold as 'twer\\nthe Mirrour vp to Nature; to shew Vertue her owne\\nFeature, Scorne her owne Image, and the verie Age and\\nBodie of the Time, his forme and pressure. Now, this\\nouer-done, or come tardie off, though it make the vnskilfull\\nlaugh, cannot but make the Iudicious greeue; The\\ncensure of the which One, must in your allowance o'reway\\na whole Theater of Others. Oh, there bee Players\\nthat I haue seene Play, and heard others praise, and that\\nhighly (not to speake it prophanely) that neyther hauing\\nthe accent of Christians, nor the gate of Christian, Pagan,\\nor Norman, haue so strutted and bellowed, that I haue\\nthought some of Natures Iouerney-men had made men,\\nand not made them well, they imitated Humanity so abhominably\\n\\n   Play. I hope we haue reform'd that indifferently with\\nvs, Sir\\n\\n   Ham. O reforme it altogether. And let those that\\nplay your Clownes, speake no more then is set downe for\\nthem. For there be of them, that will themselues laugh,\\nto set on some quantitie of barren Spectators to laugh\\ntoo, though in the meane time, some necessary Question\\nof the Play be then to be considered: that's Villanous, &\\nshewes a most pittifull Ambition in the Foole that vses\\nit. Go make you readie.\\n\\nExit Players.\\n\\nEnter Polonius, Rosincrance, and Guildensterne.\\n\\nHow now my Lord,\\nWill the King heare this peece of Worke?\\n  Pol. And the Queene too, and that presently\\n\\n   Ham. Bid the Players make hast.\\n\\nExit Polonius.\\n\\nWill you two helpe to hasten them?\\n  Both. We will my Lord.\\n\\nExeunt.\\n\\nEnter Horatio.\\n\\n  Ham. What hoa, Horatio?\\n  Hora. Heere sweet Lord, at your Seruice\\n\\n   Ham. Horatio, thou art eene as iust a man\\nAs ere my Conuersation coap'd withall\\n\\n   Hora. O my deere Lord\\n\\n   Ham. Nay, do not thinke I flatter:\\nFor what aduancement may I hope from thee,\\nThat no Reuennew hast, but thy good spirits\\nTo feed & cloath thee. Why shold the poor be flatter'd?\\nNo, let the Candied tongue, like absurd pompe,\\nAnd crooke the pregnant Hindges of the knee,\\nWhere thrift may follow faining? Dost thou heare,\\nSince my deere Soule was Mistris of my choyse,\\nAnd could of men distinguish, her election\\nHath seal'd thee for her selfe. For thou hast bene\\nAs one in suffering all, that suffers nothing.\\nA man that Fortunes buffets, and Rewards\\nHath 'tane with equall Thankes. And blest are those,\\nWhose Blood and Iudgement are so well co-mingled,\\nThat they are not a Pipe for Fortunes finger.\\nTo sound what stop she please. Giue me that man,\\nThat is not Passions Slaue, and I will weare him\\nIn my hearts Core. I, in my Heart of heart,\\nAs I do thee. Something too much of this.\\nThere is a Play to night to before the King.\\nOne Scoene of it comes neere the Circumstance\\nWhich I haue told thee, of my Fathers death.\\nI prythee, when thou see'st that Acte a-foot,\\nEuen with the verie Comment of my Soule\\nObserue mine Vnkle: If his occulted guilt,\\nDo not it selfe vnkennell in one speech,\\nIt is a damned Ghost that we haue seene:\\nAnd my Imaginations are as foule\\nAs Vulcans Stythe. Giue him needfull note,\\nFor I mine eyes will riuet to his Face:\\nAnd after we will both our iudgements ioyne,\\nTo censure of his seeming\\n\\n   Hora. Well my Lord.\\nIf he steale ought the whil'st this Play is Playing,\\nAnd scape detecting, I will pay the Theft.\\nEnter King, Queene, Polonius, Ophelia, Rosincrance,\\nGuildensterne, and\\nother Lords attendant with his Guard carrying Torches. Danish\\nMarch. Sound\\na Flourish.\\n\\n  Ham. They are comming to the Play: I must be idle.\\nGet you a place\\n\\n   King. How fares our Cosin Hamlet?\\n  Ham. Excellent Ifaith, of the Camelions dish: I eate\\nthe Ayre promise-cramm'd, you cannot feed Capons so\\n\\n   King. I haue nothing with this answer Hamlet, these\\nwords are not mine\\n\\n   Ham. No, nor mine. Now my Lord, you plaid once\\ni'th' Vniuersity, you say?\\n  Polon. That I did my Lord, and was accounted a good\\nActor\\n\\n   Ham. And what did you enact?\\n  Pol. I did enact Iulius Caesar, I was kill'd i'th' Capitol:\\nBrutus kill'd me\\n\\n   Ham. It was a bruite part of him, to kill so Capitall a\\nCalfe there. Be the Players ready?\\n  Rosin. I my Lord, they stay vpon your patience\\n\\n   Qu. Come hither my good Hamlet, sit by me\\n\\n   Ha. No good Mother, here's Mettle more attractiue\\n\\n   Pol. Oh ho, do you marke that?\\n  Ham. Ladie, shall I lye in your Lap?\\n  Ophe. No my Lord\\n\\n   Ham. I meane, my Head vpon your Lap?\\n  Ophe. I my Lord\\n\\n   Ham. Do you thinke I meant Country matters?\\n  Ophe. I thinke nothing, my Lord\\n\\n   Ham. That's a faire thought to ly betweene Maids legs\\n  Ophe. What is my Lord?\\n  Ham. Nothing\\n\\n   Ophe. You are merrie, my Lord?\\n  Ham. Who I?\\n  Ophe. I my Lord\\n\\n   Ham. Oh God, your onely Iigge-maker: what should\\na man do, but be merrie. For looke you how cheerefully\\nmy Mother lookes, and my Father dyed within's two\\nHoures\\n\\n   Ophe. Nay, 'tis twice two moneths, my Lord\\n\\n   Ham. So long? Nay then let the Diuel weare blacke,\\nfor Ile haue a suite of Sables. Oh Heauens! dye two moneths\\nago, and not forgotten yet? Then there's hope, a\\ngreat mans Memorie, may out-liue his life halfe a yeare:\\nBut byrlady he must builde Churches then: or else shall\\nhe suffer not thinking on, with the Hoby-horsse, whose\\nEpitaph is, For o, For o, the Hoby-horse is forgot.\\n\\nHoboyes play. The dumbe shew enters.\\n\\nEnter a King and Queene, very louingly; the Queene embracing\\nhim. She\\nkneeles, and makes shew of Protestation vnto him. He takes her\\nvp, and\\ndeclines his head vpon her neck. Layes him downe vpon a Banke\\nof Flowers.\\nShe seeing him a-sleepe, leaues him. Anon comes in a Fellow,\\ntakes off his\\nCrowne, kisses it, and powres poyson in the Kings eares, and\\nExits. The\\nQueene returnes, findes the King dead, and makes passionate\\nAction. The\\nPoysoner, with some two or three Mutes comes in againe, seeming\\nto lament\\nwith her. The dead body is carried away: The Poysoner Wooes the\\nQueene with\\nGifts, she seemes loath and vnwilling awhile, but in the end,\\naccepts his\\nloue.\\n\\nExeunt.\\n\\n  Ophe. What meanes this, my Lord?\\n  Ham. Marry this is Miching Malicho, that meanes\\nMischeefe\\n\\n   Ophe. Belike this shew imports the Argument of the\\nPlay?\\n  Ham. We shall know by these Fellowes: the Players\\ncannot keepe counsell, they'l tell all\\n\\n   Ophe. Will they tell vs what this shew meant?\\n  Ham. I, or any shew that you'l shew him. Bee not\\nyou asham'd to shew, hee'l not shame to tell you what it\\nmeanes\\n\\n   Ophe. You are naught, you are naught, Ile marke the\\nPlay.\\nEnter Prologue.\\n\\nFor vs, and for our Tragedie,\\nHeere stooping to your Clemencie:\\nWe begge your hearing Patientlie\\n\\n   Ham. Is this a Prologue, or the Poesie of a Ring?\\n  Ophe. 'Tis briefe my Lord\\n\\n   Ham. As Womans loue.\\nEnter King and his Queene.\\n\\n  King. Full thirtie times hath Phoebus Cart gon round,\\nNeptunes salt Wash, and Tellus Orbed ground:\\nAnd thirtie dozen Moones with borrowed sheene,\\nAbout the World haue times twelue thirties beene,\\nSince loue our hearts, and Hymen did our hands\\nVnite comutuall, in most sacred Bands\\n\\n   Bap. So many iournies may the Sunne and Moone\\nMake vs againe count o're, ere loue be done.\\nBut woe is me, you are so sicke of late,\\nSo farre from cheere, and from your former state,\\nThat I distrust you: yet though I distrust,\\nDiscomfort you (my Lord) it nothing must:\\nFor womens Feare and Loue, holds quantitie,\\nIn neither ought, or in extremity:\\nNow what my loue is, proofe hath made you know,\\nAnd as my Loue is siz'd, my Feare is so\\n\\n   King. Faith I must leaue thee Loue, and shortly too:\\nMy operant Powers my Functions leaue to do:\\nAnd thou shalt liue in this faire world behinde,\\nHonour'd, belou'd, and haply, one as kinde.\\nFor Husband shalt thou-\\n  Bap. Oh confound the rest:\\nSuch Loue, must needs be Treason in my brest:\\nIn second Husband, let me be accurst,\\nNone wed the second, but who kill'd the first\\n\\n   Ham. Wormwood, Wormwood\\n\\n   Bapt. The instances that second Marriage moue,\\nAre base respects of Thrift, but none of Loue.\\nA second time, I kill my Husband dead,\\nWhen second Husband kisses me in Bed\\n\\n   King. I do beleeue you. Think what now you speak:\\nBut what we do determine, oft we breake:\\nPurpose is but the slaue to Memorie,\\nOf violent Birth, but poore validitie:\\nWhich now like Fruite vnripe stickes on the Tree,\\nBut fall vnshaken, when they mellow bee.\\nMost necessary 'tis, that we forget\\nTo pay our selues, what to our selues is debt:\\nWhat to our selues in passion we propose,\\nThe passion ending, doth the purpose lose.\\nThe violence of other Greefe or Ioy,\\nTheir owne ennactors with themselues destroy:\\nWhere Ioy most Reuels, Greefe doth most lament;\\nGreefe ioyes, Ioy greeues on slender accident.\\nThis world is not for aye, nor 'tis not strange\\nThat euen our Loues should with our Fortunes change.\\nFor 'tis a question left vs yet to proue,\\nWhether Loue lead Fortune, or else Fortune Loue.\\nThe great man downe, you marke his fauourites flies,\\nThe poore aduanc'd, makes Friends of Enemies:\\nAnd hitherto doth Loue on Fortune tend,\\nFor who not needs, shall neuer lacke a Frend:\\nAnd who in want a hollow Friend doth try,\\nDirectly seasons him his Enemie.\\nBut orderly to end, where I begun,\\nOur Willes and Fates do so contrary run,\\nThat our Deuices still are ouerthrowne,\\nOur thoughts are ours, their ends none of our owne.\\nSo thinke thou wilt no second Husband wed.\\nBut die thy thoughts, when thy first Lord is dead\\n\\n   Bap. Nor Earth to giue me food, nor Heauen light,\\nSport and repose locke from me day and night:\\nEach opposite that blankes the face of ioy,\\nMeet what I would haue well, and it destroy:\\nBoth heere, and hence, pursue me lasting strife,\\nIf once a Widdow, euer I be Wife\\n\\n   Ham. If she should breake it now\\n\\n   King. 'Tis deepely sworne:\\nSweet, leaue me heere a while,\\nMy spirits grow dull, and faine I would beguile\\nThe tedious day with sleepe\\n\\n   Qu. Sleepe rocke thy Braine,\\n\\nSleepes\\n\\nAnd neuer come mischance betweene vs twaine.\\n\\nExit\\n\\n  Ham. Madam, how like you this Play?\\n  Qu. The Lady protests to much me thinkes\\n\\n   Ham. Oh but shee'l keepe her word\\n\\n   King. Haue you heard the Argument, is there no Offence\\nin't?\\n  Ham. No, no, they do but iest, poyson in iest, no Offence\\ni'th' world\\n\\n   King. What do you call the Play?\\n  Ham. The Mouse-trap: Marry how? Tropically:\\nThis Play is the Image of a murder done in Vienna: Gonzago\\nis the Dukes name, his wife Baptista: you shall see\\nanon: 'tis a knauish peece of worke: But what o'that?\\nYour Maiestie, and wee that haue free soules, it touches\\nvs not: let the gall'd iade winch: our withers are vnrung.\\nEnter Lucianus.\\n\\nThis is one Lucianus nephew to the King\\n\\n   Ophe. You are a good Chorus, my Lord\\n\\n   Ham. I could interpret betweene you and your loue:\\nif I could see the Puppets dallying\\n\\n   Ophe. You are keene my Lord, you are keene\\n\\n   Ham. It would cost you a groaning, to take off my\\nedge\\n\\n   Ophe. Still better and worse\\n\\n   Ham. So you mistake Husbands.\\nBegin Murderer. Pox, leaue thy damnable Faces, and\\nbegin. Come, the croaking Rauen doth bellow for Reuenge\\n\\n   Lucian. Thoughts blacke, hands apt,\\nDrugges fit, and Time agreeing:\\nConfederate season, else, no Creature seeing:\\nThou mixture ranke, of Midnight Weeds collected,\\nWith Hecats Ban, thrice blasted, thrice infected,\\nThy naturall Magicke, and dire propertie,\\nOn wholsome life, vsurpe immediately.\\n\\nPowres the poyson in his eares.\\n\\n  Ham. He poysons him i'th' Garden for's estate: His\\nname's Gonzago: the Story is extant and writ in choyce\\nItalian. You shall see anon how the Murtherer gets the\\nloue of Gonzago's wife\\n\\n   Ophe. The King rises\\n\\n   Ham. What, frighted with false fire\\n\\n   Qu. How fares my Lord?\\n  Pol. Giue o're the Play\\n\\n   King. Giue me some Light. Away\\n\\n   All. Lights, Lights, Lights.\\n\\nExeunt.\\n\\nManet Hamlet & Horatio.\\n\\n  Ham. Why let the strucken Deere go weepe,\\nThe Hart vngalled play:\\nFor some must watch, while some must sleepe;\\nSo runnes the world away.\\nWould not this Sir, and a Forrest of Feathers, if the rest of\\nmy Fortunes turne Turke with me; with two Prouinciall\\nRoses on my rac'd Shooes, get me a Fellowship in a crie\\nof Players sir\\n\\n   Hor. Halfe a share\\n\\n   Ham. A whole one I,\\nFor thou dost know: Oh Damon deere,\\nThis Realme dismantled was of Ioue himselfe,\\nAnd now reignes heere.\\nA verie verie Paiocke\\n\\n   Hora. You might haue Rim'd\\n\\n   Ham. Oh good Horatio, Ile take the Ghosts word for\\na thousand pound. Did'st perceiue?\\n  Hora. Verie well my Lord\\n\\n   Ham. Vpon the talke of the poysoning?\\n  Hora. I did verie well note him.\\nEnter Rosincrance and Guildensterne.\\n\\n  Ham. Oh, ha? Come some Musick. Come y Recorders:\\nFor if the King like not the Comedie,\\nWhy then belike he likes it not perdie.\\nCome some Musicke\\n\\n   Guild. Good my Lord, vouchsafe me a word with you\\n\\n   Ham. Sir, a whole History\\n\\n   Guild. The King, sir\\n\\n   Ham. I sir, what of him?\\n  Guild. Is in his retyrement, maruellous distemper'd\\n\\n   Ham. With drinke Sir?\\n  Guild. No my Lord, rather with choller\\n\\n   Ham. Your wisedome should shew it selfe more richer,\\nto signifie this to his Doctor: for for me to put him\\nto his Purgation, would perhaps plundge him into farre\\nmore Choller\\n\\n   Guild. Good my Lord put your discourse into some\\nframe, and start not so wildely from my affayre\\n\\n   Ham. I am tame Sir, pronounce\\n\\n   Guild. The Queene your Mother, in most great affliction\\nof spirit, hath sent me to you\\n\\n   Ham. You are welcome\\n\\n   Guild. Nay, good my Lord, this courtesie is not of\\nthe right breed. If it shall please you to make me a wholsome\\nanswer, I will doe your Mothers command'ment:\\nif not, your pardon, and my returne shall bee the end of\\nmy Businesse\\n\\n   Ham. Sir, I cannot\\n\\n   Guild. What, my Lord?\\n  Ham. Make you a wholsome answere: my wits diseas'd.\\nBut sir, such answers as I can make, you shal command:\\nor rather you say, my Mother: therfore no more\\nbut to the matter. My Mother you say\\n\\n   Rosin. Then thus she sayes: your behauior hath stroke\\nher into amazement, and admiration\\n\\n   Ham. Oh wonderfull Sonne, that can so astonish a\\nMother. But is there no sequell at the heeles of this Mothers\\nadmiration?\\n  Rosin. She desires to speake with you in her Closset,\\nere you go to bed\\n\\n   Ham. We shall obey, were she ten times our Mother.\\nHaue you any further Trade with vs?\\n  Rosin. My Lord, you once did loue me\\n\\n   Ham. So I do still, by these pickers and stealers\\n\\n   Rosin. Good my Lord, what is your cause of distemper?\\nYou do freely barre the doore of your owne Libertie,\\nif you deny your greefes to your Friend\\n\\n   Ham. Sir I lacke Aduancement\\n\\n   Rosin. How can that be, when you haue the voyce of\\nthe King himselfe, for your Succession in Denmarke?\\n  Ham. I, but while the grasse growes, the Prouerbe is\\nsomething musty.\\nEnter one with a Recorder.\\n\\nO the Recorder. Let me see, to withdraw with you, why\\ndo you go about to recouer the winde of mee, as if you\\nwould driue me into a toyle?\\n  Guild. O my Lord, if my Dutie be too bold, my loue\\nis too vnmannerly\\n\\n   Ham. I do not well vnderstand that. Will you play\\nvpon this Pipe?\\n  Guild. My Lord, I cannot\\n\\n   Ham. I pray you\\n\\n   Guild. Beleeue me, I cannot\\n\\n   Ham. I do beseech you\\n\\n   Guild. I know no touch of it, my Lord\\n\\n   Ham. 'Tis as easie as lying: gouerne these Ventiges\\nwith your finger and thumbe, giue it breath with your\\nmouth, and it will discourse most excellent Musicke.\\nLooke you, these are the stoppes\\n\\n   Guild. But these cannot I command to any vtterance\\nof hermony, I haue not the skill\\n\\n   Ham. Why looke you now, how vnworthy a thing\\nyou make of me: you would play vpon mee; you would\\nseeme to know my stops: you would pluck out the heart\\nof my Mysterie; you would sound mee from my lowest\\nNote, to the top of my Compasse: and there is much Musicke,\\nexcellent Voice, in this little Organe, yet cannot\\nyou make it. Why do you thinke, that I am easier to bee\\nplaid on, then a Pipe? Call me what Instrument you will,\\nthough you can fret me, you cannot play vpon me. God\\nblesse you Sir.\\nEnter Polonius.\\n\\n  Polon. My Lord; the Queene would speak with you,\\nand presently\\n\\n   Ham. Do you see that Clowd? that's almost in shape\\nlike a Camell\\n\\n   Polon. By'th' Masse, and it's like a Camell indeed\\n\\n   Ham. Me thinkes it is like a Weazell\\n\\n   Polon. It is back'd like a Weazell\\n\\n   Ham. Or like a Whale?\\n  Polon. Verie like a Whale\\n\\n   Ham. Then will I come to my Mother, by and by:\\nThey foole me to the top of my bent.\\nI will come by and by\\n\\n   Polon. I will say so.\\nEnter.\\n\\n  Ham. By and by, is easily said. Leaue me Friends:\\n'Tis now the verie witching time of night,\\nWhen Churchyards yawne, and Hell it selfe breaths out\\nContagion to this world. Now could I drink hot blood,\\nAnd do such bitter businesse as the day\\nWould quake to looke on. Soft now, to my Mother:\\nOh Heart, loose not thy Nature; let not euer\\nThe Soule of Nero, enter this firme bosome:\\nLet me be cruell, not vnnaturall,\\nI will speake Daggers to her, but vse none:\\nMy Tongue and Soule in this be Hypocrites.\\nHow in my words someuer she be shent,\\nTo giue them Seales, neuer my Soule consent.\\nEnter King, Rosincrance, and Guildensterne.\\n\\n  King. I like him not, nor stands it safe with vs,\\nTo let his madnesse range. Therefore prepare you,\\nI your Commission will forthwith dispatch,\\nAnd he to England shall along with you:\\nThe termes of our estate, may not endure\\nHazard so dangerous as doth hourely grow\\nOut of his Lunacies\\n\\n   Guild. We will our selues prouide:\\nMost holie and Religious feare it is\\nTo keepe those many many bodies safe\\nThat liue and feede vpon your Maiestie\\n\\n   Rosin. The single\\nAnd peculiar life is bound\\nWith all the strength and Armour of the minde,\\nTo keepe it selfe from noyance: but much more,\\nThat Spirit, vpon whose spirit depends and rests\\nThe liues of many, the cease of Maiestie\\nDies not alone; but like a Gulfe doth draw\\nWhat's neere it, with it. It is a massie wheele\\nFixt on the Somnet of the highest Mount.\\nTo whose huge Spoakes, ten thousand lesser things\\nAre mortiz'd and adioyn'd: which when it falles,\\nEach small annexment, pettie consequence\\nAttends the boystrous Ruine. Neuer alone\\nDid the King sighe, but with a generall grone\\n\\n   King. Arme you, I pray you to this speedie Voyage;\\nFor we will Fetters put vpon this feare,\\nWhich now goes too free-footed\\n\\n   Both. We will haste vs.\\n\\nExeunt. Gent.\\n\\nEnter Polonius.\\n\\n  Pol. My Lord, he's going to his Mothers Closset:\\nBehinde the Arras Ile conuey my selfe\\nTo heare the Processe. Ile warrant shee'l tax him home,\\nAnd as you said, and wisely was it said,\\n'Tis meete that some more audience then a Mother,\\nSince Nature makes them partiall, should o're-heare\\nThe speech of vantage. Fare you well my Liege,\\nIle call vpon you ere you go to bed,\\nAnd tell you what I know\\n\\n   King. Thankes deere my Lord.\\nOh my offence is ranke, it smels to heauen,\\nIt hath the primall eldest curse vpon't,\\nA Brothers murther. Pray can I not,\\nThough inclination be as sharpe as will:\\nMy stronger guilt, defeats my strong intent,\\nAnd like a man to double businesse bound,\\nI stand in pause where I shall first begin,\\nAnd both neglect; what if this cursed hand\\nWere thicker then it selfe with Brothers blood,\\nIs there not Raine enough in the sweet Heauens\\nTo wash it white as Snow? Whereto serues mercy,\\nBut to confront the visage of Offence?\\nAnd what's in Prayer, but this two-fold force,\\nTo be fore-stalled ere we come to fall,\\nOr pardon'd being downe? Then Ile looke vp,\\nMy fault is past. But oh, what forme of Prayer\\nCan serue my turne? Forgiue me my foule Murther:\\nThat cannot be, since I am still possest\\nOf those effects for which I did the Murther.\\nMy Crowne, mine owne Ambition, and my Queene:\\nMay one be pardon'd, and retaine th' offence?\\nIn the corrupted currants of this world,\\nOffences gilded hand may shoue by Iustice,\\nAnd oft 'tis seene, the wicked prize it selfe\\nBuyes out the Law; but 'tis not so aboue,\\nThere is no shuffling, there the Action lyes\\nIn his true Nature, and we our selues compell'd\\nEuen to the teeth and forehead of our faults,\\nTo giue in euidence. What then? What rests?\\nTry what Repentance can. What can it not?\\nYet what can it, when one cannot repent?\\nOh wretched state! Oh bosome, blacke as death!\\nOh limed soule, that strugling to be free,\\nArt more ingag'd: Helpe Angels, make assay:\\nBow stubborne knees, and heart with strings of Steele,\\nBe soft as sinewes of the new-borne Babe,\\nAll may be well.\\nEnter Hamlet.\\n\\n  Ham. Now might I do it pat, now he is praying,\\nAnd now Ile doo't, and so he goes to Heauen,\\nAnd so am I reueng'd: that would be scann'd,\\nA Villaine killes my Father, and for that\\nI his foule Sonne, do this same Villaine send\\nTo heauen. Oh this is hyre and Sallery, not Reuenge.\\nHe tooke my Father grossely, full of bread,\\nWith all his Crimes broad blowne, as fresh as May,\\nAnd how his Audit stands, who knowes, saue Heauen:\\nBut in our circumstance and course of thought\\n'Tis heauie with him: and am I then reueng'd,\\nTo take him in the purging of his Soule,\\nWhen he is fit and season'd for his passage? No.\\nVp Sword, and know thou a more horrid hent\\nWhen he is drunke asleepe: or in his Rage,\\nOr in th' incestuous pleasure of his bed,\\nAt gaming, swearing, or about some acte\\nThat ha's no rellish of Saluation in't,\\nThen trip him, that his heeles may kicke at Heauen,\\nAnd that his Soule may be as damn'd and blacke\\nAs Hell, whereto it goes. My Mother stayes,\\nThis Physicke but prolongs thy sickly dayes.\\nEnter.\\n\\n  King. My words flye vp, my thoughts remain below,\\nWords without thoughts, neuer to Heauen go.\\nEnter.\\n\\nEnter Queene and Polonius.\\n\\n  Pol. He will come straight:\\nLooke you lay home to him,\\nTell him his prankes haue been too broad to beare with,\\nAnd that your Grace hath screen'd, and stoode betweene\\nMuch heate, and him. Ile silence me e'ene heere:\\nPray you be round with him\\n\\n   Ham. within. Mother, mother, mother\\n\\n   Qu. Ile warrant you, feare me not.\\nWithdraw, I heare him coming.\\nEnter Hamlet.\\n\\n  Ham. Now Mother, what's the matter?\\n  Qu. Hamlet, thou hast thy Father much offended\\n\\n\\n   Ham. Mother, you haue my Father much offended\\n\\n   Qu. Come, come, you answer with an idle tongue\\n\\n   Ham. Go, go, you question with an idle tongue\\n\\n   Qu. Why how now Hamlet?\\n  Ham. Whats the matter now?\\n  Qu. Haue you forgot me?\\n  Ham. No by the Rood, not so:\\nYou are the Queene, your Husbands Brothers wife,\\nBut would you were not so. You are my Mother\\n\\n   Qu. Nay, then Ile set those to you that can speake\\n\\n   Ham. Come, come, and sit you downe, you shall not\\nboudge:\\nYou go not till I set you vp a glasse,\\nWhere you may see the inmost part of you?\\n  Qu. What wilt thou do? thou wilt not murther me?\\nHelpe, helpe, hoa\\n\\n   Pol. What hoa, helpe, helpe, helpe\\n\\n   Ham. How now, a Rat? dead for a Ducate, dead\\n\\n   Pol. Oh I am slaine.\\n\\nKilles Polonius\\n\\n   Qu. Oh me, what hast thou done?\\n  Ham. Nay I know not, is it the King?\\n  Qu. Oh what a rash, and bloody deed is this?\\n  Ham. A bloody deed, almost as bad good Mother,\\nAs kill a King, and marrie with his Brother\\n\\n   Qu. As kill a King?\\n  Ham. I Lady, 'twas my word.\\nThou wretched, rash, intruding foole farewell,\\nI tooke thee for thy Betters, take thy Fortune,\\nThou find'st to be too busie, is some danger.\\nLeaue wringing of your hands, peace, sit you downe,\\nAnd let me wring your heart, for so I shall\\nIf it be made of penetrable stuffe;\\nIf damned Custome haue not braz'd it so,\\nThat it is proofe and bulwarke against Sense\\n\\n   Qu. What haue I done, that thou dar'st wag thy tong,\\nIn noise so rude against me?\\n  Ham. Such an Act\\nThat blurres the grace and blush of Modestie,\\nCals Vertue Hypocrite, takes off the Rose\\nFrom the faire forehead of an innocent loue,\\nAnd makes a blister there. Makes marriage vowes\\nAs false as Dicers Oathes. Oh such a deed,\\nAs from the body of Contraction pluckes\\nThe very soule, and sweete Religion makes\\nA rapsidie of words. Heauens face doth glow,\\nYea this solidity and compound masse,\\nWith tristfull visage as against the doome,\\nIs thought-sicke at the act\\n\\n   Qu. Aye me; what act, that roares so lowd, & thunders\\nin the Index\\n\\n   Ham. Looke heere vpon this Picture, and on this,\\nThe counterfet presentment of two Brothers:\\nSee what a grace was seated on his Brow,\\nHyperions curles, the front of Ioue himselfe,\\nAn eye like Mars, to threaten or command\\nA Station, like the Herald Mercurie\\nNew lighted on a heauen-kissing hill:\\nA Combination, and a forme indeed,\\nWhere euery God did seeme to set his Seale,\\nTo giue the world assurance of a man.\\nThis was your Husband. Looke you now what followes.\\nHeere is your Husband, like a Mildew'd eare\\nBlasting his wholsom breath. Haue you eyes?\\nCould you on this faire Mountaine leaue to feed,\\nAnd batten on this Moore? Ha? Haue you eyes?\\nYou cannot call it Loue: For at your age,\\nThe hey-day in the blood is tame, it's humble,\\nAnd waites vpon the Iudgement: and what Iudgement\\nWould step from this, to this? What diuell was't,\\nThat thus hath cousend you at hoodman-blinde?\\nO Shame! where is thy Blush? Rebellious Hell,\\nIf thou canst mutine in a Matrons bones,\\nTo flaming youth, let Vertue be as waxe.\\nAnd melt in her owne fire. Proclaime no shame,\\nWhen the compulsiue Ardure giues the charge,\\nSince Frost it selfe, as actiuely doth burne,\\nAs Reason panders Will\\n\\n   Qu. O Hamlet, speake no more.\\nThou turn'st mine eyes into my very soule,\\nAnd there I see such blacke and grained spots,\\nAs will not leaue their Tinct\\n\\n   Ham. Nay, but to liue\\nIn the ranke sweat of an enseamed bed,\\nStew'd in Corruption; honying and making loue\\nOuer the nasty Stye\\n\\n   Qu. Oh speake to me, no more,\\nThese words like Daggers enter in mine eares.\\nNo more sweet Hamlet\\n\\n   Ham. A Murderer, and a Villaine:\\nA Slaue, that is not twentieth part the tythe\\nOf your precedent Lord. A vice of Kings,\\nA Cutpurse of the Empire and the Rule.\\nThat from a shelfe, the precious Diadem stole,\\nAnd put it in his Pocket\\n\\n   Qu. No more.\\nEnter Ghost.\\n\\n  Ham. A King of shreds and patches.\\nSaue me; and houer o're me with your wings\\nYou heauenly Guards. What would your gracious figure?\\n  Qu. Alas he's mad\\n\\n   Ham. Do you not come your tardy Sonne to chide,\\nThat laps't in Time and Passion, lets go by\\nTh' important acting of your dread command? Oh say\\n\\n   Ghost. Do not forget: this Visitation\\nIs but to whet thy almost blunted purpose.\\nBut looke, Amazement on thy Mother sits;\\nO step betweene her, and her fighting Soule,\\nConceit in weakest bodies, strongest workes.\\nSpeake to her Hamlet\\n\\n   Ham. How is it with you Lady?\\n  Qu. Alas, how is't with you?\\nThat you bend your eye on vacancie,\\nAnd with their corporall ayre do hold discourse.\\nForth at your eyes, your spirits wildely peepe,\\nAnd as the sleeping Soldiours in th' Alarme,\\nYour bedded haire, like life in excrements,\\nStart vp, and stand an end. Oh gentle Sonne,\\nVpon the heate and flame of thy distemper\\nSprinkle coole patience. Whereon do you looke?\\n  Ham. On him, on him: look you how pale he glares,\\nHis forme and cause conioyn'd, preaching to stones,\\nWould make them capeable. Do not looke vpon me,\\nLeast with this pitteous action you conuert\\nMy sterne effects: then what I haue to do,\\nWill want true colour; teares perchance for blood\\n\\n   Qu. To who do you speake this?\\n  Ham. Do you see nothing there?\\n  Qu. Nothing at all, yet all that is I see\\n\\n   Ham. Nor did you nothing heare?\\n  Qu. No, nothing but our selues\\n\\n   Ham. Why look you there: looke how it steals away:\\nMy Father in his habite, as he liued,\\nLooke where he goes euen now out at the Portall.\\nEnter.\\n\\n  Qu. This is the very coynage of your Braine,\\nThis bodilesse Creation extasie is very cunning in\\n\\n   Ham. Extasie?\\nMy Pulse as yours doth temperately keepe time,\\nAnd makes as healthfull Musicke. It is not madnesse\\nThat I haue vttered; bring me to the Test\\nAnd I the matter will re-word: which madnesse\\nWould gamboll from. Mother, for loue of Grace,\\nLay not a flattering Vnction to your soule,\\nThat not your trespasse, but my madnesse speakes:\\nIt will but skin and filme the Vlcerous place,\\nWhil'st ranke Corruption mining all within,\\nInfects vnseene. Confesse your selfe to Heauen,\\nRepent what's past, auoyd what is to come,\\nAnd do not spred the Compost on the Weedes,\\nTo make them ranke. Forgiue me this my Vertue,\\nFor in the fatnesse of this pursie times,\\nVertue it selfe, of Vice must pardon begge,\\nYea courb, and woe, for leaue to do him good\\n\\n   Qu. Oh Hamlet,\\nThou hast cleft my heart in twaine\\n\\n   Ham. O throw away the worser part of it,\\nAnd liue the purer with the other halfe.\\nGood night, but go not to mine Vnkles bed,\\nAssume a Vertue, if you haue it not, refraine to night,\\nAnd that shall lend a kinde of easinesse\\nTo the next abstinence. Once more goodnight,\\nAnd when you are desirous to be blest,\\nIle blessing begge of you. For this same Lord,\\nI do repent: but heauen hath pleas'd it so,\\nTo punish me with this, and this with me,\\nThat I must be their Scourge and Minister.\\nI will bestow him, and will answer well\\nThe death I gaue him: so againe, good night.\\nI must be cruell, onely to be kinde;\\nThus bad begins and worse remaines behinde\\n\\n   Qu. What shall I do?\\n  Ham. Not this by no meanes that I bid you do:\\nLet the blunt King tempt you againe to bed,\\nPinch Wanton on your cheeke, call you his Mouse,\\nAnd let him for a paire of reechie kisses,\\nOr padling in your necke with his damn'd Fingers,\\nMake you to rauell all this matter out,\\nThat I essentially am not in madnesse,\\nBut made in craft. 'Twere good you let him know,\\nFor who that's but a Queene, faire, sober, wise,\\nWould from a Paddocke, from a Bat, a Gibbe,\\nSuch deere concernings hide, Who would do so,\\nNo in despight of Sense and Secrecie,\\nVnpegge the Basket on the houses top:\\nLet the Birds flye, and like the famous Ape\\nTo try Conclusions in the Basket, creepe\\nAnd breake your owne necke downe\\n\\n   Qu. Be thou assur'd, if words be made of breath,\\nAnd breath of life: I haue no life to breath\\nWhat thou hast saide to me\\n\\n   Ham. I must to England, you know that?\\n  Qu. Alacke I had forgot: 'Tis so concluded on\\n\\n   Ham. This man shall set me packing:\\nIle lugge the Guts into the Neighbor roome,\\nMother goodnight. Indeede this Counsellor\\nIs now most still, most secret, and most graue,\\nWho was in life, a foolish prating Knaue.\\nCome sir, to draw toward an end with you.\\nGood night Mother.\\nExit Hamlet tugging in Polonius.\\n\\nEnter King.\\n\\n  King. There's matters in these sighes.\\nThese profound heaues\\nYou must translate; Tis fit we vnderstand them.\\nWhere is your Sonne?\\n  Qu. Ah my good Lord, what haue I seene to night?\\n  King. What Gertrude? How do's Hamlet?\\n  Qu. Mad as the Seas, and winde, when both contend\\nWhich is the Mightier, in his lawlesse fit\\nBehinde the Arras, hearing something stirre,\\nHe whips his Rapier out, and cries a Rat, a Rat,\\nAnd in his brainish apprehension killes\\nThe vnseene good old man\\n\\n   King. Oh heauy deed:\\nIt had bin so with vs had we beene there:\\nHis Liberty is full of threats to all,\\nTo you your selfe, to vs, to euery one.\\nAlas, how shall this bloody deede be answered?\\nIt will be laide to vs, whose prouidence\\nShould haue kept short, restrain'd, and out of haunt,\\nThis mad yong man. But so much was our loue,\\nWe would not vnderstand what was most fit,\\nBut like the Owner of a foule disease,\\nTo keepe it from divulging, let's it feede\\nEuen on the pith of life. Where is he gone?\\n  Qu. To draw apart the body he hath kild,\\nO're whom his very madnesse like some Oare\\nAmong a Minerall of Mettels base\\nShewes it selfe pure. He weepes for what is done\\n\\n   King. Oh Gertrude, come away:\\nThe Sun no sooner shall the Mountaines touch,\\nBut we will ship him hence, and this vilde deed,\\nWe must with all our Maiesty and Skill\\nBoth countenance, and excuse.\\nEnter Ros. & Guild.\\n\\nHo Guildenstern:\\nFriends both go ioyne you with some further ayde:\\nHamlet in madnesse hath Polonius slaine,\\nAnd from his Mother Clossets hath he drag'd him.\\nGo seeke him out, speake faire, and bring the body\\nInto the Chappell. I pray you hast in this.\\nExit Gent.\\n\\nCome Gertrude, wee'l call vp our wisest friends,\\nTo let them know both what we meane to do,\\nAnd what's vntimely done. Oh come away,\\nMy soule is full of discord and dismay.\\n\\nExeunt.\\n\\nEnter Hamlet.\\n\\n  Ham. Safely stowed\\n\\n   Gentlemen within. Hamlet, Lord Hamlet\\n\\n   Ham. What noise? Who cals on Hamlet?\\nOh heere they come.\\nEnter Ros. and Guildensterne.\\n\\n  Ro. What haue you done my Lord with the dead body?\\n  Ham. Compounded it with dust, whereto 'tis Kinne\\n\\n   Rosin. Tell vs where 'tis, that we may take it thence,\\nAnd beare it to the Chappell\\n\\n   Ham. Do not beleeue it\\n\\n   Rosin. Beleeue what?\\n  Ham. That I can keepe your counsell, and not mine\\nowne. Besides, to be demanded of a Spundge, what replication\\nshould be made by the Sonne of a King\\n\\n   Rosin. Take you me for a Spundge, my Lord?\\n  Ham. I sir, that sokes vp the Kings Countenance, his\\nRewards, his Authorities (but such Officers do the King\\nbest seruice in the end. He keepes them like an Ape in\\nthe corner of his iaw, first mouth'd to be last swallowed,\\nwhen he needes what you haue glean'd, it is but squeezing\\nyou, and Spundge you shall be dry againe\\n\\n   Rosin. I vnderstand you not my Lord\\n\\n   Ham. I am glad of it: a knauish speech sleepes in a\\nfoolish eare\\n\\n   Rosin. My Lord, you must tell vs where the body is,\\nand go with vs to the King\\n\\n   Ham. The body is with the King, but the King is not\\nwith the body. The King, is a thing-\\n  Guild. A thing my Lord?\\n  Ham. Of nothing: bring me to him, hide Fox, and all\\nafter.\\n\\nExeunt.\\n\\nEnter King.\\n\\n  King. I haue sent to seeke him, and to find the bodie:\\nHow dangerous is it that this man goes loose:\\nYet must not we put the strong Law on him:\\nHee's loued of the distracted multitude,\\nWho like not in their iudgement, but their eyes:\\nAnd where 'tis so, th' Offenders scourge is weigh'd\\nBut neerer the offence: to beare all smooth, and euen,\\nThis sodaine sending him away, must seeme\\nDeliberate pause, diseases desperate growne,\\nBy desperate appliance are releeued,\\nOr not at all.\\nEnter Rosincrane.\\n\\nHow now? What hath befalne?\\n  Rosin. Where the dead body is bestow'd my Lord,\\nWe cannot get from him\\n\\n   King. But where is he?\\n  Rosin. Without my Lord, guarded to know your\\npleasure\\n\\n   King. Bring him before vs\\n\\n   Rosin. Hoa, Guildensterne? Bring in my Lord.\\nEnter Hamlet and Guildensterne.\\n\\n  King. Now Hamlet, where's Polonius?\\n  Ham. At Supper\\n\\n   King. At Supper? Where?\\n  Ham. Not where he eats, but where he is eaten, a certaine\\nconuocation of wormes are e'ne at him. Your worm\\nis your onely Emperor for diet. We fat all creatures else\\nto fat vs, and we fat our selfe for Magots. Your fat King,\\nand your leane Begger is but variable seruice to dishes,\\nbut to one Table that's the end\\n\\n   King. What dost thou meane by this?\\n  Ham. Nothing but to shew you how a King may go\\na Progresse through the guts of a Begger\\n\\n   King. Where is Polonius\\n\\n   Ham. In heauen, send thither to see. If your Messenger\\nfinde him not there, seeke him i'th other place your\\nselfe: but indeed, if you finde him not this moneth, you\\nshall nose him as you go vp the staires into the Lobby\\n\\n   King. Go seeke him there\\n\\n   Ham. He will stay till ye come\\n\\n   K. Hamlet, this deed of thine, for thine especial safety\\nWhich we do tender, as we deerely greeue\\nFor that which thou hast done, must send thee hence\\nWith fierie Quicknesse. Therefore prepare thy selfe,\\nThe Barke is readie, and the winde at helpe,\\nTh' Associates tend, and euery thing at bent\\nFor England\\n\\n   Ham. For England?\\n  King. I Hamlet\\n\\n   Ham. Good\\n\\n   King. So is it, if thou knew'st our purposes\\n\\n   Ham. I see a Cherube that see's him: but come, for\\nEngland. Farewell deere Mother\\n\\n   King. Thy louing Father Hamlet\\n\\n   Hamlet. My Mother: Father and Mother is man and\\nwife: man & wife is one flesh, and so my mother. Come,\\nfor England.\\n\\nExit\\n\\n  King. Follow him at foote,\\nTempt him with speed aboord:\\nDelay it not, Ile haue him hence to night.\\nAway, for euery thing is Seal'd and done\\nThat else leanes on th' Affaire, pray you make hast.\\nAnd England, if my loue thou holdst at ought,\\nAs my great power thereof may giue thee sense,\\nSince yet thy Cicatrice lookes raw and red\\nAfter the Danish Sword, and thy free awe\\nPayes homage to vs; thou maist not coldly set\\nOur Soueraigne Processe, which imports at full\\nBy Letters coniuring to that effect\\nThe present death of Hamlet. Do it England,\\nFor like the Hecticke in my blood he rages,\\nAnd thou must cure me: Till I know 'tis done,\\nHow ere my happes, my ioyes were ne're begun.\\n\\nExit\\n\\nEnter Fortinbras with an Armie.\\n\\n  For. Go Captaine, from me greet the Danish King,\\nTell him that by his license, Fortinbras\\nClaimes the conueyance of a promis'd March\\nOuer his Kingdome. You know the Rendeuous:\\nIf that his Maiesty would ought with vs,\\nWe shall expresse our dutie in his eye,\\nAnd let him know so\\n\\n   Cap. I will doo't, my Lord\\n\\n   For. Go safely on.\\nEnter.\\n\\nEnter Queene and Horatio.\\n\\n  Qu. I will not speake with her\\n\\n   Hor. She is importunate, indeed distract, her moode\\nwill needs be pittied\\n\\n   Qu. What would she haue?\\n  Hor. She speakes much of her Father; saies she heares\\nThere's trickes i'th' world, and hems, and beats her heart,\\nSpurnes enuiously at Strawes, speakes things in doubt,\\nThat carry but halfe sense: Her speech is nothing,\\nYet the vnshaped vse of it doth moue\\nThe hearers to Collection; they ayme at it,\\nAnd botch the words vp fit to their owne thoughts,\\nWhich as her winkes, and nods, and gestures yeeld them,\\nIndeed would make one thinke there would be thought,\\nThough nothing sure, yet much vnhappily\\n\\n   Qu. 'Twere good she were spoken with,\\nFor she may strew dangerous coniectures\\nIn ill breeding minds. Let her come in.\\nTo my sicke soule (as sinnes true Nature is)\\nEach toy seemes Prologue, to some great amisse,\\nSo full of Artlesse iealousie is guilt,\\nIt spill's it selfe, in fearing to be spilt.\\nEnter Ophelia distracted.\\n\\n  Ophe. Where is the beauteous Maiesty of Denmark\\n\\n   Qu. How now Ophelia?\\n  Ophe. How should I your true loue know from another one?\\nBy his Cockle hat and staffe, and his Sandal shoone\\n\\n   Qu. Alas sweet Lady: what imports this Song?\\n  Ophe. Say you? Nay pray you marke.\\nHe is dead and gone Lady, he is dead and gone,\\nAt his head a grasse-greene Turfe, at his heeles a stone.\\nEnter King.\\n\\n  Qu. Nay but Ophelia\\n\\n   Ophe. Pray you marke.\\nWhite his Shrow'd as the Mountaine Snow\\n\\n   Qu. Alas, looke heere my Lord\\n\\n   Ophe. Larded with sweet Flowers:\\nWhich bewept to the graue did not go,\\nWith true-loue showres\\n\\n   King. How do ye, pretty Lady?\\n  Ophe. Well, God dil'd you. They say the Owle was\\na Bakers daughter. Lord, wee know what we are, but\\nknow not what we may be. God be at your Table\\n\\n   King. Conceit vpon her Father\\n\\n   Ophe. Pray you let's haue no words of this: but when\\nthey aske you what it meanes, say you this:\\nTo morrow is S[aint]. Valentines day, all in the morning betime,\\nAnd I a Maid at your Window, to be your Valentine.\\nThen vp he rose, & don'd his clothes, & dupt the chamber dore,\\nLet in the Maid, that out a Maid, neuer departed more\\n\\n   King. Pretty Ophelia\\n\\n   Ophe. Indeed la? without an oath Ile make an end ont.\\nBy gis, and by S[aint]. Charity,\\nAlacke, and fie for shame:\\nYong men wil doo't, if they come too't,\\nBy Cocke they are too blame.\\nQuoth she before you tumbled me,\\nYou promis'd me to Wed:\\nSo would I ha done by yonder Sunne,\\nAnd thou hadst not come to my bed\\n\\n   King. How long hath she bin thus?\\n  Ophe. I hope all will be well. We must bee patient,\\nbut I cannot choose but weepe, to thinke they should\\nlay him i'th' cold ground: My brother shall knowe of it,\\nand so I thanke you for your good counsell. Come, my\\nCoach: Goodnight Ladies: Goodnight sweet Ladies:\\nGoodnight, goodnight.\\nEnter.\\n\\n  King. Follow her close,\\nGiue her good watch I pray you:\\nOh this is the poyson of deepe greefe, it springs\\nAll from her Fathers death. Oh Gertrude, Gertrude,\\nWhen sorrowes comes, they come not single spies,\\nBut in Battalians. First, her Father slaine,\\nNext your Sonne gone, and he most violent Author\\nOf his owne iust remoue: the people muddied,\\nThicke and vnwholsome in their thoughts, and whispers\\nFor good Polonius death; and we haue done but greenly\\nIn hugger mugger to interre him. Poore Ophelia\\nDiuided from her selfe, and her faire Iudgement,\\nWithout the which we are Pictures, or meere Beasts.\\nLast, and as much containing as all these,\\nHer Brother is in secret come from France,\\nKeepes on his wonder, keepes himselfe in clouds,\\nAnd wants not Buzzers to infect his eare\\nWith pestilent Speeches of his Fathers death,\\nWhere in necessitie of matter Beggard,\\nWill nothing sticke our persons to Arraigne\\nIn eare and eare. O my deere Gertrude, this,\\nLike to a murdering Peece in many places,\\nGiues me superfluous death.\\n\\nA Noise within.\\n\\nEnter a Messenger.\\n\\n  Qu. Alacke, what noyse is this?\\n  King. Where are my Switzers?\\nLet them guard the doore. What is the matter?\\n  Mes. Saue your selfe, my Lord.\\nThe Ocean (ouer-peering of his List)\\nEates not the Flats with more impittious haste\\nThen young Laertes, in a Riotous head,\\nOre-beares your Officers, the rabble call him Lord,\\nAnd as the world were now but to begin,\\nAntiquity forgot, Custome not knowne,\\nThe Ratifiers and props of euery word,\\nThey cry choose we? Laertes shall be King,\\nCaps, hands, and tongues, applaud it to the clouds,\\nLaertes shall be King, Laertes King\\n\\n   Qu. How cheerefully on the false Traile they cry,\\nOh this is Counter you false Danish Dogges.\\n\\nNoise within. Enter Laertes.\\n\\n  King. The doores are broke\\n\\n   Laer. Where is the King, sirs? Stand you all without\\n\\n   All. No, let's come in\\n\\n   Laer. I pray you giue me leaue\\n\\n   Al. We will, we will\\n\\n   Laer. I thanke you: Keepe the doore.\\nOh thou vilde King, giue me my Father\\n\\n   Qu. Calmely good Laertes\\n\\n   Laer. That drop of blood, that calmes\\nProclaimes me Bastard:\\nCries Cuckold to my Father, brands the Harlot\\nEuen heere betweene the chaste vnsmirched brow\\nOf my true Mother\\n\\n   King. What is the cause Laertes,\\nThat thy Rebellion lookes so Gyant-like?\\nLet him go Gertrude: Do not feare our person:\\nThere's such Diuinity doth hedge a King,\\nThat Treason can but peepe to what it would,\\nActs little of his will. Tell me Laertes,\\nWhy thou art thus Incenst? Let him go Gertrude.\\nSpeake man\\n\\n   Laer. Where's my Father?\\n  King. Dead\\n\\n   Qu. But not by him\\n\\n   King. Let him demand his fill\\n\\n   Laer. How came he dead? Ile not be Iuggel'd with.\\nTo hell Allegeance: Vowes, to the blackest diuell.\\nConscience and Grace, to the profoundest Pit.\\nI dare Damnation: to this point I stand,\\nThat both the worlds I giue to negligence,\\nLet come what comes: onely Ile be reueng'd\\nMost throughly for my Father\\n\\n   King. Who shall stay you?\\n  Laer. My Will, not all the world,\\nAnd for my meanes, Ile husband them so well,\\nThey shall go farre with little\\n\\n   King. Good Laertes:\\nIf you desire to know the certaintie\\nOf your deere Fathers death, if writ in your reuenge,\\nThat Soop-stake you will draw both Friend and Foe,\\nWinner and Looser\\n\\n   Laer. None but his Enemies\\n\\n   King. Will you know them then\\n\\n   La. To his good Friends, thus wide Ile ope my Armes:\\nAnd like the kinde Life-rend'ring Politician,\\nRepast them with my blood\\n\\n   King. Why now you speake\\nLike a good Childe, and a true Gentleman.\\nThat I am guiltlesse of your Fathers death,\\nAnd am most sensible in greefe for it,\\nIt shall as leuell to your Iudgement pierce\\nAs day do's to your eye.\\n\\nA noise within. Let her come in.\\n\\nEnter Ophelia.\\n\\n  Laer. How now? what noise is that?\\nOh heate drie vp my Braines, teares seuen times salt,\\nBurne out the Sence and Vertue of mine eye.\\nBy Heauen, thy madnesse shall be payed by waight,\\nTill our Scale turnes the beame. Oh Rose of May,\\nDeere Maid, kinde Sister, sweet Ophelia:\\nOh Heauens, is't possible, a yong Maids wits,\\nShould be as mortall as an old mans life?\\nNature is fine in Loue, and where 'tis fine,\\nIt sends some precious instance of it selfe\\nAfter the thing it loues\\n\\n   Ophe. They bore him bare fac'd on the Beer,\\nHey non nony, nony, hey nony:\\nAnd on his graue raines many a teare,\\nFare you well my Doue\\n\\n   Laer. Had'st thou thy wits, and did'st perswade Reuenge,\\nit could not moue thus\\n\\n   Ophe. You must sing downe a-downe, and you call\\nhim a-downe-a. Oh, how the wheele becomes it? It is\\nthe false Steward that stole his masters daughter\\n\\n   Laer. This nothings more then matter\\n\\n   Ophe. There's Rosemary, that's for Remembraunce.\\nPray loue remember: and there is Paconcies, that's for\\nThoughts\\n\\n   Laer. A document in madnesse, thoughts & remembrance\\nfitted\\n\\n   Ophe. There's Fennell for you, and Columbines: ther's\\nRew for you, and heere's some for me. Wee may call it\\nHerbe-Grace a Sundaies: Oh you must weare your Rew\\nwith a difference. There's a Daysie, I would giue you\\nsome Violets, but they wither'd all when my Father dyed:\\nThey say, he made a good end;\\nFor bonny sweet Robin is all my ioy\\n\\n   Laer. Thought, and Affliction, Passion, Hell it selfe:\\nShe turnes to Fauour, and to prettinesse\\n\\n   Ophe. And will he not come againe,\\nAnd will he not come againe:\\nNo, no, he is dead, go to thy Death-bed,\\nHe neuer wil come againe.\\nHis Beard as white as Snow,\\nAll Flaxen was his Pole:\\nHe is gone, he is gone, and we cast away mone,\\nGramercy on his Soule.\\nAnd of all Christian Soules, I pray God.\\nGod buy ye.\\n\\nExeunt. Ophelia\\n\\n  Laer. Do you see this, you Gods?\\n  King. Laertes, I must common with your greefe,\\nOr you deny me right: go but apart,\\nMake choice of whom your wisest Friends you will,\\nAnd they shall heare and iudge 'twixt you and me;\\nIf by direct or by Colaterall hand\\nThey finde vs touch'd, we will our Kingdome giue,\\nOur Crowne, our Life, and all that we call Ours\\nTo you in satisfaction. But if not,\\nBe you content to lend your patience to vs,\\nAnd we shall ioyntly labour with your soule\\nTo giue it due content\\n\\n   Laer. Let this be so:\\nHis meanes of death, his obscure buriall;\\nNo Trophee, Sword, nor Hatchment o're his bones,\\nNo Noble rite, nor formall ostentation,\\nCry to be heard, as 'twere from Heauen to Earth,\\nThat I must call in question\\n\\n   King. So you shall:\\nAnd where th' offence is, let the great Axe fall.\\nI pray you go with me.\\n\\nExeunt.\\n\\nEnter Horatio, with an Attendant.\\n\\n  Hora. What are they that would speake with me?\\n  Ser. Saylors sir, they say they haue Letters for you\\n\\n   Hor. Let them come in,\\nI do not know from what part of the world\\nI should be greeted, if not from Lord Hamlet.\\nEnter Saylor.\\n\\n  Say. God blesse you Sir\\n\\n   Hor. Let him blesse thee too\\n\\n   Say. Hee shall Sir, and't please him. There's a Letter\\nfor you Sir: It comes from th' Ambassadours that was\\nbound for England, if your name be Horatio, as I am let\\nto know it is.\\n\\nReads the Letter.\\n\\nHoratio, When thou shalt haue ouerlook'd this, giue these\\nFellowes some meanes to the King: They haue Letters\\nfor him. Ere we were two dayes old at Sea, a Pyrate of very\\nWarlicke appointment gaue vs Chace. Finding our selues too\\nslow of Saile, we put on a compelled Valour. In the Grapple, I\\nboorded them: On the instant they got cleare of our Shippe, so\\nI alone became their Prisoner. They haue dealt with mee, like\\nTheeues of Mercy, but they knew what they did. I am to doe\\na good turne for them. Let the King haue the Letters I haue\\nsent, and repaire thou to me with as much hast as thou wouldest\\nflye death. I haue words to speake in your eare, will make thee\\ndumbe, yet are they much too light for the bore of the Matter.\\nThese good Fellowes will bring thee where I am. Rosincrance\\nand Guildensterne, hold their course for England. Of them\\nI haue much to tell thee, Farewell.\\nHe that thou knowest thine,\\nHamlet.\\nCome, I will giue you way for these your Letters,\\nAnd do't the speedier, that you may direct me\\nTo him from whom you brought them.\\nEnter.\\n\\nEnter King and Laertes.\\n\\n  King. Now must your conscience my acquittance seal,\\nAnd you must put me in your heart for Friend,\\nSith you haue heard, and with a knowing eare,\\nThat he which hath your Noble Father slaine,\\nPursued my life\\n\\n   Laer. It well appeares. But tell me,\\nWhy you proceeded not against these feates,\\nSo crimefull, and so Capitall in Nature,\\nAs by your Safety, Wisedome, all things else,\\nYou mainly were stirr'd vp?\\n  King. O for two speciall Reasons,\\nWhich may to you (perhaps) seeme much vnsinnowed,\\nAnd yet to me they are strong. The Queen his Mother,\\nLiues almost by his lookes: and for my selfe,\\nMy Vertue or my Plague, be it either which,\\nShe's so coniunctiue to my life, and soule;\\nThat as the Starre moues not but in his Sphere,\\nI could not but by her. The other Motiue,\\nWhy to a publike count I might not go,\\nIs the great loue the generall gender beare him,\\nWho dipping all his Faults in their affection,\\nWould like the Spring that turneth Wood to Stone,\\nConuert his Gyues to Graces. So that my Arrowes\\nToo slightly timbred for so loud a Winde,\\nWould haue reuerted to my Bow againe,\\nAnd not where I had arm'd them\\n\\n   Laer. And so haue I a Noble Father lost,\\nA Sister driuen into desperate tearmes,\\nWho was (if praises may go backe againe)\\nStood Challenger on mount of all the Age\\nFor her perfections. But my reuenge will come\\n\\n   King. Breake not your sleepes for that,\\nYou must not thinke\\nThat we are made of stuffe, so flat, and dull,\\nThat we can let our Beard be shooke with danger,\\nAnd thinke it pastime. You shortly shall heare more,\\nI lou'd your Father, and we loue our Selfe,\\nAnd that I hope will teach you to imagine-\\nEnter a Messenger.\\n\\nHow now? What Newes?\\n  Mes. Letters my Lord from Hamlet, This to your\\nMaiesty: this to the Queene\\n\\n   King. From Hamlet? Who brought them?\\n  Mes. Saylors my Lord they say, I saw them not:\\nThey were giuen me by Claudio, he receiu'd them\\n\\n   King. Laertes you shall heare them:\\nLeaue vs.\\n\\nExit Messenger\\n\\nHigh and Mighty, you shall know I am set naked on your\\nKingdome. To morrow shall I begge leaue to see your Kingly\\nEyes. When I shall (first asking your Pardon thereunto) recount\\nth' Occasions of my sodaine, and more strange returne.\\nHamlet.\\nWhat should this meane? Are all the rest come backe?\\nOr is it some abuse? Or no such thing?\\n  Laer. Know you the hand?\\n  Kin. 'Tis Hamlets Character, naked and in a Postscript\\nhere he sayes alone: Can you aduise me?\\n  Laer. I'm lost in it my Lord; but let him come,\\nIt warmes the very sicknesse in my heart,\\nThat I shall liue and tell him to his teeth;\\nThus diddest thou\\n\\n   Kin. If it be so Laertes, as how should it be so:\\nHow otherwise will you be rul'd by me?\\n  Laer. If so you'l not o'rerule me to a peace\\n\\n   Kin. To thine owne peace: if he be now return'd,\\nAs checking at his Voyage, and that he meanes\\nNo more to vndertake it; I will worke him\\nTo an exployt now ripe in my Deuice,\\nVnder the which he shall not choose but fall;\\nAnd for his death no winde of blame shall breath,\\nBut euen his Mother shall vncharge the practice,\\nAnd call it accident: Some two Monthes hence\\nHere was a Gentleman of Normandy,\\nI'ue seene my selfe, and seru'd against the French,\\nAnd they ran well on Horsebacke; but this Gallant\\nHad witchcraft in't; he grew into his Seat,\\nAnd to such wondrous doing brought his Horse,\\nAs had he beene encorps't and demy-Natur'd\\nWith the braue Beast, so farre he past my thought,\\nThat I in forgery of shapes and trickes,\\nCome short of what he did\\n\\n   Laer. A Norman was't?\\n  Kin. A Norman\\n\\n   Laer. Vpon my life Lamound\\n\\n   Kin. The very same\\n\\n   Laer. I know him well, he is the Brooch indeed,\\nAnd Iemme of all our Nation\\n\\n   Kin. Hee mad confession of you,\\nAnd gaue you such a Masterly report,\\nFor Art and exercise in your defence;\\nAnd for your Rapier most especiall,\\nThat he cryed out, t'would be a sight indeed,\\nIf one could match you Sir. This report of his\\nDid Hamlet so envenom with his Enuy,\\nThat he could nothing doe but wish and begge,\\nYour sodaine comming ore to play with him;\\nNow out of this\\n\\n   Laer. Why out of this, my Lord?\\n  Kin. Laertes was your Father deare to you?\\nOr are you like the painting of a sorrow,\\nA face without a heart?\\n  Laer. Why aske you this?\\n  Kin. Not that I thinke you did not loue your Father,\\nBut that I know Loue is begun by Time:\\nAnd that I see in passages of proofe,\\nTime qualifies the sparke and fire of it:\\nHamlet comes backe: what would you vndertake,\\nTo show your selfe your Fathers sonne indeed,\\nMore then in words?\\n  Laer. To cut his throat i'th' Church\\n\\n   Kin. No place indeed should murder Sancturize;\\nReuenge should haue no bounds: but good Laertes\\nWill you doe this, keepe close within your Chamber,\\nHamlet return'd, shall know you are come home:\\nWee'l put on those shall praise your excellence,\\nAnd set a double varnish on the fame\\nThe Frenchman gaue you, bring you in fine together,\\nAnd wager on your heads, he being remisse,\\nMost generous, and free from all contriuing,\\nWill not peruse the Foiles? So that with ease,\\nOr with a little shuffling, you may choose\\nA Sword vnbaited, and in a passe of practice,\\nRequit him for your Father\\n\\n   Laer. I will doo't.\\nAnd for that purpose Ile annoint my Sword:\\nI bought an Vnction of a Mountebanke\\nSo mortall, I but dipt a knife in it,\\nWhere it drawes blood, no Cataplasme so rare,\\nCollected from all Simples that haue Vertue\\nVnder the Moone, can saue the thing from death,\\nThat is but scratcht withall: Ile touch my point,\\nWith this contagion, that if I gall him slightly,\\nIt may be death\\n\\n   Kin. Let's further thinke of this,\\nWeigh what conuenience both of time and meanes\\nMay fit vs to our shape, if this should faile;\\nAnd that our drift looke through our bad performance,\\n'Twere better not assaid; therefore this Proiect\\nShould haue a backe or second, that might hold,\\nIf this should blast in proofe: Soft, let me see\\nWee'l make a solemne wager on your commings,\\nI ha't: when in your motion you are hot and dry,\\nAs make your bowts more violent to the end,\\nAnd that he cals for drinke; Ile haue prepar'd him\\nA Challice for the nonce; whereon but sipping,\\nIf he by chance escape your venom'd stuck,\\nOur purpose may hold there; how sweet Queene.\\nEnter Queene.\\n\\n  Queen. One woe doth tread vpon anothers heele,\\nSo fast they'l follow: your Sister's drown'd Laertes\\n\\n   Laer. Drown'd! O where?\\n  Queen. There is a Willow growes aslant a Brooke,\\nThat shewes his hore leaues in the glassie streame:\\nThere with fantasticke Garlands did she come,\\nOf Crow-flowers, Nettles, Daysies, and long Purples,\\nThat liberall Shepheards giue a grosser name;\\nBut our cold Maids doe Dead Mens Fingers call them:\\nThere on the pendant boughes, her Coronet weeds\\nClambring to hang; an enuious sliuer broke,\\nWhen downe the weedy Trophies, and her selfe,\\nFell in the weeping Brooke, her cloathes spred wide,\\nAnd Mermaid-like, a while they bore her vp,\\nWhich time she chaunted snatches of old tunes,\\nAs one incapable of her owne distresse,\\nOr like a creature Natiue, and indued\\nVnto that Element: but long it could not be,\\nTill that her garments, heauy with her drinke,\\nPul'd the poore wretch from her melodious buy,\\nTo muddy death\\n\\n   Laer. Alas then, is she drown'd?\\n  Queen. Drown'd, drown'd\\n\\n   Laer. Too much of water hast thou poore Ophelia,\\nAnd therefore I forbid my teares: but yet\\nIt is our tricke, Nature her custome holds,\\nLet shame say what it will; when these are gone\\nThe woman will be out: Adue my Lord,\\nI haue a speech of fire, that faine would blaze,\\nBut that this folly doubts it.\\nEnter.\\n\\n  Kin. Let's follow, Gertrude:\\nHow much I had to doe to calme his rage?\\nNow feare I this will giue it start againe;\\nTherefore let's follow.\\n\\nExeunt.\\n\\nEnter two Clownes.\\n\\n  Clown. Is she to bee buried in Christian buriall, that\\nwilfully seekes her owne saluation?\\n  Other. I tell thee she is, and therefore make her Graue\\nstraight, the Crowner hath sate on her, and finds it Christian\\nburiall\\n\\n   Clo. How can that be, vnlesse she drowned her selfe in\\nher owne defence?\\n  Other. Why 'tis found so\\n\\n   Clo. It must be Se offendendo, it cannot bee else: for\\nheere lies the point; If I drowne my selfe wittingly, it argues\\nan Act: and an Act hath three branches. It is an\\nAct to doe and to performe; argall she drown'd her selfe\\nwittingly\\n\\n   Other. Nay but heare you Goodman Deluer\\n\\n   Clown. Giue me leaue; heere lies the water; good:\\nheere stands the man; good: If the man goe to this water\\nand drowne himselfe; it is will he nill he, he goes;\\nmarke you that? But if the water come to him & drowne\\nhim; hee drownes not himselfe. Argall, hee that is not\\nguilty of his owne death, shortens not his owne life\\n\\n   Other. But is this law?\\n  Clo. I marry is't, Crowners Quest Law\\n\\n   Other. Will you ha the truth on't: if this had not\\nbeene a Gentlewoman, shee should haue beene buried\\nout of Christian Buriall\\n\\n   Clo. Why there thou say'st. And the more pitty that\\ngreat folke should haue countenance in this world to\\ndrowne or hang themselues, more then their euen Christian.\\nCome, my Spade; there is no ancient Gentlemen,\\nbut Gardiners, Ditchers and Graue-makers; they hold vp\\nAdams Profession\\n\\n   Other. Was he a Gentleman?\\n  Clo. He was the first that euer bore Armes\\n\\n   Other. Why he had none\\n\\n   Clo. What, ar't a Heathen? how doth thou vnderstand\\nthe Scripture? the Scripture sayes Adam dig'd;\\ncould hee digge without Armes? Ile put another question\\nto thee; if thou answerest me not to the purpose, confesse\\nthy selfe-\\n  Other. Go too\\n\\n   Clo. What is he that builds stronger then either the\\nMason, the Shipwright, or the Carpenter?\\n  Other. The Gallowes maker; for that Frame outliues a\\nthousand Tenants\\n\\n   Clo. I like thy wit well in good faith, the Gallowes\\ndoes well; but how does it well? it does well to those\\nthat doe ill: now, thou dost ill to say the Gallowes is\\nbuilt stronger then the Church: Argall, the Gallowes\\nmay doe well to thee. Too't againe, Come\\n\\n   Other. Who builds stronger then a Mason, a Shipwright,\\nor a Carpenter?\\n  Clo. I, tell me that, and vnyoake\\n\\n   Other. Marry, now I can tell\\n\\n   Clo. Too't\\n\\n   Other. Masse, I cannot tell.\\nEnter Hamlet and Horatio a farre off.\\n\\n  Clo. Cudgell thy braines no more about it; for your\\ndull Asse will not mend his pace with beating; and when\\nyou are ask't this question next, say a Graue-maker: the\\nHouses that he makes, lasts till Doomesday: go, get thee\\nto Yaughan, fetch me a stoupe of Liquor.\\n\\nSings.\\n\\nIn youth when I did loue, did loue,\\nme thought it was very sweete:\\nTo contract O the time for a my behoue,\\nO me thought there was nothing meete\\n\\n   Ham. Ha's this fellow no feeling of his businesse, that\\nhe sings at Graue-making?\\n  Hor. Custome hath made it in him a property of easinesse\\n\\n   Ham. 'Tis ee'n so; the hand of little Imployment hath\\nthe daintier sense\\n\\n   Clowne sings. But Age with his stealing steps\\nhath caught me in his clutch:\\nAnd hath shipped me intill the Land,\\nas if I had neuer beene such\\n\\n   Ham. That Scull had a tongue in it, and could sing\\nonce: how the knaue iowles it to th' grownd, as if it\\nwere Caines Iaw-bone, that did the first murther: It\\nmight be the Pate of a Polititian which this Asse o're Offices:\\none that could circumuent God, might it not?\\n  Hor. It might, my Lord\\n\\n   Ham. Or of a Courtier, which could say, Good Morrow\\nsweet Lord: how dost thou, good Lord? this\\nmight be my Lord such a one, that prais'd my Lord such\\na ones Horse, when he meant to begge it; might it not?\\n  Hor. I, my Lord\\n\\n   Ham. Why ee'n so: and now my Lady Wormes,\\nChaplesse, and knockt about the Mazard with a Sextons\\nSpade; heere's fine Reuolution, if wee had the tricke to\\nsee't. Did these bones cost no more the breeding, but\\nto play at Loggets with 'em? mine ake to thinke\\non't\\n\\n   Clowne sings. A Pickhaxe and a Spade, a Spade,\\nfor and a shrowding-Sheete:\\nO a Pit of Clay for to be made,\\nfor such a Guest is meete\\n\\n   Ham. There's another: why might not that bee the\\nScull of a Lawyer? where be his Quiddits now? his\\nQuillets? his Cases? his Tenures, and his Tricks? why\\ndoe's he suffer this rude knaue now to knocke him about\\nthe Sconce with a dirty Shouell, and will not tell him of\\nhis Action of Battery? hum. This fellow might be in's\\ntime a great buyer of Land, with his Statutes, his Recognizances,\\nhis Fines, his double Vouchers, his Recoueries:\\nIs this the fine of his Fines, and the recouery of his Recoueries,\\nto haue his fine Pate full of fine Dirt? will his\\nVouchers vouch him no more of his Purchases, and double\\nones too, then the length and breadth of a paire of\\nIndentures? the very Conueyances of his Lands will\\nhardly lye in this Boxe; and must the Inheritor himselfe\\nhaue no more? ha?\\n  Hor. Not a iot more, my Lord\\n\\n   Ham. Is not Parchment made of Sheep-skinnes?\\n  Hor. I my Lord, and of Calue-skinnes too\\n\\n   Ham. They are Sheepe and Calues that seek out assurance\\nin that. I will speake to this fellow: whose Graue's\\nthis Sir?\\n  Clo. Mine Sir:\\nO a Pit of Clay for to be made,\\nfor such a Guest is meete\\n\\n   Ham. I thinke it be thine indeed: for thou liest in't\\n\\n   Clo. You lye out on't Sir, and therefore it is not yours:\\nfor my part, I doe not lye in't; and yet it is mine\\n\\n   Ham. Thou dost lye in't, to be in't and say 'tis thine:\\n'tis for the dead, not for the quicke, therefore thou\\nlyest\\n\\n   Clo. 'Tis a quicke lye Sir, 'twill away againe from me\\nto you\\n\\n   Ham. What man dost thou digge it for?\\n  Clo. For no man Sir\\n\\n   Ham. What woman then?\\n  Clo. For none neither\\n\\n   Ham. Who is to be buried in't?\\n  Clo. One that was a woman Sir; but rest her Soule,\\nshee's dead\\n\\n   Ham. How absolute the knaue is? wee must speake\\nby the Carde, or equiuocation will vndoe vs: by the\\nLord Horatio, these three yeares I haue taken note of it,\\nthe Age is growne so picked, that the toe of the Pesant\\ncomes so neere the heeles of our Courtier, hee galls his\\nKibe. How long hast thou been a Graue-maker?\\n  Clo. Of all the dayes i'th' yeare, I came too't that day\\nthat our last King Hamlet o'recame Fortinbras\\n\\n   Ham. How long is that since?\\n  Clo. Cannot you tell that? euery foole can tell that:\\nIt was the very day, that young Hamlet was borne, hee\\nthat was mad, and sent into England\\n\\n   Ham. I marry, why was he sent into England?\\n  Clo. Why, because he was mad; hee shall recouer his\\nwits there; or if he do not, it's no great matter there\\n\\n   Ham. Why?\\n  Clo. 'Twill not be seene in him, there the men are as\\nmad as he\\n\\n   Ham. How came he mad?\\n  Clo. Very strangely they say\\n\\n   Ham. How strangely?\\n  Clo. Faith e'ene with loosing his wits\\n\\n   Ham. Vpon what ground?\\n  Clo. Why heere in Denmarke: I haue bin sixeteene\\nheere, man and Boy thirty yeares\\n\\n   Ham. How long will a man lie i'th' earth ere he rot?\\n  Clo. Ifaith, if he be not rotten before he die (as we haue\\nmany pocky Coarses now adaies, that will scarce hold\\nthe laying in) he will last you some eight yeare, or nine\\nyeare. A Tanner will last you nine yeare\\n\\n   Ham. Why he, more then another?\\n  Clo. Why sir, his hide is so tan'd with his Trade, that\\nhe will keepe out water a great while. And your water,\\nis a sore Decayer of your horson dead body. Heres a Scull\\nnow: this Scul, has laine in the earth three & twenty years\\n\\n   Ham. Whose was it?\\n  Clo. A whoreson mad Fellowes it was;\\nWhose doe you thinke it was?\\n  Ham. Nay, I know not\\n\\n   Clo. A pestilence on him for a mad Rogue, a pour'd a\\nFlaggon of Renish on my head once. This same Scull\\nSir, this same Scull sir, was Yoricks Scull, the Kings Iester\\n\\n   Ham. This?\\n  Clo. E'ene that\\n\\n   Ham. Let me see. Alas poore Yorick, I knew him Horatio,\\na fellow of infinite Iest; of most excellent fancy, he\\nhath borne me on his backe a thousand times: And how\\nabhorred my Imagination is, my gorge rises at it. Heere\\nhung those lipps, that I haue kist I know not how oft.\\nWhere be your Iibes now? Your Gambals? Your\\nSongs? Your flashes of Merriment that were wont to\\nset the Table on a Rore? No one now to mock your own\\nIeering? Quite chopfalne? Now get you to my Ladies\\nChamber, and tell her, let her paint an inch thicke, to this\\nfauour she must come. Make her laugh at that: prythee\\nHoratio tell me one thing\\n\\n   Hor. What's that my Lord?\\n  Ham. Dost thou thinke Alexander lookt o'this fashion\\ni'th' earth?\\n  Hor. E'ene so\\n\\n   Ham. And smelt so? Puh\\n\\n   Hor. E'ene so, my Lord\\n\\n   Ham. To what base vses we may returne Horatio.\\nWhy may not Imagination trace the Noble dust of Alexander,\\ntill he find it stopping a bunghole\\n\\n   Hor. 'Twere to consider: to curiously to consider so\\n\\n   Ham. No faith, not a iot. But to follow him thether\\nwith modestie enough, & likeliehood to lead it; as thus.\\nAlexander died: Alexander was buried: Alexander returneth\\ninto dust; the dust is earth; of earth we make\\nLome, and why of that Lome (whereto he was conuerted)\\nmight they not stopp a Beere-barrell?\\nImperiall Caesar, dead and turn'd to clay,\\nMight stop a hole to keepe the winde away.\\nOh, that that earth, which kept the world in awe,\\nShould patch a Wall, t' expell the winters flaw.\\nBut soft, but soft, aside; heere comes the King.\\nEnter King, Queene, Laertes, and a Coffin, with Lords attendant.\\n\\nThe Queene, the Courtiers. Who is that they follow,\\nAnd with such maimed rites? This doth betoken,\\nThe Coarse they follow, did with disperate hand,\\nFore do it owne life; 'twas some Estate.\\nCouch we a while, and mark\\n\\n   Laer. What Cerimony else?\\n  Ham. That is Laertes, a very Noble youth: Marke\\n\\n   Laer. What Cerimony else?\\n  Priest. Her Obsequies haue bin as farre inlarg'd.\\nAs we haue warrantie, her death was doubtfull,\\nAnd but that great Command, o're-swaies the order,\\nShe should in ground vnsanctified haue lodg'd,\\nTill the last Trumpet. For charitable praier,\\nShardes, Flints, and Peebles, should be throwne on her:\\nYet heere she is allowed her Virgin Rites,\\nHer Maiden strewments, and the bringing home\\nOf Bell and Buriall\\n\\n   Laer. Must there no more be done ?\\n  Priest. No more be done:\\nWe should prophane the seruice of the dead,\\nTo sing sage Requiem, and such rest to her\\nAs to peace-parted Soules\\n\\n   Laer. Lay her i'th' earth,\\nAnd from her faire and vnpolluted flesh,\\nMay Violets spring. I tell thee (churlish Priest)\\nA Ministring Angell shall my Sister be,\\nWhen thou liest howling?\\n  Ham. What, the faire Ophelia?\\n  Queene. Sweets, to the sweet farewell.\\nI hop'd thou should'st haue bin my Hamlets wife:\\nI thought thy Bride-bed to haue deckt (sweet Maid)\\nAnd not t'haue strew'd thy Graue\\n\\n   Laer. Oh terrible woer,\\nFall ten times trebble, on that cursed head\\nWhose wicked deed, thy most Ingenious sence\\nDepriu'd thee of. Hold off the earth a while,\\nTill I haue caught her once more in mine armes:\\n\\nLeaps in the graue.\\n\\nNow pile your dust, vpon the quicke, and dead,\\nTill of this flat a Mountaine you haue made,\\nTo o're top old Pelion, or the skyish head\\nOf blew Olympus\\n\\n   Ham. What is he, whose griefes\\nBeares such an Emphasis? whose phrase of Sorrow\\nConiure the wandring Starres, and makes them stand\\nLike wonder-wounded hearers? This is I,\\nHamlet the Dane\\n\\n   Laer. The deuill take thy soule\\n\\n   Ham. Thou prai'st not well,\\nI prythee take thy fingers from my throat;\\nSir though I am not Spleenatiue, and rash,\\nYet haue I something in me dangerous,\\nWhich let thy wisenesse feare. Away thy hand\\n\\n   King. Pluck them asunder\\n\\n   Qu. Hamlet, Hamlet\\n\\n   Gen. Good my Lord be quiet\\n\\n   Ham. Why I will fight with him vppon this Theme.\\nVntill my eielids will no longer wag\\n\\n   Qu. Oh my Sonne, what Theame?\\n  Ham. I lou'd Ophelia; fortie thousand Brothers\\nCould not (with all there quantitie of Loue)\\nMake vp my summe. What wilt thou do for her?\\n  King. Oh he is mad Laertes,\\n  Qu. For loue of God forbeare him\\n\\n   Ham. Come show me what thou'lt doe.\\nWoo't weepe? Woo't fight? Woo't teare thy selfe?\\nWoo't drinke vp Esile, eate a Crocodile?\\nIle doo't. Dost thou come heere to whine;\\nTo outface me with leaping in her Graue?\\nBe buried quicke with her, and so will I.\\nAnd if thou prate of Mountaines; let them throw\\nMillions of Akers on vs; till our ground\\nSindging his pate against the burning Zone,\\nMake Ossa like a wart. Nay, and thou'lt mouth,\\nIle rant as well as thou\\n\\n   Kin. This is meere Madnesse:\\nAnd thus awhile the fit will worke on him:\\nAnon as patient as the female Doue,\\nWhen that her Golden Cuplet are disclos'd;\\nHis silence will sit drooping\\n\\n   Ham. Heare you Sir:\\nWhat is the reason that you vse me thus?\\nI lou'd you euer; but it is no matter:\\nLet Hercules himselfe doe what he may,\\nThe Cat will Mew, and Dogge will haue his day.\\nEnter.\\n\\n  Kin. I pray you good Horatio wait vpon him,\\nStrengthen your patience in our last nights speech,\\nWee'l put the matter to the present push:\\nGood Gertrude set some watch ouer your Sonne,\\nThis Graue shall haue a liuing Monument:\\nAn houre of quiet shortly shall we see;\\nTill then, in patience our proceeding be.\\n\\nExeunt.\\n\\nEnter Hamlet and Horatio\\n\\n   Ham. So much for this Sir; now let me see the other,\\nYou doe remember all the Circumstance\\n\\n   Hor. Remember it my Lord?\\n  Ham. Sir, in my heart there was a kinde of fighting,\\nThat would not let me sleepe; me thought I lay\\nWorse then the mutines in the Bilboes, rashly,\\n(And praise be rashnesse for it) let vs know,\\nOur indiscretion sometimes serues vs well,\\nWhen our deare plots do paule, and that should teach vs,\\nThere's a Diuinity that shapes our ends,\\nRough-hew them how we will\\n\\n   Hor. That is most certaine\\n\\n   Ham. Vp from my Cabin\\nMy sea-gowne scarft about me in the darke,\\nGrop'd I to finde out them; had my desire,\\nFinger'd their Packet, and in fine, withdrew\\nTo mine owne roome againe, making so bold,\\n(My feares forgetting manners) to vnseale\\nTheir grand Commission, where I found Horatio,\\nOh royall knauery: An exact command,\\nLarded with many seuerall sorts of reason;\\nImporting Denmarks health, and Englands too,\\nWith hoo, such Bugges and Goblins in my life,\\nThat on the superuize no leasure bated,\\nNo not to stay the grinding of the Axe,\\nMy head should be struck off\\n\\n   Hor. Ist possible?\\n  Ham. Here's the Commission, read it at more leysure:\\nBut wilt thou heare me how I did proceed?\\n  Hor. I beseech you\\n\\n   Ham. Being thus benetted round with Villaines,\\nEre I could make a Prologue to my braines,\\nThey had begun the Play. I sate me downe,\\nDeuis'd a new Commission, wrote it faire,\\nI once did hold it as our Statists doe,\\nA basenesse to write faire; and laboured much\\nHow to forget that learning: but Sir now,\\nIt did me Yeomans seriuce: wilt thou know\\nThe effects of what I wrote?\\n  Hor. I, good my Lord\\n\\n   Ham. An earnest Coniuration from the King,\\nAs England was his faithfull Tributary,\\nAs loue betweene them, as the Palme should flourish,\\nAs Peace should still her wheaten Garland weare,\\nAnd stand a Comma 'tweene their amities,\\nAnd many such like Assis of great charge,\\nThat on the view and know of these Contents,\\nWithout debatement further, more or lesse,\\nHe should the bearers put to sodaine death,\\nNot shriuing time allowed\\n\\n   Hor. How was this seal'd?\\n  Ham. Why, euen in that was Heauen ordinate;\\nI had my fathers Signet in my Purse,\\nWhich was the Modell of that Danish Seale:\\nFolded the Writ vp in forme of the other,\\nSubscrib'd it, gau't th' impression, plac't it safely,\\nThe changeling neuer knowne: Now, the next day\\nWas our Sea Fight, and what to this was sement,\\nThou know'st already\\n\\n   Hor. So Guildensterne and Rosincrance, go too't\\n\\n   Ham. Why man, they did make loue to this imployment\\nThey are not neere my Conscience; their debate\\nDoth by their owne insinuation grow:\\n'Tis dangerous, when the baser nature comes\\nBetweene the passe, and fell incensed points\\nOf mighty opposites\\n\\n   Hor. Why, what a King is this?\\n  Ham. Does it not, thinkst thee, stand me now vpon\\nHe that hath kil'd my King, and whor'd my Mother,\\nPopt in betweene th' election and my hopes,\\nThrowne out his Angle for my proper life,\\nAnd with such coozenage; is't not perfect conscience,\\nTo quit him with this arme? And is't not to be damn'd\\nTo let this Canker of our nature come\\nIn further euill\\n\\n   Hor. It must be shortly knowne to him from England\\nWhat is the issue of the businesse there\\n\\n   Ham. It will be short,\\nThe interim's mine, and a mans life's no more\\nThen to say one: but I am very sorry good Horatio,\\nThat to Laertes I forgot my selfe;\\nFor by the image of my Cause, I see\\nThe Portraiture of his; Ile count his fauours:\\nBut sure the brauery of his griefe did put me\\nInto a Towring passion\\n\\n   Hor. Peace, who comes heere?\\nEnter young Osricke.\\n\\n  Osr. Your Lordship is right welcome back to Denmarke\\n\\n   Ham. I humbly thank you Sir, dost know this waterflie?\\n  Hor. No my good Lord\\n\\n   Ham. Thy state is the more gracious; for 'tis a vice to\\nknow him: he hath much Land, and fertile; let a Beast\\nbe Lord of Beasts, and his Crib shall stand at the Kings\\nMesse; 'tis a Chowgh; but as I saw spacious in the possession\\nof dirt\\n\\n   Osr. Sweet Lord, if your friendship were at leysure,\\nI should impart a thing to you from his Maiesty\\n\\n   Ham. I will receiue it with all diligence of spirit; put\\nyour Bonet to his right vse, 'tis for the head\\n\\n   Osr. I thanke your Lordship, 'tis very hot\\n\\n   Ham. No, beleeue mee 'tis very cold, the winde is\\nNortherly\\n\\n   Osr. It is indifferent cold my Lord indeed\\n\\n   Ham. Mee thinkes it is very soultry, and hot for my\\nComplexion\\n\\n   Osr. Exceedingly, my Lord, it is very soultry, as 'twere\\nI cannot tell how: but my Lord, his Maiesty bad me signifie\\nto you, that he ha's laid a great wager on your head:\\nSir, this is the matter\\n\\n   Ham. I beseech you remember\\n\\n   Osr. Nay, in good faith, for mine ease in good faith:\\nSir, you are not ignorant of what excellence Laertes is at\\nhis weapon\\n\\n   Ham. What's his weapon?\\n  Osr. Rapier and dagger\\n\\n   Ham. That's two of his weapons; but well\\n\\n   Osr. The sir King ha's wag'd with him six Barbary horses,\\nagainst the which he impon'd as I take it, sixe French\\nRapiers and Poniards, with their assignes, as Girdle,\\nHangers or so: three of the Carriages infaith are very\\ndeare to fancy, very responsiue to the hilts, most delicate\\ncarriages, and of very liberall conceit\\n\\n   Ham. What call you the Carriages?\\n  Osr. The Carriages Sir, are the hangers\\n\\n   Ham. The phrase would bee more Germaine to the\\nmatter: If we could carry Cannon by our sides; I would\\nit might be Hangers till then; but on sixe Barbary Horses\\nagainst sixe French Swords: their Assignes, and three\\nliberall conceited Carriages, that's the French but against\\nthe Danish; why is this impon'd as you call it?\\n  Osr. The King Sir, hath laid that in a dozen passes betweene\\nyou and him, hee shall not exceed you three hits;\\nHe hath one twelue for mine, and that would come to\\nimediate tryall, if your Lordship would vouchsafe the\\nAnswere\\n\\n   Ham. How if I answere no?\\n  Osr. I meane my Lord, the opposition of your person\\nin tryall\\n\\n   Ham. Sir, I will walke heere in the Hall; if it please\\nhis Maiestie, 'tis the breathing time of day with me; let\\nthe Foyles bee brought, the Gentleman willing, and the\\nKing hold his purpose; I will win for him if I can: if\\nnot, Ile gaine nothing but my shame, and the odde hits\\n\\n   Osr. Shall I redeliuer you ee'n so?\\n  Ham. To this effect Sir, after what flourish your nature\\nwill\\n\\n   Osr. I commend my duty to your Lordship\\n\\n   Ham. Yours, yours; hee does well to commend it\\nhimselfe, there are no tongues else for's tongue\\n\\n   Hor. This Lapwing runs away with the shell on his\\nhead\\n\\n   Ham. He did Complie with his Dugge before hee\\nsuck't it: thus had he and mine more of the same Beauty\\nthat I know the drossie age dotes on; only got the tune of\\nthe time, and outward habite of encounter, a kinde of\\nyesty collection, which carries them through & through\\nthe most fond and winnowed opinions; and doe but blow\\nthem to their tryalls: the Bubbles are out\\n\\n   Hor. You will lose this wager, my Lord\\n\\n   Ham. I doe not thinke so, since he went into France,\\nI haue beene in continuall practice; I shall winne at the\\noddes: but thou wouldest not thinke how all heere about\\nmy heart: but it is no matter\\n\\n   Hor. Nay, good my Lord\\n\\n   Ham. It is but foolery; but it is such a kinde of\\ngain-giuing as would perhaps trouble a woman\\n\\n   Hor. If your minde dislike any thing, obey. I will forestall\\ntheir repaire hither, and say you are not fit\\n\\n   Ham. Not a whit, we defie Augury; there's a speciall\\nProuidence in the fall of a sparrow. If it be now, 'tis not\\nto come: if it bee not to come, it will bee now: if it\\nbe not now; yet it will come; the readinesse is all, since no\\nman ha's ought of what he leaues. What is't to leaue betimes?\\nEnter King, Queene, Laertes and Lords, with other Attendants with\\nFoyles,\\nand Gauntlets, a Table and Flagons of Wine on it.\\n\\n  Kin. Come Hamlet, come, and take this hand from me\\n\\n   Ham. Giue me your pardon Sir, I'ue done you wrong,\\nBut pardon't as you are a Gentleman.\\nThis presence knowes,\\nAnd you must needs haue heard how I am punisht\\nWith sore distraction? What I haue done\\nThat might your nature honour, and exception\\nRoughly awake, I heere proclaime was madnesse:\\nWas't Hamlet wrong'd Laertes? Neuer Hamlet.\\nIf Hamlet from himselfe be tane away:\\nAnd when he's not himselfe, do's wrong Laertes,\\nThen Hamlet does it not, Hamlet denies it:\\nWho does it then? His Madnesse? If't be so,\\nHamlet is of the Faction that is wrong'd,\\nHis madnesse is poore Hamlets Enemy.\\nSir, in this Audience,\\nLet my disclaiming from a purpos'd euill,\\nFree me so farre in your most generous thoughts,\\nThat I haue shot mine Arrow o're the house,\\nAnd hurt my Mother\\n\\n   Laer. I am satisfied in Nature,\\nWhose motiue in this case should stirre me most\\nTo my Reuenge. But in my termes of Honor\\nI stand aloofe, and will no reconcilement,\\nTill by some elder Masters of knowne Honor,\\nI haue a voyce, and president of peace\\nTo keepe my name vngorg'd. But till that time,\\nI do receiue your offer'd loue like loue,\\nAnd wil not wrong it\\n\\n   Ham. I do embrace it freely,\\nAnd will this Brothers wager frankely play.\\nGiue vs the Foyles: Come on\\n\\n   Laer. Come one for me\\n\\n   Ham. Ile be your foile Laertes, in mine ignorance,\\nYour Skill shall like a Starre i'th' darkest night,\\nSticke fiery off indeede\\n\\n   Laer. You mocke me Sir\\n\\n   Ham. No by this hand\\n\\n   King. Giue them the Foyles yong Osricke,\\nCousen Hamlet, you know the wager\\n\\n   Ham. Verie well my Lord,\\nYour Grace hath laide the oddes a'th' weaker side\\n\\n   King. I do not feare it,\\nI haue seene you both:\\nBut since he is better'd, we haue therefore oddes\\n\\n   Laer. This is too heauy,\\nLet me see another\\n\\n   Ham. This likes me well,\\nThese Foyles haue all a length.\\n\\nPrepare to play.\\n\\n  Osricke. I my good Lord\\n\\n   King. Set me the Stopes of wine vpon that Table:\\nIf Hamlet giue the first, or second hit,\\nOr quit in answer of the third exchange,\\nLet all the Battlements their Ordinance fire,\\nThe King shal drinke to Hamlets better breath,\\nAnd in the Cup an vnion shal he throw\\nRicher then that, which foure successiue Kings\\nIn Denmarkes Crowne haue worne.\\nGiue me the Cups,\\nAnd let the Kettle to the Trumpets speake,\\nThe Trumpet to the Cannoneer without,\\nThe Cannons to the Heauens, the Heauen to Earth,\\nNow the King drinkes to Hamlet. Come, begin,\\nAnd you the Iudges beare a wary eye\\n\\n   Ham. Come on sir\\n\\n   Laer. Come on sir.\\n\\nThey play.\\n\\n  Ham. One\\n\\n   Laer. No\\n\\n   Ham. Iudgement\\n\\n   Osr. A hit, a very palpable hit\\n\\n   Laer. Well: againe\\n\\n   King. Stay, giue me drinke.\\nHamlet, this Pearle is thine,\\nHere's to thy health. Giue him the cup,\\n\\nTrumpets sound, and shot goes off.\\n\\n  Ham. Ile play this bout first, set by a-while.\\nCome: Another hit; what say you?\\n  Laer. A touch, a touch, I do confesse\\n\\n   King. Our Sonne shall win\\n\\n   Qu. He's fat, and scant of breath.\\nHeere's a Napkin, rub thy browes,\\nThe Queene Carowses to thy fortune, Hamlet\\n\\n   Ham. Good Madam\\n\\n   King. Gertrude, do not drinke\\n\\n   Qu. I will my Lord;\\nI pray you pardon me\\n\\n   King. It is the poyson'd Cup, it is too late\\n\\n   Ham. I dare not drinke yet Madam,\\nBy and by\\n\\n   Qu. Come, let me wipe thy face\\n\\n   Laer. My Lord, Ile hit him now\\n\\n   King. I do not thinke't\\n\\n   Laer. And yet 'tis almost 'gainst my conscience\\n\\n   Ham. Come for the third.\\nLaertes, you but dally,\\nI pray you passe with your best violence,\\nI am affear'd you make a wanton of me\\n\\n   Laer. Say you so? Come on.\\n\\nPlay.\\n\\n  Osr. Nothing neither way\\n\\n   Laer. Haue at you now.\\n\\nIn scuffling they change Rapiers.\\n\\n  King. Part them, they are incens'd\\n\\n   Ham. Nay come, againe\\n\\n   Osr. Looke to the Queene there hoa\\n\\n   Hor. They bleed on both sides. How is't my Lord?\\n  Osr. How is't Laertes?\\n  Laer. Why as a Woodcocke\\nTo mine Sprindge, Osricke,\\nI am iustly kill'd with mine owne Treacherie\\n\\n   Ham. How does the Queene?\\n  King. She sounds to see them bleede\\n\\n   Qu. No, no, the drinke, the drinke.\\nOh my deere Hamlet, the drinke, the drinke,\\nI am poyson'd\\n\\n   Ham. Oh Villany! How? Let the doore be lock'd.\\nTreacherie, seeke it out\\n\\n   Laer. It is heere Hamlet.\\nHamlet, thou art slaine,\\nNo Medicine in the world can do thee good.\\nIn thee, there is not halfe an houre of life;\\nThe Treacherous Instrument is in thy hand,\\nVnbated and envenom'd: the foule practise\\nHath turn'd it selfe on me. Loe, heere I lye,\\nNeuer to rise againe: Thy Mothers poyson'd:\\nI can no more, the King, the King's too blame\\n\\n   Ham. The point envenom'd too,\\nThen venome to thy worke.\\n\\nHurts the King.\\n\\n  All. Treason, Treason\\n\\n   King. O yet defend me Friends, I am but hurt\\n\\n   Ham. Heere thou incestuous, murdrous,\\nDamned Dane,\\nDrinke off this Potion: Is thy Vnion heere?\\nFollow my Mother.\\n\\nKing Dyes.\\n\\n  Laer. He is iustly seru'd.\\nIt is a poyson temp'red by himselfe:\\nExchange forgiuenesse with me, Noble Hamlet;\\nMine and my Fathers death come not vpon thee,\\nNor thine on me.\\n\\nDyes.\\n\\n  Ham. Heauen make thee free of it, I follow thee.\\nI am dead Horatio, wretched Queene adiew,\\nYou that looke pale, and tremble at this chance,\\nThat are but Mutes or audience to this acte:\\nHad I but time (as this fell Sergeant death\\nIs strick'd in his Arrest) oh I could tell you.\\nBut let it be: Horatio, I am dead,\\nThou liu'st, report me and my causes right\\nTo the vnsatisfied\\n\\n   Hor. Neuer beleeue it.\\nI am more an Antike Roman then a Dane:\\nHeere's yet some Liquor left\\n\\n   Ham. As th'art a man, giue me the Cup.\\nLet go, by Heauen Ile haue't.\\nOh good Horatio, what a wounded name,\\n(Things standing thus vnknowne) shall liue behind me.\\nIf thou did'st euer hold me in thy heart,\\nAbsent thee from felicitie awhile,\\nAnd in this harsh world draw thy breath in paine,\\nTo tell my Storie.\\n\\nMarch afarre off, and shout within.\\n\\nWhat warlike noyse is this?\\nEnter Osricke.\\n\\n  Osr. Yong Fortinbras, with conquest come fro[m] Poland\\nTo th' Ambassadors of England giues this warlike volly\\n\\n   Ham. O I dye Horatio:\\nThe potent poyson quite ore-crowes my spirit,\\nI cannot liue to heare the Newes from England,\\nBut I do prophesie th' election lights\\nOn Fortinbras, he ha's my dying voyce,\\nSo tell him with the occurrents more and lesse,\\nWhich haue solicited. The rest is silence. O, o, o, o.\\n\\nDyes\\n\\n  Hora. Now cracke a Noble heart:\\nGoodnight sweet Prince,\\nAnd flights of Angels sing thee to thy rest,\\nWhy do's the Drumme come hither?\\nEnter Fortinbras and English Ambassador, with Drumme, Colours,\\nand\\nAttendants.\\n\\n  Fortin. Where is this sight?\\n  Hor. What is it ye would see;\\nIf ought of woe, or wonder, cease your search\\n\\n   For. His quarry cries on hauocke. Oh proud death,\\nWhat feast is toward in thine eternall Cell.\\nThat thou so many Princes, at a shoote,\\nSo bloodily hast strooke\\n\\n   Amb. The sight is dismall,\\nAnd our affaires from England come too late,\\nThe eares are senselesse that should giue vs hearing,\\nTo tell him his command'ment is fulfill'd,\\nThat Rosincrance and Guildensterne are dead:\\nWhere should we haue our thankes?\\n  Hor. Not from his mouth,\\nHad it th' abilitie of life to thanke you:\\nHe neuer gaue command'ment for their death.\\nBut since so iumpe vpon this bloodie question,\\nYou from the Polake warres, and you from England\\nAre heere arriued. Giue order that these bodies\\nHigh on a stage be placed to the view,\\nAnd let me speake to th' yet vnknowing world,\\nHow these things came about. So shall you heare\\nOf carnall, bloudie, and vnnaturall acts,\\nOf accidentall iudgements, casuall slaughters\\nOf death's put on by cunning, and forc'd cause,\\nAnd in this vpshot, purposes mistooke,\\nFalne on the Inuentors head. All this can I\\nTruly deliuer\\n\\n   For. Let vs hast to heare it,\\nAnd call the Noblest to the Audience.\\nFor me, with sorrow, I embrace my Fortune,\\nI haue some Rites of memory in this Kingdome,\\nWhich are to claime, my vantage doth\\nInuite me,\\n  Hor. Of that I shall haue alwayes cause to speake,\\nAnd from his mouth\\nWhose voyce will draw on more:\\nBut let this same be presently perform'd,\\nEuen whiles mens mindes are wilde,\\nLest more mischance\\nOn plots, and errors happen\\n\\n   For. Let foure Captaines\\nBeare Hamlet like a Soldier to the Stage,\\nFor he was likely, had he beene put on\\nTo haue prou'd most royally:\\nAnd for his passage,\\nThe Souldiours Musicke, and the rites of Warre\\nSpeake lowdly for him.\\nTake vp the body; Such a sight as this\\nBecomes the Field, but heere shewes much amis.\\nGo, bid the Souldiers shoote.\\n\\nExeunt. Marching: after the which, a Peale of Ordenance are shot\\noff.\\n\\n\\nFINIS. The tragedie of HAMLET, Prince of Denmarke.\\n\")]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "doc=TextLoader(r'D:\\btch 16\\NPl\\hamlet.txt')\n",
    "fin_t= doc.load()\n",
    "fin_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ef33457b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='[The Tragedie of Hamlet by William Shakespeare 1599]\\n\\n\\nActus Primus. Scoena Prima.\\n\\nEnter Barnardo and Francisco two Centinels.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Barnardo. Who's there?\\n  Fran. Nay answer me: Stand & vnfold\\nyour selfe\\n\\n   Bar. Long liue the King\\n\\n   Fran. Barnardo?\\n  Bar. He\\n\\n   Fran. You come most carefully vpon your houre\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Bar. 'Tis now strook twelue, get thee to bed Francisco\\n\\n   Fran. For this releefe much thankes: 'Tis bitter cold,\\nAnd I am sicke at heart\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Barn. Haue you had quiet Guard?\\n  Fran. Not a Mouse stirring'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Barn. Well, goodnight. If you do meet Horatio and\\nMarcellus, the Riuals of my Watch, bid them make hast.\\nEnter Horatio and Marcellus.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Fran. I thinke I heare them. Stand: who's there?\\n  Hor. Friends to this ground\\n\\n   Mar. And Leige-men to the Dane\\n\\n   Fran. Giue you good night\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Mar. O farwel honest Soldier, who hath relieu'd you?\\n  Fra. Barnardo ha's my place: giue you goodnight.\\n\\nExit Fran.\\n\\n  Mar. Holla Barnardo\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Bar. Say, what is Horatio there?\\n  Hor. A peece of him\\n\\n   Bar. Welcome Horatio, welcome good Marcellus\\n\\n   Mar. What, ha's this thing appear'd againe to night\\n\\n   Bar. I haue seene nothing\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Mar. Horatio saies, 'tis but our Fantasie,\\nAnd will not let beleefe take hold of him\\nTouching this dreaded sight, twice seene of vs,\\nTherefore I haue intreated him along\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='With vs, to watch the minutes of this Night,\\nThat if againe this Apparition come,\\nHe may approue our eyes, and speake to it'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Hor. Tush, tush, 'twill not appeare\\n\\n   Bar. Sit downe a-while,\\nAnd let vs once againe assaile your eares,\\nThat are so fortified against our Story,\\nWhat we two Nights haue seene\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Hor. Well, sit we downe,\\nAnd let vs heare Barnardo speake of this'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Barn. Last night of all,\\nWhen yond same Starre that's Westward from the Pole\\nHad made his course t' illume that part of Heauen\\nWhere now it burnes, Marcellus and my selfe,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='The Bell then beating one'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Mar. Peace, breake thee of:\\nEnter the Ghost.\\n\\nLooke where it comes againe\\n\\n   Barn. In the same figure, like the King that's dead\\n\\n   Mar. Thou art a Scholler; speake to it Horatio\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Barn. Lookes it not like the King? Marke it Horatio\\n\\n   Hora. Most like: It harrowes me with fear & wonder\\n  Barn. It would be spoke too\\n\\n   Mar. Question it Horatio'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Hor. What art thou that vsurp'st this time of night,\\nTogether with that Faire and Warlike forme\\nIn which the Maiesty of buried Denmarke\\nDid sometimes march: By Heauen I charge thee speake\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Mar. It is offended\\n\\n   Barn. See, it stalkes away\\n\\n   Hor. Stay: speake; speake: I Charge thee, speake.\\n\\nExit the Ghost.\\n\\n  Mar. 'Tis gone, and will not answer\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Barn. How now Horatio? You tremble & look pale:\\nIs not this something more then Fantasie?\\nWhat thinke you on't?\\n  Hor. Before my God, I might not this beleeue\\nWithout the sensible and true auouch\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Of mine owne eyes'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Mar. Is it not like the King?\\n  Hor. As thou art to thy selfe,\\nSuch was the very Armour he had on,\\nWhen th' Ambitious Norwey combatted:\\nSo frown'd he once, when in an angry parle\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"He smot the sledded Pollax on the Ice.\\n'Tis strange\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Mar. Thus twice before, and iust at this dead houre,\\nWith Martiall stalke, hath he gone by our Watch'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Hor. In what particular thought to work, I know not:\\nBut in the grosse and scope of my Opinion,\\nThis boades some strange erruption to our State'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Mar. Good now sit downe, & tell me he that knowes\\nWhy this same strict and most obseruant Watch,\\nSo nightly toyles the subiect of the Land,\\nAnd why such dayly Cast of Brazon Cannon'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"And Forraigne Mart for Implements of warre:\\nWhy such impresse of Ship-wrights, whose sore Taske\\nDo's not diuide the Sunday from the weeke,\\nWhat might be toward, that this sweaty hast\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Doth make the Night ioynt-Labourer with the day:\\nWho is't that can informe me?\\n  Hor. That can I,\\nAt least the whisper goes so: Our last King,\\nWhose Image euen but now appear'd to vs,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Was (as you know) by Fortinbras of Norway,\\n(Thereto prick'd on by a most emulate Pride)\\nDar'd to the Combate. In which, our Valiant Hamlet,\\n(For so this side of our knowne world esteem'd him)\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Did slay this Fortinbras: who by a Seal'd Compact,\\nWell ratified by Law, and Heraldrie,\\nDid forfeite (with his life) all those his Lands\\nWhich he stood seiz'd on, to the Conqueror:\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Against the which, a Moity competent\\nWas gaged by our King: which had return'd\\nTo the Inheritance of Fortinbras,\\nHad he bin Vanquisher, as by the same Cou'nant\\nAnd carriage of the Article designe,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"His fell to Hamlet. Now sir, young Fortinbras,\\nOf vnimproued Mettle, hot and full,\\nHath in the skirts of Norway, heere and there,\\nShark'd vp a List of Landlesse Resolutes,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"For Foode and Diet, to some Enterprize\\nThat hath a stomacke in't: which is no other\\n(And it doth well appeare vnto our State)\\nBut to recouer of vs by strong hand\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='And termes Compulsatiue, those foresaid Lands\\nSo by his Father lost: and this (I take it)\\nIs the maine Motiue of our Preparations,\\nThe Sourse of this our Watch, and the cheefe head'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Of this post-hast, and Romage in the Land.\\nEnter Ghost againe.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='But soft, behold: Loe, where it comes againe:\\nIle crosse it, though it blast me. Stay Illusion:\\nIf thou hast any sound, or vse of Voyce,\\nSpeake to me. If there be any good thing to be done,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='That may to thee do ease, and grace to me; speak to me.\\nIf thou art priuy to thy Countries Fate\\n(Which happily foreknowing may auoyd) Oh speake.\\nOr, if thou hast vp-hoorded in thy life'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Extorted Treasure in the wombe of Earth,\\n(For which, they say, you Spirits oft walke in death)\\nSpeake of it. Stay, and speake. Stop it Marcellus'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Mar. Shall I strike at it with my Partizan?\\n  Hor. Do, if it will not stand\\n\\n   Barn. 'Tis heere\\n\\n   Hor. 'Tis heere\\n\\n   Mar. 'Tis gone.\\n\\nExit Ghost.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Exit Ghost.\\n\\nWe do it wrong, being so Maiesticall\\nTo offer it the shew of Violence,\\nFor it is as the Ayre, invulnerable,\\nAnd our vaine blowes, malicious Mockery'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Barn. It was about to speake, when the Cocke crew'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Hor. And then it started, like a guilty thing\\nVpon a fearfull Summons. I haue heard,\\nThe Cocke that is the Trumpet to the day,\\nDoth with his lofty and shrill-sounding Throate'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Awake the God of Day: and at his warning,\\nWhether in Sea, or Fire, in Earth, or Ayre,\\nTh' extrauagant, and erring Spirit, hyes\\nTo his Confine. And of the truth heerein,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='This present Obiect made probation'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Mar. It faded on the crowing of the Cocke.\\nSome sayes, that euer 'gainst that Season comes\\nWherein our Sauiours Birch is celebrated,\\nThe Bird of Dawning singeth all night long:\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"And then (they say) no Spirit can walke abroad,\\nThe nights are wholsome, then no Planets strike,\\nNo Faiery talkes, nor Witch hath power to Charme:\\nSo hallow'd, and so gracious is the time\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Hor. So haue I heard, and do in part beleeue it.\\nBut looke, the Morne in Russet mantle clad,\\nWalkes o're the dew of yon high Easterne Hill,\\nBreake we our Watch vp, and by my aduice\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Let vs impart what we haue seene to night\\nVnto yong Hamlet. For vpon my life,\\nThis Spirit dumbe to vs, will speake to him:\\nDo you consent we shall acquaint him with it,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"As needfull in our Loues, fitting our Duty?\\n  Mar. Let do't I pray, and I this morning know\\nWhere we shall finde him most conueniently.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Exeunt.\\n\\nScena Secunda.\\n\\nEnter Claudius King of Denmarke, Gertrude the Queene, Hamlet,\\nPolonius,\\nLaertes, and his Sister Ophelia, Lords Attendant.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='King. Though yet of Hamlet our deere Brothers death\\nThe memory be greene: and that it vs befitted\\nTo beare our hearts in greefe, and our whole Kingdome\\nTo be contracted in one brow of woe:'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Yet so farre hath Discretion fought with Nature,\\nThat we with wisest sorrow thinke on him,\\nTogether with remembrance of our selues.\\nTherefore our sometimes Sister, now our Queene,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Th' imperiall Ioyntresse of this warlike State,\\nHaue we, as 'twere, with a defeated ioy,\\nWith one Auspicious, and one Dropping eye,\\nWith mirth in Funerall, and with Dirge in Marriage,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"In equall Scale weighing Delight and Dole\\nTaken to Wife; nor haue we heerein barr'd\\nYour better Wisedomes, which haue freely gone\\nWith this affaire along, for all our Thankes.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Now followes, that you know young Fortinbras,\\nHolding a weake supposall of our worth;\\nOr thinking by our late deere Brothers death,\\nOur State to be disioynt, and out of Frame,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Colleagued with the dreame of his Aduantage;\\nHe hath not fayl'd to pester vs with Message,\\nImporting the surrender of those Lands\\nLost by his Father: with all Bonds of Law\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='To our most valiant Brother. So much for him.\\nEnter Voltemand and Cornelius.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Now for our selfe, and for this time of meeting\\nThus much the businesse is. We haue heere writ\\nTo Norway, Vncle of young Fortinbras,\\nWho Impotent and Bedrid, scarsely heares'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Of this his Nephewes purpose, to suppresse\\nHis further gate heerein. In that the Leuies,\\nThe Lists, and full proportions are all made\\nOut of his subiect: and we heere dispatch'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='You good Cornelius, and you Voltemand,\\nFor bearing of this greeting to old Norway,\\nGiuing to you no further personall power\\nTo businesse with the King, more then the scope'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Of these dilated Articles allow:\\nFarewell, and let your hast commend your duty'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Volt. In that, and all things, will we shew our duty\\n\\n   King. We doubt it nothing, heartily farewell.\\n\\nExit Voltemand and Cornelius.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"And now Laertes, what's the newes with you?\\nYou told vs of some suite. What is't Laertes?\\nYou cannot speake of Reason to the Dane,\\nAnd loose your voyce. What would'st thou beg Laertes,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='That shall not be my Offer, not thy Asking?\\nThe Head is not more Natiue to the Heart,\\nThe Hand more instrumentall to the Mouth,\\nThen is the Throne of Denmarke to thy Father.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"What would'st thou haue Laertes?\\n  Laer. Dread my Lord,\\nYour leaue and fauour to returne to France,\\nFrom whence, though willingly I came to Denmarke\\nTo shew my duty in your Coronation,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Yet now I must confesse, that duty done,\\nMy thoughts and wishes bend againe towards France,\\nAnd bow them to your gracious leaue and pardon'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='King. Haue you your Fathers leaue?\\nWhat sayes Pollonius?\\n  Pol. He hath my Lord:\\nI do beseech you giue him leaue to go'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='King. Take thy faire houre Laertes, time be thine,\\nAnd thy best graces spend it at thy will:\\nBut now my Cosin Hamlet, and my Sonne?\\n  Ham. A little more then kin, and lesse then kinde'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"King. How is it that the Clouds still hang on you?\\n  Ham. Not so my Lord, I am too much i'th' Sun\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Queen. Good Hamlet cast thy nightly colour off,\\nAnd let thine eye looke like a Friend on Denmarke.\\nDo not for euer with thy veyled lids\\nSeeke for thy Noble Father in the dust;'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Thou know'st 'tis common, all that liues must dye,\\nPassing through Nature, to Eternity\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. I Madam, it is common\\n\\n   Queen. If it be;\\nWhy seemes it so particular with thee'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Seemes Madam? Nay, it is: I know not Seemes:\\n'Tis not alone my Inky Cloake (good Mother)\\nNor Customary suites of solemne Blacke,\\nNor windy suspiration of forc'd breath,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='No, nor the fruitfull Riuer in the Eye,\\nNor the deiected hauiour of the Visage,\\nTogether with all Formes, Moods, shewes of Griefe,\\nThat can denote me truly. These indeed Seeme,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='For they are actions that a man might play:\\nBut I haue that Within, which passeth show;\\nThese, but the Trappings, and the Suites of woe'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"King. 'Tis sweet and commendable\\nIn your Nature Hamlet,\\nTo giue these mourning duties to your Father:\\nBut you must know, your Father lost a Father,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='That Father lost, lost his, and the Suruiuer bound\\nIn filiall Obligation, for some terme\\nTo do obsequious Sorrow. But to perseuer\\nIn obstinate Condolement, is a course'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Of impious stubbornnesse. 'Tis vnmanly greefe,\\nIt shewes a will most incorrect to Heauen,\\nA Heart vnfortified, a Minde impatient,\\nAn Vnderstanding simple, and vnschool'd:\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"For, what we know must be, and is as common\\nAs any the most vulgar thing to sence,\\nWhy should we in our peeuish Opposition\\nTake it to heart? Fye, 'tis a fault to Heauen,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='A fault against the Dead, a fault to Nature,\\nTo Reason most absurd, whose common Theame\\nIs death of Fathers, and who still hath cried,\\nFrom the first Coarse, till he that dyed to day,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='This must be so. We pray you throw to earth\\nThis vnpreuayling woe, and thinke of vs\\nAs of a Father; For let the world take note,\\nYou are the most immediate to our Throne,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='And with no lesse Nobility of Loue,\\nThen that which deerest Father beares his Sonne,\\nDo I impart towards you. For your intent\\nIn going backe to Schoole in Wittenberg,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='It is most retrograde to our desire:\\nAnd we beseech you, bend you to remaine\\nHeere in the cheere and comfort of our eye,\\nOur cheefest Courtier Cosin, and our Sonne'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Qu. Let not thy Mother lose her Prayers Hamlet:\\nI prythee stay with vs, go not to Wittenberg\\n\\n   Ham. I shall in all my best\\nObey you Madam'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"King. Why 'tis a louing, and a faire Reply,\\nBe as our selfe in Denmarke. Madam come,\\nThis gentle and vnforc'd accord of Hamlet\\nSits smiling to my heart; in grace whereof,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='No iocond health that Denmarke drinkes to day,\\nBut the great Cannon to the Clowds shall tell,\\nAnd the Kings Rouce, the Heauens shall bruite againe,\\nRespeaking earthly Thunder. Come away.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Exeunt.\\n\\nManet Hamlet.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Oh that this too too solid Flesh, would melt,\\nThaw, and resolue it selfe into a Dew:\\nOr that the Euerlasting had not fixt\\nHis Cannon 'gainst Selfe-slaughter. O God, O God!\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"How weary, stale, flat, and vnprofitable\\nSeemes to me all the vses of this world?\\nFie on't? Oh fie, fie, 'tis an vnweeded Garden\\nThat growes to Seed: Things rank, and grosse in Nature\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Possesse it meerely. That it should come to this:\\nBut two months dead: Nay, not so much; not two,\\nSo excellent a King, that was to this\\nHiperion to a Satyre: so louing to my Mother,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='That he might not beteene the windes of heauen\\nVisit her face too roughly. Heauen and Earth\\nMust I remember: why she would hang on him,\\nAs if encrease of Appetite had growne'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"By what is fed on; and yet within a month?\\nLet me not thinke on't: Frailty, thy name is woman.\\nA little Month, or ere those shooes were old,\\nWith which she followed my poore Fathers body\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Like Niobe, all teares. Why she, euen she.\\n(O Heauen! A beast that wants discourse of Reason\\nWould haue mourn'd longer) married with mine Vnkle,\\nMy Fathers Brother: but no more like my Father,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Then I to Hercules. Within a Moneth?\\nEre yet the salt of most vnrighteous Teares\\nHad left the flushing of her gauled eyes,\\nShe married. O most wicked speed, to post'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='With such dexterity to Incestuous sheets:\\nIt is not, nor it cannot come to good.\\nBut breake my heart, for I must hold my tongue.\\nEnter Horatio, Barnardo, and Marcellus.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Hor. Haile to your Lordship\\n\\n   Ham. I am glad to see you well:\\nHoratio, or I do forget my selfe\\n\\n   Hor. The same my Lord,\\nAnd your poore Seruant euer'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. Sir my good friend,\\nIle change that name with you:\\nAnd what make you from Wittenberg Horatio?\\nMarcellus\\n\\n   Mar. My good Lord'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. I am very glad to see you: good euen Sir.\\nBut what in faith make you from Wittemberge?\\n  Hor. A truant disposition, good my Lord'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. I would not haue your Enemy say so;\\nNor shall you doe mine eare that violence,\\nTo make it truster of your owne report\\nAgainst your selfe. I know you are no Truant:'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"But what is your affaire in Elsenour?\\nWee'l teach you to drinke deepe, ere you depart\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Hor. My Lord, I came to see your Fathers Funerall\\n\\n   Ham. I pray thee doe not mock me (fellow Student)\\nI thinke it was to see my Mothers Wedding\\n\\n   Hor. Indeed my Lord, it followed hard vpon'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. Thrift thrift Horatio: the Funerall Bakt-meats\\nDid coldly furnish forth the Marriage Tables;\\nWould I had met my dearest foe in heauen,\\nEre I had euer seene that day Horatio.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='My father, me thinkes I see my father'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Hor. Oh where my Lord?\\n  Ham. In my minds eye (Horatio)\\n  Hor. I saw him once; he was a goodly King\\n\\n   Ham. He was a man, take him for all in all:\\nI shall not look vpon his like againe'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Hor. My Lord, I thinke I saw him yesternight\\n\\n   Ham. Saw? Who?\\n  Hor. My Lord, the King your Father'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. The King my Father?\\n  Hor. Season your admiration for a while\\nWith an attent eare; till I may deliuer\\nVpon the witnesse of these Gentlemen,\\nThis maruell to you'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. For Heauens loue let me heare'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Hor. Two nights together, had these Gentlemen\\n(Marcellus and Barnardo) on their Watch\\nIn the dead wast and middle of the night\\nBeene thus encountred. A figure like your Father,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Arm'd at all points exactly, Cap a Pe,\\nAppeares before them, and with sollemne march\\nGoes slow and stately: By them thrice he walkt,\\nBy their opprest and feare-surprized eyes,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Within his Truncheons length; whilst they bestil'd\\nAlmost to Ielly with the Act of feare,\\nStand dumbe and speake not to him. This to me\\nIn dreadfull secrecie impart they did,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"And I with them the third Night kept the Watch,\\nWhereas they had deliuer'd both in time,\\nForme of the thing; each word made true and good,\\nThe Apparition comes. I knew your Father:\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='These hands are not more like'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. But where was this?\\n  Mar. My Lord vpon the platforme where we watcht'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. Did you not speake to it?\\n  Hor. My Lord, I did;\\nBut answere made it none: yet once me thought\\nIt lifted vp it head, and did addresse\\nIt selfe to motion, like as it would speake:'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='But euen then, the Morning Cocke crew lowd;\\nAnd at the sound it shrunke in hast away,\\nAnd vanisht from our sight'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Tis very strange\\n\\n   Hor. As I doe liue my honourd Lord 'tis true;\\nAnd we did thinke it writ downe in our duty\\nTo let you know of it\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Indeed, indeed Sirs; but this troubles me.\\nHold you the watch to Night?\\n  Both. We doe my Lord\\n\\n   Ham. Arm'd, say you?\\n  Both. Arm'd, my Lord\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. From top to toe?\\n  Both. My Lord, from head to foote\\n\\n   Ham. Then saw you not his face?\\n  Hor. O yes, my Lord, he wore his Beauer vp'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. What, lookt he frowningly?\\n  Hor. A countenance more in sorrow then in anger\\n\\n   Ham. Pale, or red?\\n  Hor. Nay very pale\\n\\n   Ham. And fixt his eyes vpon you?\\n  Hor. Most constantly'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. I would I had beene there\\n\\n   Hor. It would haue much amaz'd you\\n\\n   Ham. Very like, very like: staid it long?\\n  Hor. While one with moderate hast might tell a hundred\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"All. Longer, longer\\n\\n   Hor. Not when I saw't\\n\\n   Ham. His Beard was grisly? no\\n\\n   Hor. It was, as I haue seene it in his life,\\nA Sable Siluer'd\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Ile watch to Night; perchance 'twill wake againe\\n\\n   Hor. I warrant you it will\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. If it assume my noble Fathers person,\\nIle speake to it, though Hell it selfe should gape\\nAnd bid me hold my peace. I pray you all,\\nIf you haue hitherto conceald this sight;'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Let it bee treble in your silence still:\\nAnd whatsoeuer els shall hap to night,\\nGiue it an vnderstanding but no tongue;\\nI will requite your loues; so fare ye well:'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Vpon the Platforme twixt eleuen and twelue,\\nIle visit you'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='All. Our duty to your Honour.\\n\\nExeunt'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. Your loue, as mine to you: farewell.\\nMy Fathers Spirit in Armes? All is not well:\\nI doubt some foule play: would the Night were come;\\nTill then sit still my soule; foule deeds will rise,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Though all the earth orewhelm them to mens eies.\\nEnter.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Scena Tertia\\n\\n\\nEnter Laertes and Ophelia.\\n\\n  Laer. My necessaries are imbark't; Farewell:\\nAnd Sister, as the Winds giue Benefit,\\nAnd Conuoy is assistant; doe not sleepe,\\nBut let me heare from you\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ophel. Doe you doubt that?\\n  Laer. For Hamlet, and the trifling of his fauours,\\nHold it a fashion and a toy in Bloude;\\nA Violet in the youth of Primy Nature;'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Froward, not permanent; sweet not lasting\\nThe suppliance of a minute? No more'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ophel. No more but so'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Laer. Thinke it no more:\\nFor nature cressant does not grow alone,\\nIn thewes and Bulke: but as his Temple waxes,\\nThe inward seruice of the Minde and Soule'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Growes wide withall. Perhaps he loues you now,\\nAnd now no soyle nor cautell doth besmerch\\nThe vertue of his feare: but you must feare\\nHis greatnesse weigh'd, his will is not his owne;\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='For hee himselfe is subiect to his Birth:\\nHee may not, as vnuallued persons doe,\\nCarue for himselfe; for, on his choyce depends\\nThe sanctity and health of the whole State.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"And therefore must his choyce be circumscrib'd\\nVnto the voyce and yeelding of that Body,\\nWhereof he is the Head. Then if he sayes he loues you,\\nIt fits your wisedome so farre to beleeue it;\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='As he in his peculiar Sect and force\\nMay giue his saying deed: which is no further,\\nThen the maine voyce of Denmarke goes withall.\\nThen weight what losse your Honour may sustaine,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='If with too credent eare you list his Songs;\\nOr lose your Heart; or your chast Treasure open\\nTo his vnmastred importunity.\\nFeare it Ophelia, feare it my deare Sister,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='And keepe within the reare of your Affection;\\nOut of the shot and danger of Desire.\\nThe chariest Maid is Prodigall enough,\\nIf she vnmaske her beauty to the Moone:'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Vertue it selfe scapes not calumnious stroakes,\\nThe Canker Galls, the Infants of the Spring\\nToo oft before the buttons be disclos'd,\\nAnd in the Morne and liquid dew of Youth,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Contagious blastments are most imminent.\\nBe wary then, best safety lies in feare;\\nYouth to it selfe rebels, though none else neere'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ophe. I shall th' effect of this good Lesson keepe,\\nAs watchmen to my heart: but good my Brother\\nDoe not as some vngracious Pastors doe,\\nShew me the steepe and thorny way to Heauen;\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Whilst like a puft and recklesse Libertine\\nHimselfe, the Primrose path of dalliance treads,\\nAnd reaks not his owne reade'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Laer. Oh, feare me not.\\nEnter Polonius.\\n\\nI stay too long; but here my Father comes:\\nA double blessing is a double grace;\\nOccasion smiles vpon a second leaue'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Polon. Yet heere Laertes? Aboord, aboord for shame,\\nThe winde sits in the shoulder of your saile,\\nAnd you are staid for there: my blessing with you;\\nAnd these few Precepts in thy memory,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"See thou Character. Giue thy thoughts no tongue,\\nNor any vnproportion'd thoughts his Act:\\nBe thou familiar; but by no meanes vulgar:\\nThe friends thou hast, and their adoption tride,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Grapple them to thy Soule, with hoopes of Steele:\\nBut doe not dull thy palme, with entertainment\\nOf each vnhatch't, vnfledg'd Comrade. Beware\\nOf entrance to a quarrell: but being in\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Bear't that th' opposed may beware of thee.\\nGiue euery man thine eare; but few thy voyce:\\nTake each mans censure; but reserue thy iudgement:\\nCostly thy habit as thy purse can buy;\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='But not exprest in fancie; rich, not gawdie:\\nFor the Apparell oft proclaimes the man.\\nAnd they in France of the best ranck and station,\\nAre of a most select and generous cheff in that.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Neither a borrower, nor a lender be;\\nFor lone oft loses both it selfe and friend:\\nAnd borrowing duls the edge of Husbandry.\\nThis aboue all; to thine owne selfe be true:'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='And it must follow, as the Night the Day,\\nThou canst not then be false to any man.\\nFarewell: my Blessing season this in thee'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Laer. Most humbly doe I take my leaue, my Lord\\n\\n   Polon. The time inuites you, goe, your seruants tend\\n\\n   Laer. Farewell Ophelia, and remember well\\nWhat I haue said to you'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ophe. Tis in my memory lockt,\\nAnd you your selfe shall keepe the key of it\\n\\n   Laer. Farewell.\\n\\nExit Laer.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Exit Laer.\\n\\n  Polon. What ist Ophelia he hath said to you?\\n  Ophe. So please you, somthing touching the L[ord]. Hamlet'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Polon. Marry, well bethought:\\nTis told me he hath very oft of late\\nGiuen priuate time to you; and you your selfe\\nHaue of your audience beene most free and bounteous.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='If it be so, as so tis put on me;\\nAnd that in way of caution: I must tell you,\\nYou doe not vnderstand your selfe so cleerely,\\nAs it behoues my Daughter, and your Honour.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='What is betweene you, giue me vp the truth?\\n  Ophe. He hath my Lord of late, made many tenders\\nOf his affection to me'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Polon. Affection, puh. You speake like a greene Girle,\\nVnsifted in such perillous Circumstance.\\nDoe you beleeue his tenders, as you call them?\\n  Ophe. I do not know, my Lord, what I should thinke'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Polon. Marry Ile teach you; thinke your selfe a Baby,\\nThat you haue tane his tenders for true pay,\\nWhich are not starling. Tender your selfe more dearly;'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Or not to crack the winde of the poore Phrase,\\nRoaming it thus, you'l tender me a foole\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ophe. My Lord, he hath importun'd me with loue,\\nIn honourable fashion\\n\\n   Polon. I, fashion you may call it, go too, go too\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ophe. And hath giuen countenance to his speech,\\nMy Lord, with all the vowes of Heauen'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Polon. I, Springes to catch Woodcocks. I doe know\\nWhen the Bloud burnes, how Prodigall the Soule\\nGiues the tongue vowes: these blazes, Daughter,\\nGiuing more light then heate; extinct in both,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Euen in their promise, as it is a making;\\nYou must not take for fire. For this time Daughter,\\nBe somewhat scanter of your Maiden presence;\\nSet your entreatments at a higher rate,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Then a command to parley. For Lord Hamlet,\\nBeleeue so much in him, that he is young,\\nAnd with a larger tether may he walke,\\nThen may be giuen you. In few, Ophelia,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Doe not beleeue his vowes; for they are Broakers,\\nNot of the eye, which their Inuestments show:\\nBut meere implorators of vnholy Sutes,\\nBreathing like sanctified and pious bonds,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='The better to beguile. This is for all:\\nI would not, in plaine tearmes, from this time forth,\\nHaue you so slander any moment leisure,\\nAs to giue words or talke with the Lord Hamlet:'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Looke too't, I charge you; come your wayes\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ophe. I shall obey my Lord.\\n\\nExeunt.\\n\\nEnter Hamlet, Horatio, Marcellus.\\n\\n  Ham. The Ayre bites shrewdly: is it very cold?\\n  Hor. It is a nipping and an eager ayre'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. What hower now?\\n  Hor. I thinke it lacks of twelue\\n\\n   Mar. No, it is strooke'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Hor. Indeed I heard it not: then it drawes neere the season,\\nWherein the Spirit held his wont to walke.\\nWhat does this meane my Lord?\\n  Ham. The King doth wake to night, and takes his rouse,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Keepes wassels and the swaggering vpspring reeles,\\nAnd as he dreines his draughts of Renish downe,\\nThe kettle Drum and Trumpet thus bray out\\nThe triumph of his Pledge'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Horat. Is it a custome?\\n  Ham. I marry ist;\\nAnd to my mind, though I am natiue heere,\\nAnd to the manner borne: It is a Custome\\nMore honour'd in the breach, then the obseruance.\\nEnter Ghost.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Hor. Looke my Lord, it comes'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Angels and Ministers of Grace defend vs:\\nBe thou a Spirit of health, or Goblin damn'd,\\nBring with thee ayres from Heauen, or blasts from Hell,\\nBe thy euents wicked or charitable,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Thou com'st in such a questionable shape\\nThat I will speake to thee. Ile call thee Hamlet,\\nKing, Father, Royall Dane: Oh, oh, answer me,\\nLet me not burst in Ignorance; but tell\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Why thy Canoniz'd bones Hearsed in death,\\nHaue burst their cerments, why the Sepulcher\\nWherein we saw thee quietly enurn'd,\\nHath op'd his ponderous and Marble iawes,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='To cast thee vp againe? What may this meane?\\nThat thou dead Coarse againe in compleat steele,\\nReuisits thus the glimpses of the Moone,\\nMaking Night hidious? And we fooles of Nature,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='So horridly to shake our disposition,\\nWith thoughts beyond thee; reaches of our Soules,\\nSay, why is this? wherefore? what should we doe?'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ghost beckens Hamlet.\\n\\n  Hor. It beckons you to goe away with it,\\nAs if it some impartment did desire\\nTo you alone'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Mar. Looke with what courteous action\\nIt wafts you to a more remoued ground:\\nBut doe not goe with it\\n\\n   Hor. No, by no meanes\\n\\n   Ham. It will not speake: then will I follow it'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Hor. Doe not my Lord'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. Why, what should be the feare?\\nI doe not set my life at a pins fee;\\nAnd for my Soule, what can it doe to that?\\nBeing a thing immortall as it selfe:\\nIt waues me forth againe; Ile follow it'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Hor. What if it tempt you toward the Floud my Lord?\\nOr to the dreadfull Sonnet of the Cliffe,\\nThat beetles o're his base into the Sea,\\nAnd there assumes some other horrible forme,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Which might depriue your Soueraignty of Reason,\\nAnd draw you into madnesse thinke of it?\\n  Ham. It wafts me still: goe on, Ile follow thee'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Mar. You shall not goe my Lord\\n\\n   Ham. Hold off your hand\\n\\n   Hor. Be rul'd, you shall not goe\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. My fate cries out,\\nAnd makes each petty Artire in this body,\\nAs hardy as the Nemian Lions nerue:\\nStill am I cal'd? Vnhand me Gentlemen:\\nBy Heau'n, Ile make a Ghost of him that lets me:\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='I say away, goe on, Ile follow thee.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Exeunt. Ghost & Hamlet.\\n\\n  Hor. He waxes desperate with imagination\\n\\n   Mar. Let's follow; 'tis not fit thus to obey him\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Hor. Haue after, to what issue will this come?\\n  Mar. Something is rotten in the State of Denmarke\\n\\n   Hor. Heauen will direct it\\n\\n   Mar. Nay, let's follow him.\\n\\nExeunt.\\n\\nEnter Ghost and Hamlet.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. Where wilt thou lead me? speak; Ile go no further\\n\\n   Gho. Marke me\\n\\n   Ham. I will\\n\\n   Gho. My hower is almost come,\\nWhen I to sulphurous and tormenting Flames\\nMust render vp my selfe'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. Alas poore Ghost\\n\\n   Gho. Pitty me not, but lend thy serious hearing\\nTo what I shall vnfold\\n\\n   Ham. Speake, I am bound to heare\\n\\n   Gho. So art thou to reuenge, when thou shalt heare'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. What?\\n  Gho. I am thy Fathers Spirit,\\nDoom'd for a certaine terme to walke the night;\\nAnd for the day confin'd to fast in Fiers,\\nTill the foule crimes done in my dayes of Nature\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Are burnt and purg'd away? But that I am forbid\\nTo tell the secrets of my Prison-House;\\nI could a Tale vnfold, whose lightest word\\nWould harrow vp thy soule, freeze thy young blood,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Make thy two eyes like Starres, start from their Spheres,\\nThy knotty and combined lockes to part,\\nAnd each particular haire to stand an end,\\nLike Quilles vpon the fretfull Porpentine:'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='But this eternall blason must not be\\nTo eares of flesh and bloud; list Hamlet, oh list,\\nIf thou didst euer thy deare Father loue'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. Oh Heauen!\\n  Gho. Reuenge his foule and most vnnaturall Murther\\n\\n   Ham. Murther?\\n  Ghost. Murther most foule, as in the best it is;\\nBut this most foule, strange, and vnnaturall'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. Hast, hast me to know it,\\nThat with wings as swift\\nAs meditation, or the thoughts of Loue,\\nMay sweepe to my Reuenge'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ghost. I finde thee apt,\\nAnd duller should'st thou be then the fat weede\\nThat rots it selfe in ease, on Lethe Wharfe,\\nWould'st thou not stirre in this. Now Hamlet heare:\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"It's giuen out, that sleeping in mine Orchard,\\nA Serpent stung me: so the whole eare of Denmarke,\\nIs by a forged processe of my death\\nRankly abus'd: But know thou Noble youth,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='The Serpent that did sting thy Fathers life,\\nNow weares his Crowne'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. O my Propheticke soule: mine Vncle?\\n  Ghost. I that incestuous, that adulterate Beast\\nWith witchcraft of his wits, hath Traitorous guifts.\\nOh wicked Wit, and Gifts, that haue the power'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='So to seduce? Won to this shamefull Lust\\nThe will of my most seeming vertuous Queene:\\nOh Hamlet, what a falling off was there,\\nFrom me, whose loue was of that dignity,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='That it went hand in hand, euen with the Vow\\nI made to her in Marriage; and to decline\\nVpon a wretch, whose Naturall gifts were poore\\nTo those of mine. But Vertue, as it neuer wil be moued,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Though Lewdnesse court it in a shape of Heauen:\\nSo Lust, though to a radiant Angell link'd,\\nWill sate it selfe in a Celestiall bed, & prey on Garbage.\\nBut soft, me thinkes I sent the Mornings Ayre;\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Briefe let me be: Sleeping within mine Orchard,\\nMy custome alwayes in the afternoone;\\nVpon my secure hower thy Vncle stole\\nWith iuyce of cursed Hebenon in a Violl,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='And in the Porches of mine eares did poure\\nThe leaperous Distilment; whose effect\\nHolds such an enmity with bloud of Man,\\nThat swift as Quick-siluer, it courses through'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='The naturall Gates and Allies of the body;\\nAnd with a sodaine vigour it doth posset\\nAnd curd, like Aygre droppings into Milke,\\nThe thin and wholsome blood: so did it mine;'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"And a most instant Tetter bak'd about,\\nMost Lazar-like, with vile and loathsome crust,\\nAll my smooth Body.\\nThus was I, sleeping, by a Brothers hand,\\nOf Life, of Crowne, and Queene at once dispatcht;\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Cut off euen in the Blossomes of my Sinne,\\nVnhouzzled, disappointed, vnnaneld,\\nNo reckoning made, but sent to my account\\nWith all my imperfections on my head;\\nOh horrible Oh horrible, most horrible:'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='If thou hast nature in thee beare it not;\\nLet not the Royall Bed of Denmarke be\\nA Couch for Luxury and damned Incest.\\nBut howsoeuer thou pursuest this Act,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Taint not thy mind; nor let thy Soule contriue\\nAgainst thy Mother ought; leaue her to heauen,\\nAnd to those Thornes that in her bosome lodge,\\nTo pricke and sting her. Fare thee well at once;'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='The Glow-worme showes the Matine to be neere,\\nAnd gins to pale his vneffectuall Fire:\\nAdue, adue, Hamlet: remember me.\\nEnter.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. Oh all you host of Heauen! Oh Earth; what els?\\nAnd shall I couple Hell? Oh fie: hold my heart;\\nAnd you my sinnewes, grow not instant Old;\\nBut beare me stiffely vp: Remember thee?'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='I, thou poore Ghost, while memory holds a seate\\nIn this distracted Globe: Remember thee?\\nYea, from the Table of my Memory,\\nIle wipe away all triuiall fond Records,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='All sawes of Bookes, all formes, all presures past,\\nThat youth and obseruation coppied there;\\nAnd thy Commandment all alone shall liue\\nWithin the Booke and Volume of my Braine,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Vnmixt with baser matter; yes yes, by Heauen:\\nOh most pernicious woman!\\nOh Villaine, Villaine, smiling damned Villaine!\\nMy Tables, my Tables; meet it is I set it downe,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"That one may smile, and smile and be a Villaine;\\nAt least I'm sure it may be so in Denmarke;\\nSo Vnckle there you are: now to my word;\\nIt is; Adue, Adue, Remember me: I haue sworn't\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Hor. & Mar. within. My Lord, my Lord.\\nEnter Horatio and Marcellus.\\n\\n  Mar. Lord Hamlet\\n\\n   Hor. Heauen secure him\\n\\n   Mar. So be it\\n\\n   Hor. Illo, ho, ho, my Lord'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Hillo, ho, ho, boy; come bird, come\\n\\n   Mar. How ist my Noble Lord?\\n  Hor. What newes, my Lord?\\n  Ham. Oh wonderfull!\\n  Hor. Good my Lord tell it\\n\\n   Ham. No you'l reueale it\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Hor. Not I, my Lord, by Heauen\\n\\n   Mar. Nor I, my Lord\\n\\n   Ham. How say you then, would heart of man once think it?\\nBut you'l be secret?\\n  Both. I, by Heau'n, my Lord\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. There's nere a villaine dwelling in all Denmarke\\nBut hee's an arrant knaue\\n\\n   Hor. There needs no Ghost my Lord, come from the\\nGraue, to tell vs this\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Why right, you are i'th' right;\\nAnd so, without more circumstance at all,\\nI hold it fit that we shake hands, and part:\\nYou, as your busines and desires shall point you:\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"For euery man ha's businesse and desire,\\nSuch as it is: and for mine owne poore part,\\nLooke you, Ile goe pray\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Hor. These are but wild and hurling words, my Lord\\n\\n   Ham. I'm sorry they offend you heartily:\\nYes faith, heartily\\n\\n   Hor. There's no offence my Lord\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. Yes, by Saint Patricke, but there is my Lord,\\nAnd much offence too, touching this Vision heere:\\nIt is an honest Ghost, that let me tell you:\\nFor your desire to know what is betweene vs,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"O'remaster't as you may. And now good friends,\\nAs you are Friends, Schollers and Soldiers,\\nGiue me one poore request\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Hor. What is't my Lord? we will\\n\\n   Ham. Neuer make known what you haue seen to night\\n\\n   Both. My Lord, we will not\\n\\n   Ham. Nay, but swear't\\n\\n   Hor. Infaith my Lord, not I\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Mar. Nor I my Lord: in faith\\n\\n   Ham. Vpon my sword\\n\\n   Marcell. We haue sworne my Lord already\\n\\n   Ham. Indeed, vpon my sword, Indeed\\n\\n   Gho. Sweare.\\n\\nGhost cries vnder the Stage.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. Ah ha boy, sayest thou so. Art thou there truepenny?\\nCome one you here this fellow in the selleredge\\nConsent to sweare\\n\\n   Hor. Propose the Oath my Lord'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. Neuer to speake of this that you haue seene.\\nSweare by my sword\\n\\n   Gho. Sweare'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Gho. Sweare\\n\\n   Ham. Hic & vbique? Then wee'l shift for grownd,\\nCome hither Gentlemen,\\nAnd lay your hands againe vpon my sword,\\nNeuer to speake of this that you haue heard:\\nSweare by my Sword\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Gho. Sweare\\n\\n   Ham. Well said old Mole, can'st worke i'th' ground so fast?\\nA worthy Pioner, once more remoue good friends\\n\\n   Hor. Oh day and night: but this is wondrous strange\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. And therefore as a stranger giue it welcome.\\nThere are more things in Heauen and Earth, Horatio,\\nThen are dream't of in our Philosophy. But come,\\nHere as before, neuer so helpe you mercy,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='How strange or odde so ere I beare my selfe;\\n(As I perchance heereafter shall thinke meet\\nTo put an Anticke disposition on:)\\nThat you at such time seeing me, neuer shall'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='With Armes encombred thus, or thus, head shake;\\nOr by pronouncing of some doubtfull Phrase;\\nAs well, we know, or we could and if we would,\\nOr if we list to speake; or there be and if there might,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Or such ambiguous giuing out to note,\\nThat you know ought of me; this not to doe:\\nSo grace and mercy at your most neede helpe you:\\nSweare'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ghost. Sweare'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Rest, rest perturbed Spirit: so Gentlemen,\\nWith all my loue I doe commend me to you;\\nAnd what so poore a man as Hamlet is,\\nMay doe t' expresse his loue and friending to you,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='God willing shall not lacke: let vs goe in together,\\nAnd still your fingers on your lippes I pray,\\nThe time is out of ioynt: Oh cursed spight,\\nThat euer I was borne to set it right.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Nay, come let's goe together.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Exeunt.\\n\\n\\nActus Secundus.\\n\\nEnter Polonius, and Reynoldo.\\n\\n  Polon. Giue him his money, and these notes Reynoldo\\n\\n   Reynol. I will my Lord'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Polon. You shall doe maruels wisely: good Reynoldo,\\nBefore you visite him you make inquiry\\nOf his behauiour\\n\\n   Reynol. My Lord, I did intend it'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Polon. Marry, well said;\\nVery well said. Looke you Sir,\\nEnquire me first what Danskers are in Paris;\\nAnd how, and who; what meanes; and where they keepe:'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='What company, at what expence: and finding\\nBy this encompassement and drift of question,\\nThat they doe know my sonne: Come you more neerer\\nThen your particular demands will touch it,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Take you as 'twere some distant knowledge of him,\\nAnd thus I know his father and his friends,\\nAnd in part him. Doe you marke this Reynoldo?\\n  Reynol. I, very well my Lord\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Polon. And in part him, but you may say not well;\\nBut if't be hee I meane, hees very wilde;\\nAddicted so and so; and there put on him\\nWhat forgeries you please; marry, none so ranke,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='As may dishonour him; take heed of that:\\nBut Sir, such wanton, wild, and vsuall slips,\\nAs are Companions noted and most knowne\\nTo youth and liberty'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Reynol. As gaming my Lord\\n\\n   Polon. I, or drinking, fencing, swearing,\\nQuarelling, drabbing. You may goe so farre\\n\\n   Reynol. My Lord that would dishonour him'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Polon. Faith no, as you may season it in the charge;\\nYou must not put another scandall on him,\\nThat hee is open to Incontinencie;\\nThat's not my meaning: but breath his faults so quaintly,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"That they may seeme the taints of liberty;\\nThe flash and out-breake of a fiery minde,\\nA sauagenes in vnreclaim'd bloud of generall assault\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Reynol. But my good Lord\\n\\n   Polon. Wherefore should you doe this?\\n  Reynol. I my Lord, I would know that'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Polon. Marry Sir, heere's my drift,\\nAnd I belieue it is a fetch of warrant:\\nYou laying these slight sulleyes on my Sonne,\\nAs 'twere a thing a little soil'd i'th' working:\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Marke you your party in conuerse; him you would sound,\\nHauing euer seene. In the prenominate crimes,\\nThe youth you breath of guilty, be assur'd\\nHe closes with you in this consequence:\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Good sir, or so, or friend, or Gentleman.\\nAccording to the Phrase and the Addition,\\nOf man and Country'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Reynol. Very good my Lord'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Polon. And then Sir does he this?\\nHe does: what was I about to say?\\nI was about say somthing: where did I leaue?\\n  Reynol. At closes in the consequence:\\nAt friend, or so, and Gentleman'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Polon. At closes in the consequence, I marry,\\nHe closes with you thus. I know the Gentleman,\\nI saw him yesterday, or tother day;\\nOr then or then, with such and such; and as you say,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"There was he gaming, there o'retooke in's Rouse,\\nThere falling out at Tennis; or perchance,\\nI saw him enter such a house of saile;\\nVidelicet, a Brothell, or so forth. See you now;\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Your bait of falshood, takes this Cape of truth;\\nAnd thus doe we of wisedome and of reach\\nWith windlesses, and with assaies of Bias,\\nBy indirections finde directions out:'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='So by my former Lecture and aduice\\nShall you my Sonne; you haue me, haue you not?\\n  Reynol. My Lord I haue'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Polon. God buy you; fare you well\\n\\n   Reynol. Good my Lord\\n\\n   Polon. Obserue his inclination in your selfe\\n\\n   Reynol. I shall my Lord\\n\\n   Polon. And let him plye his Musicke'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Reynol. Well, my Lord.\\nEnter.\\n\\nEnter Ophelia.\\n\\n  Polon. Farewell:\\nHow now Ophelia, what's the matter?\\n  Ophe. Alas my Lord, I haue beene so affrighted\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Polon. With what, in the name of Heauen?\\n  Ophe. My Lord, as I was sowing in my Chamber,\\nLord Hamlet with his doublet all vnbrac'd,\\nNo hat vpon his head, his stockings foul'd,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Vngartred, and downe giued to his Anckle,\\nPale as his shirt, his knees knocking each other,\\nAnd with a looke so pitious in purport,\\nAs if he had been loosed out of hell,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='To speake of horrors: he comes before me'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Polon. Mad for thy Loue?\\n  Ophe. My Lord, I doe not know: but truly I do feare it'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Polon. What said he?\\n  Ophe. He tooke me by the wrist, and held me hard;\\nThen goes he to the length of all his arme;\\nAnd with his other hand thus o're his brow,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='He fals to such perusall of my face,\\nAs he would draw it. Long staid he so,\\nAt last, a little shaking of mine Arme:\\nAnd thrice his head thus wauing vp and downe;'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"He rais'd a sigh, so pittious and profound,\\nThat it did seeme to shatter all his bulke,\\nAnd end his being. That done, he lets me goe,\\nAnd with his head ouer his shoulders turn'd,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"He seem'd to finde his way without his eyes,\\nFor out adores he went without their helpe;\\nAnd to the last, bended their light on me\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Polon. Goe with me, I will goe seeke the King,\\nThis is the very extasie of Loue,\\nWhose violent property foredoes it selfe,\\nAnd leads the will to desperate Vndertakings,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='As oft as any passion vnder Heauen,\\nThat does afflict our Natures. I am sorrie,\\nWhat haue you giuen him any hard words of late?\\n  Ophe. No my good Lord: but as you did command,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"I did repell his Letters, and deny'de\\nHis accesse to me\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Pol. That hath made him mad.\\nI am sorrie that with better speed and iudgement\\nI had not quoted him. I feare he did but trifle,\\nAnd meant to wracke thee: but beshrew my iealousie:'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='It seemes it is as proper to our Age,\\nTo cast beyond our selues in our Opinions,\\nAs it is common for the yonger sort\\nTo lacke discretion. Come, go we to the King,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='This must be knowne, being kept close might moue\\nMore greefe to hide, then hate to vtter loue.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Exeunt.\\n\\n\\nScena Secunda.\\n\\nEnter King, Queene, Rosincrane, and Guildensterne Cum alijs.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='King. Welcome deere Rosincrance and Guildensterne.\\nMoreouer, that we much did long to see you,\\nThe neede we haue to vse you, did prouoke\\nOur hastie sending. Something haue you heard'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Of Hamlets transformation: so I call it,\\nSince not th' exterior, nor the inward man\\nResembles that it was. What it should bee\\nMore then his Fathers death, that thus hath put him\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"So much from th' vnderstanding of himselfe,\\nI cannot deeme of. I intreat you both,\\nThat being of so young dayes brought vp with him:\\nAnd since so Neighbour'd to his youth, and humour,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='That you vouchsafe your rest heere in our Court\\nSome little time: so by your Companies\\nTo draw him on to pleasures, and to gather\\nSo much as from Occasions you may gleane,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"That open'd lies within our remedie\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Qu. Good Gentlemen, he hath much talk'd of you,\\nAnd sure I am, two men there are not liuing,\\nTo whom he more adheres. If it will please you\\nTo shew vs so much Gentrie, and good will,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='As to expend your time with vs a-while,\\nFor the supply and profit of our Hope,\\nYour Visitation shall receiue such thankes\\nAs fits a Kings remembrance'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Rosin. Both your Maiesties\\nMight by the Soueraigne power you haue of vs,\\nPut your dread pleasures, more into Command\\nThen to Entreatie'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Guil. We both obey,\\nAnd here giue vp our selues, in the full bent,\\nTo lay our Seruices freely at your feete,\\nTo be commanded\\n\\n   King. Thankes Rosincrance, and gentle Guildensterne'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Qu. Thankes Guildensterne and gentle Rosincrance.\\nAnd I beseech you instantly to visit\\nMy too much changed Sonne.\\nGo some of ye,\\nAnd bring the Gentlemen where Hamlet is'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Guil. Heauens make our presence and our practises\\nPleasant and helpfull to him.\\nEnter.\\n\\n  Queene. Amen.\\nEnter Polonius.\\n\\n  Pol. Th' Ambassadors from Norwey, my good Lord,\\nAre ioyfully return'd\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='King. Thou still hast bin the father of good Newes'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Pol. Haue I, my Lord? Assure you, my good Liege,\\nI hold my dutie, as I hold my Soule,\\nBoth to my God, one to my gracious King:\\nAnd I do thinke, or else this braine of mine'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Hunts not the traile of Policie, so sure\\nAs I haue vs'd to do: that I haue found\\nThe very cause of Hamlets Lunacie\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"King. Oh speake of that, that I do long to heare\\n\\n   Pol. Giue first admittance to th' Ambassadors,\\nMy Newes shall be the Newes to that great Feast\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='King. Thy selfe do grace to them, and bring them in.\\nHe tels me my sweet Queene, that he hath found\\nThe head and sourse of all your Sonnes distemper'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Qu. I doubt it is no other, but the maine,\\nHis Fathers death, and our o're-hasty Marriage.\\nEnter Polonius, Voltumand, and Cornelius.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='King. Well, we shall sift him. Welcome good Frends:\\nSay Voltumand, what from our Brother Norwey?\\n  Volt. Most faire returne of Greetings, and Desires.\\nVpon our first, he sent out to suppresse'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"His Nephewes Leuies, which to him appear'd\\nTo be a preparation 'gainst the Poleak:\\nBut better look'd into, he truly found\\nIt was against your Highnesse, whereat greeued,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='That so his Sicknesse, Age, and Impotence\\nWas falsely borne in hand, sends out Arrests\\nOn Fortinbras, which he (in breefe) obeyes,\\nReceiues rebuke from Norwey: and in fine,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Makes Vow before his Vnkle, neuer more\\nTo giue th' assay of Armes against your Maiestie.\\nWhereon old Norwey, ouercome with ioy,\\nGiues him three thousand Crownes in Annuall Fee,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='And his Commission to imploy those Soldiers\\nSo leuied as before, against the Poleak:\\nWith an intreaty heerein further shewne,\\nThat it might please you to giue quiet passe'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Through your Dominions, for his Enterprize,\\nOn such regards of safety and allowance,\\nAs therein are set downe'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"King. It likes vs well:\\nAnd at our more consider'd time wee'l read,\\nAnswer, and thinke vpon this Businesse.\\nMeane time we thanke you, for your well-tooke Labour.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Go to your rest, at night wee'l Feast together.\\nMost welcome home.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Exit Ambass.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Pol. This businesse is very well ended.\\nMy Liege, and Madam, to expostulate\\nWhat Maiestie should be, what Dutie is,\\nWhy day is day; night, night; and time is time,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Were nothing but to waste Night, Day, and Time.\\nTherefore, since Breuitie is the Soule of Wit,\\nAnd tediousnesse, the limbes and outward flourishes,\\nI will be breefe. Your Noble Sonne is mad:'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Mad call I it; for to define true Madnesse,\\nWhat is't, but to be nothing else but mad.\\nBut let that go\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Qu. More matter, with lesse Art'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Pol. Madam, I sweare I vse no Art at all:\\nThat he is mad, 'tis true: 'Tis true 'tis pittie,\\nAnd pittie it is true: A foolish figure,\\nBut farewell it: for I will vse no Art.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Mad let vs grant him then: and now remaines\\nThat we finde out the cause of this effect,\\nOr rather say, the cause of this defect;\\nFor this effect defectiue, comes by cause,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Thus it remaines, and the remainder thus. Perpend,\\nI haue a daughter: haue, whil'st she is mine,\\nWho in her Dutie and Obedience, marke,\\nHath giuen me this: now gather, and surmise.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='The Letter.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"To the Celestiall, and my Soules Idoll, the most beautifed Ophelia.\\nThat's an ill Phrase, a vilde Phrase, beautified is a vilde\\nPhrase: but you shall heare these in her excellent white\\nbosome, these\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Qu. Came this from Hamlet to her'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Pol. Good Madam stay awhile, I will be faithfull.\\nDoubt thou, the Starres are fire,\\nDoubt, that the Sunne doth moue:\\nDoubt Truth to be a Lier,\\nBut neuer Doubt, I loue.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='O deere Ophelia, I am ill at these Numbers: I haue not Art to\\nreckon my grones; but that I loue thee best, oh most Best beleeue\\nit. Adieu.\\nThine euermore most deere Lady, whilst this'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Machine is to him, Hamlet.\\nThis in Obedience hath my daughter shew'd me:\\nAnd more aboue hath his soliciting,\\nAs they fell out by Time, by Meanes, and Place,\\nAll giuen to mine eare\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"King. But how hath she receiu'd his Loue?\\n  Pol. What do you thinke of me?\\n  King. As of a man, faithfull and Honourable\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Pol. I wold faine proue so. But what might you think?\\nWhen I had seene this hot loue on the wing,\\nAs I perceiued it, I must tell you that\\nBefore my Daughter told me what might you'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Or my deere Maiestie your Queene heere, think,\\nIf I had playd the Deske or Table-booke,\\nOr giuen my heart a winking, mute and dumbe,\\nOr look'd vpon this Loue, with idle sight,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='What might you thinke? No, I went round to worke,\\nAnd (my yong Mistris) thus I did bespeake\\nLord Hamlet is a Prince out of thy Starre,\\nThis must not be: and then, I Precepts gaue her,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='That she should locke her selfe from his Resort,\\nAdmit no Messengers, receiue no Tokens:\\nWhich done, she tooke the Fruites of my Aduice,\\nAnd he repulsed. A short Tale to make,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Fell into a Sadnesse, then into a Fast,\\nThence to a Watch, thence into a Weaknesse,\\nThence to a Lightnesse, and by this declension\\nInto the Madnesse whereon now he raues,\\nAnd all we waile for'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"King. Do you thinke 'tis this?\\n  Qu. It may be very likely\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Pol. Hath there bene such a time, I'de fain know that,\\nThat I haue possitiuely said, 'tis so,\\nWhen it prou'd otherwise?\\n  King. Not that I know\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Pol. Take this from this; if this be otherwise,\\nIf Circumstances leade me, I will finde\\nWhere truth is hid, though it were hid indeede\\nWithin the Center'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"King. How may we try it further?\\n  Pol. You know sometimes\\nHe walkes foure houres together, heere\\nIn the Lobby\\n\\n   Qu. So he ha's indeed\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Pol. At such a time Ile loose my Daughter to him,\\nBe you and I behinde an Arras then,\\nMarke the encounter: If he loue her not,\\nAnd be not from his reason falne thereon;'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Let me be no Assistant for a State,\\nAnd keepe a Farme and Carters'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='King. We will try it.\\nEnter Hamlet reading on a Booke.\\n\\n  Qu. But looke where sadly the poore wretch\\nComes reading\\n\\n   Pol. Away I do beseech you, both away,\\nIle boord him presently.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Exit King & Queen.\\n\\nOh giue me leaue. How does my good Lord Hamlet?\\n  Ham. Well, God-a-mercy\\n\\n   Pol. Do you know me, my Lord?\\n  Ham. Excellent, excellent well: y'are a Fishmonger\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Pol. Not I my Lord\\n\\n   Ham. Then I would you were so honest a man\\n\\n   Pol. Honest, my Lord?\\n  Ham. I sir, to be honest as this world goes, is to bee\\none man pick'd out of two thousand\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Pol. That's very true, my Lord\\n\\n   Ham. For if the Sun breed Magots in a dead dogge,\\nbeing a good kissing Carrion-\\nHaue you a daughter?\\n  Pol. I haue my Lord\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Let her not walke i'thSunne: Conception is a\\nblessing, but not as your daughter may conceiue. Friend\\nlooke too't\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Pol. How say you by that? Still harping on my daughter:\\nyet he knew me not at first; he said I was a Fishmonger:\\nhe is farre gone, farre gone: and truly in my youth,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='I suffred much extreamity for loue: very neere this. Ile\\nspeake to him againe. What do you read my Lord?\\n  Ham. Words, words, words'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Pol. What is the matter, my Lord?\\n  Ham. Betweene who?\\n  Pol. I meane the matter you meane, my Lord'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. Slanders Sir: for the Satyricall slaue saies here,\\nthat old men haue gray Beards; that their faces are wrinkled;\\ntheir eyes purging thicke Amber, or Plum-Tree'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Gumme: and that they haue a plentifull locke of Wit,\\ntogether with weake Hammes. All which Sir, though I\\nmost powerfully, and potently beleeue; yet I holde it'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='not Honestie to haue it thus set downe: For you your\\nselfe Sir, should be old as I am, if like a Crab you could\\ngo backward'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Pol. Though this be madnesse,\\nYet there is Method in't: will you walke\\nOut of the ayre my Lord?\\n  Ham. Into my Graue?\\n  Pol. Indeed that is out o'th' Ayre:\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"How pregnant (sometimes) his Replies are?\\nA happinesse,\\nThat often Madnesse hits on,\\nWhich Reason and Sanitie could not\\nSo prosperously be deliuer'd of.\\nI will leaue him,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='I will leaue him,\\nAnd sodainely contriue the meanes of meeting\\nBetweene him, and my daughter.\\nMy Honourable Lord, I will most humbly\\nTake my leaue of you'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. You cannot Sir take from me any thing, that I\\nwill more willingly part withall, except my life, my\\nlife\\n\\n   Polon. Fare you well my Lord\\n\\n   Ham. These tedious old fooles'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Polon. You goe to seeke my Lord Hamlet; there\\nhee is.\\nEnter Rosincran and Guildensterne.\\n\\n  Rosin. God saue you Sir'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Guild. Mine honour'd Lord?\\n  Rosin. My most deare Lord?\\n  Ham. My excellent good friends? How do'st thou\\nGuildensterne? Oh, Rosincrane; good Lads: How doe ye\\nboth?\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='both?\\n  Rosin. As the indifferent Children of the earth'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Guild. Happy, in that we are not ouer-happy: on Fortunes\\nCap, we are not the very Button\\n\\n   Ham. Nor the Soales of her Shoo?\\n  Rosin. Neither my Lord'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. Then you liue about her waste, or in the middle\\nof her fauour?\\n  Guil. Faith, her priuates, we'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. In the secret parts of Fortune? Oh, most true:\\nshe is a Strumpet. What's the newes?\\n  Rosin. None my Lord; but that the World's growne\\nhonest\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. Then is Doomesday neere: But your newes is\\nnot true. Let me question more in particular: what haue\\nyou my good friends, deserued at the hands of Fortune,\\nthat she sends you to Prison hither?'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Guil. Prison, my Lord?\\n  Ham. Denmark's a Prison\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Rosin. Then is the World one\\n\\n   Ham. A goodly one, in which there are many Confines,\\nWards, and Dungeons; Denmarke being one o'th'\\nworst\\n\\n   Rosin. We thinke not so my Lord\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Why then 'tis none to you; for there is nothing\\neither good or bad, but thinking makes it so: to me it is\\na prison\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Rosin. Why then your Ambition makes it one: 'tis\\ntoo narrow for your minde\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. O God, I could be bounded in a nutshell, and\\ncount my selfe a King of infinite space; were it not that\\nI haue bad dreames'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Guil. Which dreames indeed are Ambition: for the\\nvery substance of the Ambitious, is meerely the shadow\\nof a Dreame\\n\\n   Ham. A dreame it selfe is but a shadow'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Rosin. Truely, and I hold Ambition of so ayry and\\nlight a quality, that it is but a shadowes shadow'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Then are our Beggers bodies; and our Monarchs\\nand out-stretcht Heroes the Beggers Shadowes:\\nshall wee to th' Court: for, by my fey I cannot reason?\\n  Both. Wee'l wait vpon you\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. No such matter. I will not sort you with the\\nrest of my seruants: for to speake to you like an honest\\nman: I am most dreadfully attended; but in the beaten'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='way of friendship, What make you at Elsonower?\\n  Rosin. To visit you my Lord, no other occasion'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. Begger that I am, I am euen poore in thankes;\\nbut I thanke you: and sure deare friends my thanks\\nare too deare a halfepeny; were you not sent for? Is it'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='your owne inclining? Is it a free visitation? Come,\\ndeale iustly with me: come, come; nay speake'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Guil. What should we say my Lord?\\n  Ham. Why any thing. But to the purpose; you were\\nsent for; and there is a kinde confession in your lookes;\\nwhich your modesties haue not craft enough to color,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='I know the good King & Queene haue sent for you'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Rosin. To what end my Lord?\\n  Ham. That you must teach me: but let mee coniure\\nyou by the rights of our fellowship, by the consonancy of\\nour youth, by the Obligation of our euer-preserued loue,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='and by what more deare, a better proposer could charge\\nyou withall; be euen and direct with me, whether you\\nwere sent for or no'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Rosin. What say you?\\n  Ham. Nay then I haue an eye of you: if you loue me\\nhold not off\\n\\n   Guil. My Lord, we were sent for'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. I will tell you why; so shall my anticipation\\npreuent your discouery of your secricie to the King and\\nQueene: moult no feather, I haue of late, but wherefore'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='I know not, lost all my mirth, forgone all custome of exercise;\\nand indeed, it goes so heauenly with my disposition;\\nthat this goodly frame the Earth, seemes to me a sterrill'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Promontory; this most excellent Canopy the Ayre,\\nlook you, this braue ore-hanging, this Maiesticall Roofe,\\nfretted with golden fire: why, it appeares no other thing'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='to mee, then a foule and pestilent congregation of vapours.\\nWhat a piece of worke is a man! how Noble in\\nReason? how infinite in faculty? in forme and mouing'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='how expresse and admirable? in Action, how like an Angel?\\nin apprehension, how like a God? the beauty of the\\nworld, the Parragon of Animals; and yet to me, what is'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='this Quintessence of Dust? Man delights not me; no,\\nnor Woman neither; though by your smiling you seeme\\nto say so'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Rosin. My Lord, there was no such stuffe in my\\nthoughts'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. Why did you laugh, when I said, Man delights\\nnot me?\\n  Rosin. To thinke, my Lord, if you delight not in Man,\\nwhat Lenton entertainment the Players shall receiue'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='from you: wee coated them on the way, and hither are\\nthey comming to offer you Seruice'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. He that playes the King shall be welcome; his\\nMaiesty shall haue Tribute of mee: the aduenturous\\nKnight shal vse his Foyle and Target: the Louer shall'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"not sigh gratis, the humorous man shall end his part in\\npeace: the Clowne shall make those laugh whose lungs\\nare tickled a'th' sere: and the Lady shall say her minde\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"freely; or the blanke Verse shall halt for't: what Players\\nare they?\\n  Rosin. Euen those you were wont to take delight in\\nthe Tragedians of the City\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. How chances it they trauaile? their residence\\nboth in reputation and profit was better both\\nwayes'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Rosin. I thinke their Inhibition comes by the meanes\\nof the late Innouation?\\n  Ham. Doe they hold the same estimation they did\\nwhen I was in the City? Are they so follow'd?\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Rosin. No indeed, they are not'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. How comes it? doe they grow rusty?\\n  Rosin. Nay, their indeauour keepes in the wonted\\npace; But there is Sir an ayrie of Children, little\\nYases, that crye out on the top of question; and'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"are most tyrannically clap't for't: these are now the\\nfashion, and so be-ratled the common Stages (so they\\ncall them) that many wearing Rapiers, are affraide of\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Goose-quils, and dare scarse come thither'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. What are they Children? Who maintains 'em?\\nHow are they escorted? Will they pursue the Quality no\\nlonger then they can sing? Will they not say afterwards\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='if they should grow themselues to common Players (as\\nit is most like if their meanes are not better) their Writers\\ndo them wrong, to make them exclaim against their\\nowne Succession'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Rosin. Faith there ha's bene much to do on both sides:\\nand the Nation holds it no sinne, to tarre them to Controuersie.\\nThere was for a while, no mony bid for argument,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='vnlesse the Poet and the Player went to Cuffes in\\nthe Question'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Is't possible?\\n  Guild. Oh there ha's beene much throwing about of\\nBraines\\n\\n   Ham. Do the Boyes carry it away?\\n  Rosin. I that they do my Lord. Hercules & his load too\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. It is not strange: for mine Vnckle is King of\\nDenmarke, and those that would make mowes at him\\nwhile my Father liued; giue twenty, forty, an hundred'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ducates a peece, for his picture in Little. There is something\\nin this more then Naturall, if Philosophie could\\nfinde it out.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Flourish for the Players.\\n\\n  Guil. There are the Players'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. Gentlemen, you are welcom to Elsonower: your\\nhands, come: The appurtenance of Welcome, is Fashion\\nand Ceremony. Let me comply with you in the Garbe,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"lest my extent to the Players (which I tell you must shew\\nfairely outward) should more appeare like entertainment\\nthen yours. You are welcome: but my Vnckle Father,\\nand Aunt Mother are deceiu'd\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Guil. In what my deere Lord?\\n  Ham. I am but mad North, North-West: when the\\nWinde is Southerly, I know a Hawke from a Handsaw.\\nEnter Polonius.\\n\\n  Pol. Well be with you Gentlemen'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. Hearke you Guildensterne, and you too: at each\\neare a hearer: that great Baby you see there, is not yet\\nout of his swathing clouts'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Rosin. Happily he's the second time come to them: for\\nthey say, an old man is twice a childe\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. I will Prophesie. Hee comes to tell me of the\\nPlayers. Mark it, you say right Sir: for a Monday morning\\n'twas so indeed\\n\\n   Pol. My Lord, I haue Newes to tell you\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. My Lord, I haue Newes to tell you.\\nWhen Rossius an Actor in Rome-\\n  Pol. The Actors are come hither my Lord\\n\\n   Ham. Buzze, buzze\\n\\n   Pol. Vpon mine Honor'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. Then can each Actor on his Asse-\\n  Polon. The best Actors in the world, either for Tragedie,\\nComedie, Historie, Pastorall:\\nPastoricall-Comicall-Historicall-Pastorall:'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Tragicall-Historicall: Tragicall-Comicall-Historicall-Pastorall:\\nScene indiuidible: or Poem\\nvnlimited. Seneca cannot be too heauy, nor Plautus'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='too light, for the law of Writ, and the Liberty. These are\\nthe onely men'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. O Iephta Iudge of Israel, what a Treasure had'st\\nthou?\\n  Pol. What a Treasure had he, my Lord?\\n  Ham. Why one faire Daughter, and no more,\\nThe which he loued passing well\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Pol. Still on my Daughter\\n\\n   Ham. Am I not i'th' right old Iephta?\\n  Polon. If you call me Iephta my Lord, I haue a daughter\\nthat I loue passing well\\n\\n   Ham. Nay that followes not\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Polon. What followes then, my Lord?\\n  Ha. Why, As by lot, God wot: and then you know, It\\ncame to passe, as most like it was: The first rowe of the'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Pons Chanson will shew you more. For looke where my\\nAbridgements come.\\nEnter foure or fiue Players.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Y'are welcome Masters, welcome all. I am glad to see\\nthee well: Welcome good Friends. Oh my olde Friend?\\nThy face is valiant since I saw thee last: Com'st thou to\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='beard me in Denmarke? What, my yong Lady and Mistris?\\nByrlady your Ladiship is neerer Heauen then when\\nI saw you last, by the altitude of a Choppine. Pray God'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"your voice like a peece of vncurrant Gold be not crack'd\\nwithin the ring. Masters, you are all welcome: wee'l e'ne\\nto't like French Faulconers, flie at any thing we see: wee'l\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='haue a Speech straight. Come giue vs a tast of your quality:\\ncome, a passionate speech'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='1.Play. What speech, my Lord?\\n  Ham. I heard thee speak me a speech once, but it was\\nneuer Acted: or if it was, not aboue once, for the Play I'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"remember pleas'd not the Million, 'twas Cauiarie to the\\nGenerall: but it was (as I receiu'd it, and others, whose\\niudgement in such matters, cried in the top of mine) an\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='excellent Play; well digested in the Scoenes, set downe\\nwith as much modestie, as cunning. I remember one said,\\nthere was no Sallets in the lines, to make the matter sauory;'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"nor no matter in the phrase, that might indite the\\nAuthor of affectation, but cal'd it an honest method. One\\ncheefe Speech in it, I cheefely lou'd, 'twas Aeneas Tale\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='to Dido, and thereabout of it especially, where he speaks\\nof Priams slaughter. If it liue in your memory, begin at\\nthis Line, let me see, let me see: The rugged Pyrrhus like'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"th'Hyrcanian Beast. It is not so: it begins with Pyrrhus\\nThe rugged Pyrrhus, he whose Sable Armes\\nBlacke as his purpose, did the night resemble\\nWhen he lay couched in the Ominous Horse,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Hath now this dread and blacke Complexion smear'd\\nWith Heraldry more dismall: Head to foote\\nNow is he to take Geulles, horridly Trick'd\\nWith blood of Fathers, Mothers, Daughters, Sonnes,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Bak'd and impasted with the parching streets,\\nThat lend a tyrannous, and damned light\\nTo their vilde Murthers, roasted in wrath and fire,\\nAnd thus o're-sized with coagulate gore,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='With eyes like Carbuncles, the hellish Pyrrhus\\nOlde Grandsire Priam seekes'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Pol. Fore God, my Lord, well spoken, with good accent,\\nand good discretion'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='1.Player. Anon he findes him,\\nStriking too short at Greekes. His anticke Sword,\\nRebellious to his Arme, lyes where it falles\\nRepugnant to command: vnequall match,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Pyrrhus at Priam driues, in Rage strikes wide:\\nBut with the whiffe and winde of his fell Sword,\\nTh' vnnerued Father fals. Then senselesse Illium,\\nSeeming to feele his blow, with flaming top\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Stoopes to his Bace, and with a hideous crash\\nTakes Prisoner Pyrrhus eare. For loe, his Sword\\nWhich was declining on the Milkie head\\nOf Reuerend Priam, seem'd i'th' Ayre to sticke:\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='So as a painted Tyrant Pyrrhus stood,\\nAnd like a Newtrall to his will and matter, did nothing.\\nBut as we often see against some storme,\\nA silence in the Heauens, the Racke stand still,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='The bold windes speechlesse, and the Orbe below\\nAs hush as death: Anon the dreadfull Thunder\\nDoth rend the Region. So after Pyrrhus pause,\\nA rowsed Vengeance sets him new a-worke,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"And neuer did the Cyclops hammers fall\\nOn Mars his Armours, forg'd for proofe Eterne,\\nWith lesse remorse then Pyrrhus bleeding sword\\nNow falles on Priam.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Out, out, thou Strumpet-Fortune, all you Gods,\\nIn generall Synod take away her power:\\nBreake all the Spokes and Fallies from her wheele,\\nAnd boule the round Naue downe the hill of Heauen,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='As low as to the Fiends'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Pol. This is too long\\n\\n   Ham. It shall to'th Barbars, with your beard. Prythee\\nsay on: He's for a Iigge, or a tale of Baudry, or hee\\nsleepes. Say on; come to Hecuba\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"1.Play. But who, O who, had seen the inobled Queen\\n\\n   Ham. The inobled Queene?\\n  Pol. That's good: Inobled Queene is good\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='1.Play. Run bare-foot vp and downe,\\nThreatning the flame\\nWith Bisson Rheume: A clout about that head,\\nWhere late the Diadem stood, and for a Robe\\nAbout her lanke and all ore-teamed Loines,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"A blanket in th' Alarum of feare caught vp.\\nWho this had seene, with tongue in Venome steep'd,\\n'Gainst Fortunes State, would Treason haue pronounc'd?\\nBut if the Gods themselues did see her then,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='When she saw Pyrrhus make malicious sport\\nIn mincing with his Sword her Husbands limbes,\\nThe instant Burst of Clamour that she made\\n(Vnlesse things mortall moue them not at all)'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Would haue made milche the Burning eyes of Heauen,\\nAnd passion in the Gods'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Pol. Looke where he ha's not turn'd his colour, and\\nha's teares in's eyes. Pray you no more\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. 'Tis well, Ile haue thee speake out the rest,\\nsoone. Good my Lord, will you see the Players wel bestow'd.\\nDo ye heare, let them be well vs'd: for they are\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='the Abstracts and breefe Chronicles of the time. After\\nyour death, you were better haue a bad Epitaph, then\\ntheir ill report while you liued'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Pol. My Lord, I will vse them according to their desart'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. Gods bodykins man, better. Vse euerie man\\nafter his desart, and who should scape whipping: vse\\nthem after your own Honor and Dignity. The lesse they'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='deserue, the more merit is in your bountie. Take them\\nin'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Pol. Come sirs.\\n\\nExit Polon.\\n\\n  Ham. Follow him Friends: wee'l heare a play to morrow.\\nDost thou heare me old Friend, can you play the\\nmurther of Gonzago?\\n  Play. I my Lord\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Wee'l ha't to morrow night. You could for a\\nneed study a speech of some dosen or sixteene lines, which\\nI would set downe, and insert in't? Could ye not?\\n  Play. I my Lord\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. Very well. Follow that Lord, and looke you\\nmock him not. My good Friends, Ile leaue you til night\\nyou are welcome to Elsonower?\\n  Rosin. Good my Lord.\\n\\nExeunt.\\n\\nManet Hamlet.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. I so, God buy'ye: Now I am alone.\\nOh what a Rogue and Pesant slaue am I?\\nIs it not monstrous that this Player heere,\\nBut in a Fixion, in a dreame of Passion,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Could force his soule so to his whole conceit,\\nThat from her working, all his visage warm'd;\\nTeares in his eyes, distraction in's Aspect,\\nA broken voyce, and his whole Function suiting\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"With Formes, to his Conceit? And all for nothing?\\nFor Hecuba?\\nWhat's Hecuba to him, or he to Hecuba,\\nThat he should weepe for her? What would he doe,\\nHad he the Motiue and the Cue for passion\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='That I haue? He would drowne the Stage with teares,\\nAnd cleaue the generall eare with horrid speech:\\nMake mad the guilty, and apale the free,\\nConfound the ignorant, and amaze indeed,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='The very faculty of Eyes and Eares. Yet I,\\nA dull and muddy-metled Rascall, peake\\nLike Iohn a-dreames, vnpregnant of my cause,\\nAnd can say nothing: No, not for a King,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Vpon whose property, and most deere life,\\nA damn'd defeate was made. Am I a Coward?\\nWho calles me Villaine? breakes my pate a-crosse?\\nPluckes off my Beard, and blowes it in my face?\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Tweakes me by'th' Nose? giues me the Lye i'th' Throate,\\nAs deepe as to the Lungs? Who does me this?\\nHa? Why I should take it: for it cannot be,\\nBut I am Pigeon-Liuer'd, and lacke Gall\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='To make Oppression bitter, or ere this,\\nI should haue fatted all the Region Kites\\nWith this Slaues Offall, bloudy: a Bawdy villaine,\\nRemorselesse, Treacherous, Letcherous, kindles villaine!'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Oh Vengeance!\\nWho? What an Asse am I? I sure, this is most braue,\\nThat I, the Sonne of the Deere murthered,\\nPrompted to my Reuenge by Heauen, and Hell,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Must (like a Whore) vnpacke my heart with words,\\nAnd fall a Cursing like a very Drab.\\nA Scullion? Fye vpon't: Foh. About my Braine.\\nI haue heard, that guilty Creatures sitting at a Play,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Haue by the very cunning of the Scoene,\\nBene strooke so to the soule, that presently\\nThey haue proclaim'd their Malefactions.\\nFor Murther, though it haue no tongue, will speake\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='With most myraculous Organ. Ile haue these Players,\\nPlay something like the murder of my Father,\\nBefore mine Vnkle. Ile obserue his lookes,\\nIle rent him to the quicke: If he but blench'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"I know my course. The Spirit that I haue seene\\nMay be the Diuell, and the Diuel hath power\\nT' assume a pleasing shape, yea and perhaps\\nOut of my Weaknesse, and my Melancholly,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"As he is very potent with such Spirits,\\nAbuses me to damne me. Ile haue grounds\\nMore Relatiue then this: The Play's the thing,\\nWherein Ile catch the Conscience of the King.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Exit\\n\\nEnter King, Queene, Polonius, Ophelia, Rosincrance,\\nGuildenstern, and\\nLords.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='King. And can you by no drift of circumstance\\nGet from him why he puts on this Confusion:\\nGrating so harshly all his dayes of quiet\\nWith turbulent and dangerous Lunacy'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Rosin. He does confesse he feeles himselfe distracted,\\nBut from what cause he will by no meanes speake'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Guil. Nor do we finde him forward to be sounded,\\nBut with a crafty Madnesse keepes aloofe:\\nWhen we would bring him on to some Confession\\nOf his true state'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Qu. Did he receiue you well?\\n  Rosin. Most like a Gentleman\\n\\n   Guild. But with much forcing of his disposition\\n\\n   Rosin. Niggard of question, but of our demands\\nMost free in his reply'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Qu. Did you assay him to any pastime?\\n  Rosin. Madam, it so fell out, that certaine Players\\nWe ore-wrought on the way: of these we told him,\\nAnd there did seeme in him a kinde of ioy'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='To heare of it: They are about the Court,\\nAnd (as I thinke) they haue already order\\nThis night to play before him'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Pol. 'Tis most true:\\nAnd he beseech'd me to intreate your Maiesties\\nTo heare, and see the matter\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"King. With all my heart, and it doth much content me\\nTo heare him so inclin'd. Good Gentlemen,\\nGiue him a further edge, and driue his purpose on\\nTo these delights\\n\\n   Rosin. We shall my Lord.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Exeunt.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"King. Sweet Gertrude leaue vs too,\\nFor we haue closely sent for Hamlet hither,\\nThat he, as 'twere by accident, may there\\nAffront Ophelia. Her Father, and my selfe (lawful espials)\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Will so bestow our selues, that seeing vnseene\\nWe may of their encounter frankely iudge,\\nAnd gather by him, as he is behaued,\\nIf't be th' affliction of his loue, or no.\\nThat thus he suffers for\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Qu. I shall obey you,\\nAnd for your part Ophelia, I do wish\\nThat your good Beauties be the happy cause\\nOf Hamlets wildenesse: so shall I hope your Vertues\\nWill bring him to his wonted way againe,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='To both your Honors'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ophe. Madam, I wish it may'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Pol. Ophelia, walke you heere. Gracious so please ye\\nWe will bestow our selues: Reade on this booke,\\nThat shew of such an exercise may colour\\nYour lonelinesse. We are oft too blame in this,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"'Tis too much prou'd, that with Deuotions visage,\\nAnd pious Action, we do surge o're\\nThe diuell himselfe\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"King. Oh 'tis true:\\nHow smart a lash that speech doth giue my Conscience?\\nThe Harlots Cheeke beautied with plaist'ring Art\\nIs not more vgly to the thing that helpes it,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Then is my deede, to my most painted word.\\nOh heauie burthen!\\n  Pol. I heare him comming, let's withdraw my Lord.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Exeunt.\\n\\nEnter Hamlet.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. To be, or not to be, that is the Question:\\nWhether 'tis Nobler in the minde to suffer\\nThe Slings and Arrowes of outragious Fortune,\\nOr to take Armes against a Sea of troubles,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"And by opposing end them: to dye, to sleepe\\nNo more; and by a sleepe, to say we end\\nThe Heart-ake, and the thousand Naturall shockes\\nThat Flesh is heyre too? 'Tis a consummation\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Deuoutly to be wish'd. To dye to sleepe,\\nTo sleepe, perchance to Dreame; I, there's the rub,\\nFor in that sleepe of death, what dreames may come,\\nWhen we haue shuffel'd off this mortall coile,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Must giue vs pawse. There's the respect\\nThat makes Calamity of so long life:\\nFor who would beare the Whips and Scornes of time,\\nThe Oppressors wrong, the poore mans Contumely,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"The pangs of dispriz'd Loue, the Lawes delay,\\nThe insolence of Office, and the Spurnes\\nThat patient merit of the vnworthy takes,\\nWhen he himselfe might his Quietus make\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='With a bare Bodkin? Who would these Fardles beare\\nTo grunt and sweat vnder a weary life,\\nBut that the dread of something after death,\\nThe vndiscouered Countrey, from whose Borne'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='No Traueller returnes, Puzels the will,\\nAnd makes vs rather beare those illes we haue,\\nThen flye to others that we know not of.\\nThus Conscience does make Cowards of vs all,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"And thus the Natiue hew of Resolution\\nIs sicklied o're, with the pale cast of Thought,\\nAnd enterprizes of great pith and moment,\\nWith this regard their Currants turne away,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='And loose the name of Action. Soft you now,\\nThe faire Ophelia? Nimph, in thy Orizons\\nBe all my sinnes remembred'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ophe. Good my Lord,\\nHow does your Honor for this many a day?\\n  Ham. I humbly thanke you: well, well, well'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ophe. My Lord, I haue Remembrances of yours,\\nThat I haue longed long to re-deliuer.\\nI pray you now, receiue them\\n\\n   Ham. No, no, I neuer gaue you ought'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ophe. My honor'd Lord, I know right well you did,\\nAnd with them words of so sweet breath compos'd,\\nAs made the things more rich, then perfume left:\\nTake these againe, for to the Noble minde\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Rich gifts wax poore, when giuers proue vnkinde.\\nThere my Lord'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. Ha, ha: Are you honest?\\n  Ophe. My Lord'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. Are you faire?\\n  Ophe. What meanes your Lordship?\\n  Ham. That if you be honest and faire, your Honesty\\nshould admit no discourse to your Beautie'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ophe. Could Beautie my Lord, haue better Comerce\\nthen your Honestie?\\n  Ham. I trulie: for the power of Beautie, will sooner\\ntransforme Honestie from what is, to a Bawd, then the'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='force of Honestie can translate Beautie into his likenesse.\\nThis was sometime a Paradox, but now the time giues it\\nproofe. I did loue you once'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ophe. Indeed my Lord, you made me beleeue so\\n\\n   Ham. You should not haue beleeued me. For vertue\\ncannot so innocculate our old stocke, but we shall rellish\\nof it. I loued you not'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ophe. I was the more deceiued'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Get thee to a Nunnerie. Why would'st thou\\nbe a breeder of Sinners? I am my selfe indifferent honest,\\nbut yet I could accuse me of such things, that it were better\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='my Mother had not borne me. I am very prowd, reuengefull,\\nAmbitious, with more offences at my becke,\\nthen I haue thoughts to put them in imagination, to giue'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='them shape, or time to acte them in. What should such\\nFellowes as I do, crawling betweene Heauen and Earth.\\nWe are arrant Knaues all, beleeue none of vs. Goe thy'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"wayes to a Nunnery. Where's your Father?\\n  Ophe. At home, my Lord\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Let the doores be shut vpon him, that he may\\nplay the Foole no way, but in's owne house. Farewell\\n\\n   Ophe. O helpe him, you sweet Heauens\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. If thou doest Marry, Ile giue thee this Plague\\nfor thy Dowrie. Be thou as chast as Ice, as pure as Snow,\\nthou shalt not escape Calumny. Get thee to a Nunnery.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Go, Farewell. Or if thou wilt needs Marry, marry a fool:\\nfor Wise men know well enough, what monsters you\\nmake of them. To a Nunnery go, and quickly too. Farwell'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ophe. O heauenly Powers, restore him'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. I haue heard of your pratlings too wel enough.\\nGod has giuen you one pace, and you make your selfe another:\\nyou gidge, you amble, and you lispe, and nickname'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Gods creatures, and make your Wantonnesse, your Ignorance.\\nGo too, Ile no more on't, it hath made me mad.\\nI say, we will haue no more Marriages. Those that are\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='married already, all but one shall liue, the rest shall keep\\nas they are. To a Nunnery, go.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Exit Hamlet.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ophe. O what a Noble minde is heere o're-throwne?\\nThe Courtiers, Soldiers, Schollers: Eye, tongue, sword,\\nTh' expectansie and Rose of the faire State,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"The glasse of Fashion, and the mould of Forme,\\nTh' obseru'd of all Obseruers, quite, quite downe.\\nHaue I of Ladies most deiect and wretched,\\nThat suck'd the Honie of his Musicke Vowes:\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Now see that Noble, and most Soueraigne Reason,\\nLike sweet Bels iangled out of tune, and harsh,\\nThat vnmatch'd Forme and Feature of blowne youth,\\nBlasted with extasie. Oh woe is me,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"T'haue seene what I haue seene: see what I see.\\nEnter King, and Polonius.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"King. Loue? His affections do not that way tend,\\nNor what he spake, though it lack'd Forme a little,\\nWas not like Madnesse. There's something in his soule?\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"O're which his Melancholly sits on brood,\\nAnd I do doubt the hatch, and the disclose\\nWill be some danger, which to preuent\\nI haue in quicke determination\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Thus set it downe. He shall with speed to England\\nFor the demand of our neglected Tribute:\\nHaply the Seas and Countries different\\nWith variable Obiects, shall expell'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"This something setled matter in his heart:\\nWhereon his Braines still beating, puts him thus\\nFrom fashion of himselfe. What thinke you on't?\\n  Pol. It shall do well. But yet do I beleeue\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='The Origin and Commencement of this greefe\\nSprung from neglected loue. How now Ophelia?\\nYou neede not tell vs, what Lord Hamlet saide,\\nWe heard it all. My Lord, do as you please,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"But if you hold it fit after the Play,\\nLet his Queene Mother all alone intreat him\\nTo shew his Greefes: let her be round with him,\\nAnd Ile be plac'd so, please you in the eare\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Of all their Conference. If she finde him not,\\nTo England send him: Or confine him where\\nYour wisedome best shall thinke'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"King. It shall be so:\\nMadnesse in great Ones, must not vnwatch'd go.\\n\\nExeunt.\\n\\nEnter Hamlet, and two or three of the Players.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Speake the Speech I pray you, as I pronounc'd\\nit to you trippingly on the Tongue: But if you mouth it,\\nas many of your Players do, I had as liue the Town-Cryer\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='had spoke my Lines: Nor do not saw the Ayre too much\\nyour hand thus, but vse all gently; for in the verie Torrent,\\nTempest, and (as I say) the Whirle-winde of'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Passion, you must acquire and beget a Temperance that\\nmay giue it Smoothnesse. O it offends mee to the Soule,\\nto see a robustious Pery-wig-pated Fellow, teare a Passion'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='to tatters, to verie ragges, to split the eares of the\\nGroundlings: who (for the most part) are capeable of\\nnothing, but inexplicable dumbe shewes, & noise: I could'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"haue such a Fellow whipt for o're-doing Termagant: it\\noutHerod's Herod. Pray you auoid it\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Player. I warrant your Honor'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. Be not too tame neyther: but let your owne\\nDiscretion be your Tutor. Sute the Action to the Word,\\nthe Word to the Action, with this speciall obseruance:'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"That you ore-stop not the modestie of Nature; for any\\nthing so ouer-done, is fro[m] the purpose of Playing, whose\\nend both at the first and now, was and is, to hold as 'twer\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='the Mirrour vp to Nature; to shew Vertue her owne\\nFeature, Scorne her owne Image, and the verie Age and\\nBodie of the Time, his forme and pressure. Now, this'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"ouer-done, or come tardie off, though it make the vnskilfull\\nlaugh, cannot but make the Iudicious greeue; The\\ncensure of the which One, must in your allowance o'reway\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='a whole Theater of Others. Oh, there bee Players\\nthat I haue seene Play, and heard others praise, and that\\nhighly (not to speake it prophanely) that neyther hauing'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='the accent of Christians, nor the gate of Christian, Pagan,\\nor Norman, haue so strutted and bellowed, that I haue\\nthought some of Natures Iouerney-men had made men,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='and not made them well, they imitated Humanity so abhominably'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Play. I hope we haue reform'd that indifferently with\\nvs, Sir\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. O reforme it altogether. And let those that\\nplay your Clownes, speake no more then is set downe for\\nthem. For there be of them, that will themselues laugh,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"to set on some quantitie of barren Spectators to laugh\\ntoo, though in the meane time, some necessary Question\\nof the Play be then to be considered: that's Villanous, &\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='shewes a most pittifull Ambition in the Foole that vses\\nit. Go make you readie.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Exit Players.\\n\\nEnter Polonius, Rosincrance, and Guildensterne.\\n\\nHow now my Lord,\\nWill the King heare this peece of Worke?\\n  Pol. And the Queene too, and that presently'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. Bid the Players make hast.\\n\\nExit Polonius.\\n\\nWill you two helpe to hasten them?\\n  Both. We will my Lord.\\n\\nExeunt.\\n\\nEnter Horatio.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Enter Horatio.\\n\\n  Ham. What hoa, Horatio?\\n  Hora. Heere sweet Lord, at your Seruice\\n\\n   Ham. Horatio, thou art eene as iust a man\\nAs ere my Conuersation coap'd withall\\n\\n   Hora. O my deere Lord\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Nay, do not thinke I flatter:\\nFor what aduancement may I hope from thee,\\nThat no Reuennew hast, but thy good spirits\\nTo feed & cloath thee. Why shold the poor be flatter'd?\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='No, let the Candied tongue, like absurd pompe,\\nAnd crooke the pregnant Hindges of the knee,\\nWhere thrift may follow faining? Dost thou heare,\\nSince my deere Soule was Mistris of my choyse,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"And could of men distinguish, her election\\nHath seal'd thee for her selfe. For thou hast bene\\nAs one in suffering all, that suffers nothing.\\nA man that Fortunes buffets, and Rewards\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Hath 'tane with equall Thankes. And blest are those,\\nWhose Blood and Iudgement are so well co-mingled,\\nThat they are not a Pipe for Fortunes finger.\\nTo sound what stop she please. Giue me that man,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='That is not Passions Slaue, and I will weare him\\nIn my hearts Core. I, in my Heart of heart,\\nAs I do thee. Something too much of this.\\nThere is a Play to night to before the King.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"One Scoene of it comes neere the Circumstance\\nWhich I haue told thee, of my Fathers death.\\nI prythee, when thou see'st that Acte a-foot,\\nEuen with the verie Comment of my Soule\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Obserue mine Vnkle: If his occulted guilt,\\nDo not it selfe vnkennell in one speech,\\nIt is a damned Ghost that we haue seene:\\nAnd my Imaginations are as foule'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='As Vulcans Stythe. Giue him needfull note,\\nFor I mine eyes will riuet to his Face:\\nAnd after we will both our iudgements ioyne,\\nTo censure of his seeming'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Hora. Well my Lord.\\nIf he steale ought the whil'st this Play is Playing,\\nAnd scape detecting, I will pay the Theft.\\nEnter King, Queene, Polonius, Ophelia, Rosincrance,\\nGuildensterne, and\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Guildensterne, and\\nother Lords attendant with his Guard carrying Torches. Danish\\nMarch. Sound\\na Flourish.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. They are comming to the Play: I must be idle.\\nGet you a place'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"King. How fares our Cosin Hamlet?\\n  Ham. Excellent Ifaith, of the Camelions dish: I eate\\nthe Ayre promise-cramm'd, you cannot feed Capons so\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='King. I haue nothing with this answer Hamlet, these\\nwords are not mine'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. No, nor mine. Now my Lord, you plaid once\\ni'th' Vniuersity, you say?\\n  Polon. That I did my Lord, and was accounted a good\\nActor\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. And what did you enact?\\n  Pol. I did enact Iulius Caesar, I was kill'd i'th' Capitol:\\nBrutus kill'd me\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. It was a bruite part of him, to kill so Capitall a\\nCalfe there. Be the Players ready?\\n  Rosin. I my Lord, they stay vpon your patience\\n\\n   Qu. Come hither my good Hamlet, sit by me'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ha. No good Mother, here's Mettle more attractiue\\n\\n   Pol. Oh ho, do you marke that?\\n  Ham. Ladie, shall I lye in your Lap?\\n  Ophe. No my Lord\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. I meane, my Head vpon your Lap?\\n  Ophe. I my Lord\\n\\n   Ham. Do you thinke I meant Country matters?\\n  Ophe. I thinke nothing, my Lord'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. That's a faire thought to ly betweene Maids legs\\n  Ophe. What is my Lord?\\n  Ham. Nothing\\n\\n   Ophe. You are merrie, my Lord?\\n  Ham. Who I?\\n  Ophe. I my Lord\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Oh God, your onely Iigge-maker: what should\\na man do, but be merrie. For looke you how cheerefully\\nmy Mother lookes, and my Father dyed within's two\\nHoures\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ophe. Nay, 'tis twice two moneths, my Lord\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. So long? Nay then let the Diuel weare blacke,\\nfor Ile haue a suite of Sables. Oh Heauens! dye two moneths\\nago, and not forgotten yet? Then there's hope, a\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='great mans Memorie, may out-liue his life halfe a yeare:\\nBut byrlady he must builde Churches then: or else shall\\nhe suffer not thinking on, with the Hoby-horsse, whose'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Epitaph is, For o, For o, the Hoby-horse is forgot.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Hoboyes play. The dumbe shew enters.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Enter a King and Queene, very louingly; the Queene embracing\\nhim. She\\nkneeles, and makes shew of Protestation vnto him. He takes her\\nvp, and'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='vp, and\\ndeclines his head vpon her neck. Layes him downe vpon a Banke\\nof Flowers.\\nShe seeing him a-sleepe, leaues him. Anon comes in a Fellow,\\ntakes off his'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='takes off his\\nCrowne, kisses it, and powres poyson in the Kings eares, and\\nExits. The\\nQueene returnes, findes the King dead, and makes passionate\\nAction. The'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Action. The\\nPoysoner, with some two or three Mutes comes in againe, seeming\\nto lament\\nwith her. The dead body is carried away: The Poysoner Wooes the\\nQueene with'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Queene with\\nGifts, she seemes loath and vnwilling awhile, but in the end,\\naccepts his\\nloue.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Exeunt.\\n\\n  Ophe. What meanes this, my Lord?\\n  Ham. Marry this is Miching Malicho, that meanes\\nMischeefe'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ophe. Belike this shew imports the Argument of the\\nPlay?\\n  Ham. We shall know by these Fellowes: the Players\\ncannot keepe counsell, they'l tell all\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ophe. Will they tell vs what this shew meant?\\n  Ham. I, or any shew that you'l shew him. Bee not\\nyou asham'd to shew, hee'l not shame to tell you what it\\nmeanes\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ophe. You are naught, you are naught, Ile marke the\\nPlay.\\nEnter Prologue.\\n\\nFor vs, and for our Tragedie,\\nHeere stooping to your Clemencie:\\nWe begge your hearing Patientlie'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Is this a Prologue, or the Poesie of a Ring?\\n  Ophe. 'Tis briefe my Lord\\n\\n   Ham. As Womans loue.\\nEnter King and his Queene.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='King. Full thirtie times hath Phoebus Cart gon round,\\nNeptunes salt Wash, and Tellus Orbed ground:\\nAnd thirtie dozen Moones with borrowed sheene,\\nAbout the World haue times twelue thirties beene,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Since loue our hearts, and Hymen did our hands\\nVnite comutuall, in most sacred Bands'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Bap. So many iournies may the Sunne and Moone\\nMake vs againe count o're, ere loue be done.\\nBut woe is me, you are so sicke of late,\\nSo farre from cheere, and from your former state,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='That I distrust you: yet though I distrust,\\nDiscomfort you (my Lord) it nothing must:\\nFor womens Feare and Loue, holds quantitie,\\nIn neither ought, or in extremity:'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Now what my loue is, proofe hath made you know,\\nAnd as my Loue is siz'd, my Feare is so\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"King. Faith I must leaue thee Loue, and shortly too:\\nMy operant Powers my Functions leaue to do:\\nAnd thou shalt liue in this faire world behinde,\\nHonour'd, belou'd, and haply, one as kinde.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"For Husband shalt thou-\\n  Bap. Oh confound the rest:\\nSuch Loue, must needs be Treason in my brest:\\nIn second Husband, let me be accurst,\\nNone wed the second, but who kill'd the first\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. Wormwood, Wormwood'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Bapt. The instances that second Marriage moue,\\nAre base respects of Thrift, but none of Loue.\\nA second time, I kill my Husband dead,\\nWhen second Husband kisses me in Bed'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='King. I do beleeue you. Think what now you speak:\\nBut what we do determine, oft we breake:\\nPurpose is but the slaue to Memorie,\\nOf violent Birth, but poore validitie:'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Which now like Fruite vnripe stickes on the Tree,\\nBut fall vnshaken, when they mellow bee.\\nMost necessary 'tis, that we forget\\nTo pay our selues, what to our selues is debt:\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='What to our selues in passion we propose,\\nThe passion ending, doth the purpose lose.\\nThe violence of other Greefe or Ioy,\\nTheir owne ennactors with themselues destroy:'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Where Ioy most Reuels, Greefe doth most lament;\\nGreefe ioyes, Ioy greeues on slender accident.\\nThis world is not for aye, nor 'tis not strange\\nThat euen our Loues should with our Fortunes change.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"For 'tis a question left vs yet to proue,\\nWhether Loue lead Fortune, or else Fortune Loue.\\nThe great man downe, you marke his fauourites flies,\\nThe poore aduanc'd, makes Friends of Enemies:\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='And hitherto doth Loue on Fortune tend,\\nFor who not needs, shall neuer lacke a Frend:\\nAnd who in want a hollow Friend doth try,\\nDirectly seasons him his Enemie.\\nBut orderly to end, where I begun,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Our Willes and Fates do so contrary run,\\nThat our Deuices still are ouerthrowne,\\nOur thoughts are ours, their ends none of our owne.\\nSo thinke thou wilt no second Husband wed.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='But die thy thoughts, when thy first Lord is dead'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Bap. Nor Earth to giue me food, nor Heauen light,\\nSport and repose locke from me day and night:\\nEach opposite that blankes the face of ioy,\\nMeet what I would haue well, and it destroy:'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Both heere, and hence, pursue me lasting strife,\\nIf once a Widdow, euer I be Wife'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. If she should breake it now\\n\\n   King. 'Tis deepely sworne:\\nSweet, leaue me heere a while,\\nMy spirits grow dull, and faine I would beguile\\nThe tedious day with sleepe\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Qu. Sleepe rocke thy Braine,\\n\\nSleepes\\n\\nAnd neuer come mischance betweene vs twaine.\\n\\nExit\\n\\n  Ham. Madam, how like you this Play?\\n  Qu. The Lady protests to much me thinkes'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Oh but shee'l keepe her word\\n\\n   King. Haue you heard the Argument, is there no Offence\\nin't?\\n  Ham. No, no, they do but iest, poyson in iest, no Offence\\ni'th' world\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='King. What do you call the Play?\\n  Ham. The Mouse-trap: Marry how? Tropically:\\nThis Play is the Image of a murder done in Vienna: Gonzago\\nis the Dukes name, his wife Baptista: you shall see'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"anon: 'tis a knauish peece of worke: But what o'that?\\nYour Maiestie, and wee that haue free soules, it touches\\nvs not: let the gall'd iade winch: our withers are vnrung.\\nEnter Lucianus.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='This is one Lucianus nephew to the King\\n\\n   Ophe. You are a good Chorus, my Lord\\n\\n   Ham. I could interpret betweene you and your loue:\\nif I could see the Puppets dallying'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ophe. You are keene my Lord, you are keene\\n\\n   Ham. It would cost you a groaning, to take off my\\nedge\\n\\n   Ophe. Still better and worse'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. So you mistake Husbands.\\nBegin Murderer. Pox, leaue thy damnable Faces, and\\nbegin. Come, the croaking Rauen doth bellow for Reuenge'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Lucian. Thoughts blacke, hands apt,\\nDrugges fit, and Time agreeing:\\nConfederate season, else, no Creature seeing:\\nThou mixture ranke, of Midnight Weeds collected,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='With Hecats Ban, thrice blasted, thrice infected,\\nThy naturall Magicke, and dire propertie,\\nOn wholsome life, vsurpe immediately.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Powres the poyson in his eares.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. He poysons him i'th' Garden for's estate: His\\nname's Gonzago: the Story is extant and writ in choyce\\nItalian. You shall see anon how the Murtherer gets the\\nloue of Gonzago's wife\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ophe. The King rises\\n\\n   Ham. What, frighted with false fire\\n\\n   Qu. How fares my Lord?\\n  Pol. Giue o're the Play\\n\\n   King. Giue me some Light. Away\\n\\n   All. Lights, Lights, Lights.\\n\\nExeunt.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Exeunt.\\n\\nManet Hamlet & Horatio.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. Why let the strucken Deere go weepe,\\nThe Hart vngalled play:\\nFor some must watch, while some must sleepe;\\nSo runnes the world away.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Would not this Sir, and a Forrest of Feathers, if the rest of\\nmy Fortunes turne Turke with me; with two Prouinciall\\nRoses on my rac'd Shooes, get me a Fellowship in a crie\\nof Players sir\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Hor. Halfe a share\\n\\n   Ham. A whole one I,\\nFor thou dost know: Oh Damon deere,\\nThis Realme dismantled was of Ioue himselfe,\\nAnd now reignes heere.\\nA verie verie Paiocke'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Hora. You might haue Rim'd\\n\\n   Ham. Oh good Horatio, Ile take the Ghosts word for\\na thousand pound. Did'st perceiue?\\n  Hora. Verie well my Lord\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. Vpon the talke of the poysoning?\\n  Hora. I did verie well note him.\\nEnter Rosincrance and Guildensterne.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. Oh, ha? Come some Musick. Come y Recorders:\\nFor if the King like not the Comedie,\\nWhy then belike he likes it not perdie.\\nCome some Musicke'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Guild. Good my Lord, vouchsafe me a word with you\\n\\n   Ham. Sir, a whole History\\n\\n   Guild. The King, sir\\n\\n   Ham. I sir, what of him?\\n  Guild. Is in his retyrement, maruellous distemper'd\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. With drinke Sir?\\n  Guild. No my Lord, rather with choller'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. Your wisedome should shew it selfe more richer,\\nto signifie this to his Doctor: for for me to put him\\nto his Purgation, would perhaps plundge him into farre\\nmore Choller'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Guild. Good my Lord put your discourse into some\\nframe, and start not so wildely from my affayre\\n\\n   Ham. I am tame Sir, pronounce'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Guild. The Queene your Mother, in most great affliction\\nof spirit, hath sent me to you\\n\\n   Ham. You are welcome'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Guild. Nay, good my Lord, this courtesie is not of\\nthe right breed. If it shall please you to make me a wholsome\\nanswer, I will doe your Mothers command'ment:\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='if not, your pardon, and my returne shall bee the end of\\nmy Businesse'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. Sir, I cannot'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Guild. What, my Lord?\\n  Ham. Make you a wholsome answere: my wits diseas'd.\\nBut sir, such answers as I can make, you shal command:\\nor rather you say, my Mother: therfore no more\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='but to the matter. My Mother you say'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Rosin. Then thus she sayes: your behauior hath stroke\\nher into amazement, and admiration'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. Oh wonderfull Sonne, that can so astonish a\\nMother. But is there no sequell at the heeles of this Mothers\\nadmiration?\\n  Rosin. She desires to speake with you in her Closset,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='ere you go to bed'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. We shall obey, were she ten times our Mother.\\nHaue you any further Trade with vs?\\n  Rosin. My Lord, you once did loue me\\n\\n   Ham. So I do still, by these pickers and stealers'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Rosin. Good my Lord, what is your cause of distemper?\\nYou do freely barre the doore of your owne Libertie,\\nif you deny your greefes to your Friend\\n\\n   Ham. Sir I lacke Aduancement'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Rosin. How can that be, when you haue the voyce of\\nthe King himselfe, for your Succession in Denmarke?\\n  Ham. I, but while the grasse growes, the Prouerbe is\\nsomething musty.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='something musty.\\nEnter one with a Recorder.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='O the Recorder. Let me see, to withdraw with you, why\\ndo you go about to recouer the winde of mee, as if you\\nwould driue me into a toyle?\\n  Guild. O my Lord, if my Dutie be too bold, my loue'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='is too vnmannerly'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. I do not well vnderstand that. Will you play\\nvpon this Pipe?\\n  Guild. My Lord, I cannot\\n\\n   Ham. I pray you\\n\\n   Guild. Beleeue me, I cannot\\n\\n   Ham. I do beseech you'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Guild. I know no touch of it, my Lord'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. 'Tis as easie as lying: gouerne these Ventiges\\nwith your finger and thumbe, giue it breath with your\\nmouth, and it will discourse most excellent Musicke.\\nLooke you, these are the stoppes\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Guild. But these cannot I command to any vtterance\\nof hermony, I haue not the skill'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. Why looke you now, how vnworthy a thing\\nyou make of me: you would play vpon mee; you would\\nseeme to know my stops: you would pluck out the heart'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='of my Mysterie; you would sound mee from my lowest\\nNote, to the top of my Compasse: and there is much Musicke,\\nexcellent Voice, in this little Organe, yet cannot'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='you make it. Why do you thinke, that I am easier to bee\\nplaid on, then a Pipe? Call me what Instrument you will,\\nthough you can fret me, you cannot play vpon me. God\\nblesse you Sir.\\nEnter Polonius.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Polon. My Lord; the Queene would speak with you,\\nand presently\\n\\n   Ham. Do you see that Clowd? that's almost in shape\\nlike a Camell\\n\\n   Polon. By'th' Masse, and it's like a Camell indeed\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Me thinkes it is like a Weazell\\n\\n   Polon. It is back'd like a Weazell\\n\\n   Ham. Or like a Whale?\\n  Polon. Verie like a Whale\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. Then will I come to my Mother, by and by:\\nThey foole me to the top of my bent.\\nI will come by and by\\n\\n   Polon. I will say so.\\nEnter.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. By and by, is easily said. Leaue me Friends:\\n'Tis now the verie witching time of night,\\nWhen Churchyards yawne, and Hell it selfe breaths out\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Contagion to this world. Now could I drink hot blood,\\nAnd do such bitter businesse as the day\\nWould quake to looke on. Soft now, to my Mother:\\nOh Heart, loose not thy Nature; let not euer'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='The Soule of Nero, enter this firme bosome:\\nLet me be cruell, not vnnaturall,\\nI will speake Daggers to her, but vse none:\\nMy Tongue and Soule in this be Hypocrites.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='How in my words someuer she be shent,\\nTo giue them Seales, neuer my Soule consent.\\nEnter King, Rosincrance, and Guildensterne.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='King. I like him not, nor stands it safe with vs,\\nTo let his madnesse range. Therefore prepare you,\\nI your Commission will forthwith dispatch,\\nAnd he to England shall along with you:'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='The termes of our estate, may not endure\\nHazard so dangerous as doth hourely grow\\nOut of his Lunacies'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Guild. We will our selues prouide:\\nMost holie and Religious feare it is\\nTo keepe those many many bodies safe\\nThat liue and feede vpon your Maiestie'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Rosin. The single\\nAnd peculiar life is bound\\nWith all the strength and Armour of the minde,\\nTo keepe it selfe from noyance: but much more,\\nThat Spirit, vpon whose spirit depends and rests'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"The liues of many, the cease of Maiestie\\nDies not alone; but like a Gulfe doth draw\\nWhat's neere it, with it. It is a massie wheele\\nFixt on the Somnet of the highest Mount.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"To whose huge Spoakes, ten thousand lesser things\\nAre mortiz'd and adioyn'd: which when it falles,\\nEach small annexment, pettie consequence\\nAttends the boystrous Ruine. Neuer alone\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Did the King sighe, but with a generall grone'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='King. Arme you, I pray you to this speedie Voyage;\\nFor we will Fetters put vpon this feare,\\nWhich now goes too free-footed\\n\\n   Both. We will haste vs.\\n\\nExeunt. Gent.\\n\\nEnter Polonius.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Pol. My Lord, he's going to his Mothers Closset:\\nBehinde the Arras Ile conuey my selfe\\nTo heare the Processe. Ile warrant shee'l tax him home,\\nAnd as you said, and wisely was it said,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"'Tis meete that some more audience then a Mother,\\nSince Nature makes them partiall, should o're-heare\\nThe speech of vantage. Fare you well my Liege,\\nIle call vpon you ere you go to bed,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='And tell you what I know'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"King. Thankes deere my Lord.\\nOh my offence is ranke, it smels to heauen,\\nIt hath the primall eldest curse vpon't,\\nA Brothers murther. Pray can I not,\\nThough inclination be as sharpe as will:\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='My stronger guilt, defeats my strong intent,\\nAnd like a man to double businesse bound,\\nI stand in pause where I shall first begin,\\nAnd both neglect; what if this cursed hand'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Were thicker then it selfe with Brothers blood,\\nIs there not Raine enough in the sweet Heauens\\nTo wash it white as Snow? Whereto serues mercy,\\nBut to confront the visage of Offence?'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"And what's in Prayer, but this two-fold force,\\nTo be fore-stalled ere we come to fall,\\nOr pardon'd being downe? Then Ile looke vp,\\nMy fault is past. But oh, what forme of Prayer\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Can serue my turne? Forgiue me my foule Murther:\\nThat cannot be, since I am still possest\\nOf those effects for which I did the Murther.\\nMy Crowne, mine owne Ambition, and my Queene:'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"May one be pardon'd, and retaine th' offence?\\nIn the corrupted currants of this world,\\nOffences gilded hand may shoue by Iustice,\\nAnd oft 'tis seene, the wicked prize it selfe\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Buyes out the Law; but 'tis not so aboue,\\nThere is no shuffling, there the Action lyes\\nIn his true Nature, and we our selues compell'd\\nEuen to the teeth and forehead of our faults,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='To giue in euidence. What then? What rests?\\nTry what Repentance can. What can it not?\\nYet what can it, when one cannot repent?\\nOh wretched state! Oh bosome, blacke as death!'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Oh limed soule, that strugling to be free,\\nArt more ingag'd: Helpe Angels, make assay:\\nBow stubborne knees, and heart with strings of Steele,\\nBe soft as sinewes of the new-borne Babe,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='All may be well.\\nEnter Hamlet.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Now might I do it pat, now he is praying,\\nAnd now Ile doo't, and so he goes to Heauen,\\nAnd so am I reueng'd: that would be scann'd,\\nA Villaine killes my Father, and for that\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='I his foule Sonne, do this same Villaine send\\nTo heauen. Oh this is hyre and Sallery, not Reuenge.\\nHe tooke my Father grossely, full of bread,\\nWith all his Crimes broad blowne, as fresh as May,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"And how his Audit stands, who knowes, saue Heauen:\\nBut in our circumstance and course of thought\\n'Tis heauie with him: and am I then reueng'd,\\nTo take him in the purging of his Soule,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"When he is fit and season'd for his passage? No.\\nVp Sword, and know thou a more horrid hent\\nWhen he is drunke asleepe: or in his Rage,\\nOr in th' incestuous pleasure of his bed,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"At gaming, swearing, or about some acte\\nThat ha's no rellish of Saluation in't,\\nThen trip him, that his heeles may kicke at Heauen,\\nAnd that his Soule may be as damn'd and blacke\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='As Hell, whereto it goes. My Mother stayes,\\nThis Physicke but prolongs thy sickly dayes.\\nEnter.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='King. My words flye vp, my thoughts remain below,\\nWords without thoughts, neuer to Heauen go.\\nEnter.\\n\\nEnter Queene and Polonius.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Pol. He will come straight:\\nLooke you lay home to him,\\nTell him his prankes haue been too broad to beare with,\\nAnd that your Grace hath screen'd, and stoode betweene\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Much heate, and him. Ile silence me e'ene heere:\\nPray you be round with him\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. within. Mother, mother, mother\\n\\n   Qu. Ile warrant you, feare me not.\\nWithdraw, I heare him coming.\\nEnter Hamlet.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Now Mother, what's the matter?\\n  Qu. Hamlet, thou hast thy Father much offended\\n\\n\\n   Ham. Mother, you haue my Father much offended\\n\\n   Qu. Come, come, you answer with an idle tongue\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. Go, go, you question with an idle tongue'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Qu. Why how now Hamlet?\\n  Ham. Whats the matter now?\\n  Qu. Haue you forgot me?\\n  Ham. No by the Rood, not so:\\nYou are the Queene, your Husbands Brothers wife,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='But would you were not so. You are my Mother'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Qu. Nay, then Ile set those to you that can speake'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. Come, come, and sit you downe, you shall not\\nboudge:\\nYou go not till I set you vp a glasse,\\nWhere you may see the inmost part of you?\\n  Qu. What wilt thou do? thou wilt not murther me?'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Helpe, helpe, hoa'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Pol. What hoa, helpe, helpe, helpe\\n\\n   Ham. How now, a Rat? dead for a Ducate, dead\\n\\n   Pol. Oh I am slaine.\\n\\nKilles Polonius'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Qu. Oh me, what hast thou done?\\n  Ham. Nay I know not, is it the King?\\n  Qu. Oh what a rash, and bloody deed is this?\\n  Ham. A bloody deed, almost as bad good Mother,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='As kill a King, and marrie with his Brother'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Qu. As kill a King?\\n  Ham. I Lady, 'twas my word.\\nThou wretched, rash, intruding foole farewell,\\nI tooke thee for thy Betters, take thy Fortune,\\nThou find'st to be too busie, is some danger.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Leaue wringing of your hands, peace, sit you downe,\\nAnd let me wring your heart, for so I shall\\nIf it be made of penetrable stuffe;\\nIf damned Custome haue not braz'd it so,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='That it is proofe and bulwarke against Sense'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Qu. What haue I done, that thou dar'st wag thy tong,\\nIn noise so rude against me?\\n  Ham. Such an Act\\nThat blurres the grace and blush of Modestie,\\nCals Vertue Hypocrite, takes off the Rose\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='From the faire forehead of an innocent loue,\\nAnd makes a blister there. Makes marriage vowes\\nAs false as Dicers Oathes. Oh such a deed,\\nAs from the body of Contraction pluckes'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='The very soule, and sweete Religion makes\\nA rapsidie of words. Heauens face doth glow,\\nYea this solidity and compound masse,\\nWith tristfull visage as against the doome,\\nIs thought-sicke at the act'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Qu. Aye me; what act, that roares so lowd, & thunders\\nin the Index'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. Looke heere vpon this Picture, and on this,\\nThe counterfet presentment of two Brothers:\\nSee what a grace was seated on his Brow,\\nHyperions curles, the front of Ioue himselfe,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='An eye like Mars, to threaten or command\\nA Station, like the Herald Mercurie\\nNew lighted on a heauen-kissing hill:\\nA Combination, and a forme indeed,\\nWhere euery God did seeme to set his Seale,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"To giue the world assurance of a man.\\nThis was your Husband. Looke you now what followes.\\nHeere is your Husband, like a Mildew'd eare\\nBlasting his wholsom breath. Haue you eyes?\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Could you on this faire Mountaine leaue to feed,\\nAnd batten on this Moore? Ha? Haue you eyes?\\nYou cannot call it Loue: For at your age,\\nThe hey-day in the blood is tame, it's humble,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"And waites vpon the Iudgement: and what Iudgement\\nWould step from this, to this? What diuell was't,\\nThat thus hath cousend you at hoodman-blinde?\\nO Shame! where is thy Blush? Rebellious Hell,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='If thou canst mutine in a Matrons bones,\\nTo flaming youth, let Vertue be as waxe.\\nAnd melt in her owne fire. Proclaime no shame,\\nWhen the compulsiue Ardure giues the charge,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Since Frost it selfe, as actiuely doth burne,\\nAs Reason panders Will'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Qu. O Hamlet, speake no more.\\nThou turn'st mine eyes into my very soule,\\nAnd there I see such blacke and grained spots,\\nAs will not leaue their Tinct\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Nay, but to liue\\nIn the ranke sweat of an enseamed bed,\\nStew'd in Corruption; honying and making loue\\nOuer the nasty Stye\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Qu. Oh speake to me, no more,\\nThese words like Daggers enter in mine eares.\\nNo more sweet Hamlet'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. A Murderer, and a Villaine:\\nA Slaue, that is not twentieth part the tythe\\nOf your precedent Lord. A vice of Kings,\\nA Cutpurse of the Empire and the Rule.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='That from a shelfe, the precious Diadem stole,\\nAnd put it in his Pocket'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Qu. No more.\\nEnter Ghost.\\n\\n  Ham. A King of shreds and patches.\\nSaue me; and houer o're me with your wings\\nYou heauenly Guards. What would your gracious figure?\\n  Qu. Alas he's mad\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Do you not come your tardy Sonne to chide,\\nThat laps't in Time and Passion, lets go by\\nTh' important acting of your dread command? Oh say\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ghost. Do not forget: this Visitation\\nIs but to whet thy almost blunted purpose.\\nBut looke, Amazement on thy Mother sits;\\nO step betweene her, and her fighting Soule,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Conceit in weakest bodies, strongest workes.\\nSpeake to her Hamlet'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. How is it with you Lady?\\n  Qu. Alas, how is't with you?\\nThat you bend your eye on vacancie,\\nAnd with their corporall ayre do hold discourse.\\nForth at your eyes, your spirits wildely peepe,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"And as the sleeping Soldiours in th' Alarme,\\nYour bedded haire, like life in excrements,\\nStart vp, and stand an end. Oh gentle Sonne,\\nVpon the heate and flame of thy distemper\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Sprinkle coole patience. Whereon do you looke?\\n  Ham. On him, on him: look you how pale he glares,\\nHis forme and cause conioyn'd, preaching to stones,\\nWould make them capeable. Do not looke vpon me,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Least with this pitteous action you conuert\\nMy sterne effects: then what I haue to do,\\nWill want true colour; teares perchance for blood'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Qu. To who do you speake this?\\n  Ham. Do you see nothing there?\\n  Qu. Nothing at all, yet all that is I see\\n\\n   Ham. Nor did you nothing heare?\\n  Qu. No, nothing but our selues'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. Why look you there: looke how it steals away:\\nMy Father in his habite, as he liued,\\nLooke where he goes euen now out at the Portall.\\nEnter.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Qu. This is the very coynage of your Braine,\\nThis bodilesse Creation extasie is very cunning in'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. Extasie?\\nMy Pulse as yours doth temperately keepe time,\\nAnd makes as healthfull Musicke. It is not madnesse\\nThat I haue vttered; bring me to the Test'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='And I the matter will re-word: which madnesse\\nWould gamboll from. Mother, for loue of Grace,\\nLay not a flattering Vnction to your soule,\\nThat not your trespasse, but my madnesse speakes:'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"It will but skin and filme the Vlcerous place,\\nWhil'st ranke Corruption mining all within,\\nInfects vnseene. Confesse your selfe to Heauen,\\nRepent what's past, auoyd what is to come,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='And do not spred the Compost on the Weedes,\\nTo make them ranke. Forgiue me this my Vertue,\\nFor in the fatnesse of this pursie times,\\nVertue it selfe, of Vice must pardon begge,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Yea courb, and woe, for leaue to do him good'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Qu. Oh Hamlet,\\nThou hast cleft my heart in twaine'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. O throw away the worser part of it,\\nAnd liue the purer with the other halfe.\\nGood night, but go not to mine Vnkles bed,\\nAssume a Vertue, if you haue it not, refraine to night,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='And that shall lend a kinde of easinesse\\nTo the next abstinence. Once more goodnight,\\nAnd when you are desirous to be blest,\\nIle blessing begge of you. For this same Lord,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"I do repent: but heauen hath pleas'd it so,\\nTo punish me with this, and this with me,\\nThat I must be their Scourge and Minister.\\nI will bestow him, and will answer well\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='The death I gaue him: so againe, good night.\\nI must be cruell, onely to be kinde;\\nThus bad begins and worse remaines behinde'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Qu. What shall I do?\\n  Ham. Not this by no meanes that I bid you do:\\nLet the blunt King tempt you againe to bed,\\nPinch Wanton on your cheeke, call you his Mouse,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"And let him for a paire of reechie kisses,\\nOr padling in your necke with his damn'd Fingers,\\nMake you to rauell all this matter out,\\nThat I essentially am not in madnesse,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"But made in craft. 'Twere good you let him know,\\nFor who that's but a Queene, faire, sober, wise,\\nWould from a Paddocke, from a Bat, a Gibbe,\\nSuch deere concernings hide, Who would do so,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='No in despight of Sense and Secrecie,\\nVnpegge the Basket on the houses top:\\nLet the Birds flye, and like the famous Ape\\nTo try Conclusions in the Basket, creepe\\nAnd breake your owne necke downe'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Qu. Be thou assur'd, if words be made of breath,\\nAnd breath of life: I haue no life to breath\\nWhat thou hast saide to me\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. I must to England, you know that?\\n  Qu. Alacke I had forgot: 'Tis so concluded on\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. This man shall set me packing:\\nIle lugge the Guts into the Neighbor roome,\\nMother goodnight. Indeede this Counsellor\\nIs now most still, most secret, and most graue,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Who was in life, a foolish prating Knaue.\\nCome sir, to draw toward an end with you.\\nGood night Mother.\\nExit Hamlet tugging in Polonius.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Enter King.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"King. There's matters in these sighes.\\nThese profound heaues\\nYou must translate; Tis fit we vnderstand them.\\nWhere is your Sonne?\\n  Qu. Ah my good Lord, what haue I seene to night?\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"King. What Gertrude? How do's Hamlet?\\n  Qu. Mad as the Seas, and winde, when both contend\\nWhich is the Mightier, in his lawlesse fit\\nBehinde the Arras, hearing something stirre,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='He whips his Rapier out, and cries a Rat, a Rat,\\nAnd in his brainish apprehension killes\\nThe vnseene good old man'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='King. Oh heauy deed:\\nIt had bin so with vs had we beene there:\\nHis Liberty is full of threats to all,\\nTo you your selfe, to vs, to euery one.\\nAlas, how shall this bloody deede be answered?'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"It will be laide to vs, whose prouidence\\nShould haue kept short, restrain'd, and out of haunt,\\nThis mad yong man. But so much was our loue,\\nWe would not vnderstand what was most fit,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"But like the Owner of a foule disease,\\nTo keepe it from divulging, let's it feede\\nEuen on the pith of life. Where is he gone?\\n  Qu. To draw apart the body he hath kild,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"O're whom his very madnesse like some Oare\\nAmong a Minerall of Mettels base\\nShewes it selfe pure. He weepes for what is done\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='King. Oh Gertrude, come away:\\nThe Sun no sooner shall the Mountaines touch,\\nBut we will ship him hence, and this vilde deed,\\nWe must with all our Maiesty and Skill\\nBoth countenance, and excuse.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Enter Ros. & Guild.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ho Guildenstern:\\nFriends both go ioyne you with some further ayde:\\nHamlet in madnesse hath Polonius slaine,\\nAnd from his Mother Clossets hath he drag'd him.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Go seeke him out, speake faire, and bring the body\\nInto the Chappell. I pray you hast in this.\\nExit Gent.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Come Gertrude, wee'l call vp our wisest friends,\\nTo let them know both what we meane to do,\\nAnd what's vntimely done. Oh come away,\\nMy soule is full of discord and dismay.\\n\\nExeunt.\\n\\nEnter Hamlet.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Enter Hamlet.\\n\\n  Ham. Safely stowed\\n\\n   Gentlemen within. Hamlet, Lord Hamlet\\n\\n   Ham. What noise? Who cals on Hamlet?\\nOh heere they come.\\nEnter Ros. and Guildensterne.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ro. What haue you done my Lord with the dead body?\\n  Ham. Compounded it with dust, whereto 'tis Kinne\\n\\n   Rosin. Tell vs where 'tis, that we may take it thence,\\nAnd beare it to the Chappell\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. Do not beleeue it'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Rosin. Beleeue what?\\n  Ham. That I can keepe your counsell, and not mine\\nowne. Besides, to be demanded of a Spundge, what replication\\nshould be made by the Sonne of a King'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Rosin. Take you me for a Spundge, my Lord?\\n  Ham. I sir, that sokes vp the Kings Countenance, his\\nRewards, his Authorities (but such Officers do the King'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"best seruice in the end. He keepes them like an Ape in\\nthe corner of his iaw, first mouth'd to be last swallowed,\\nwhen he needes what you haue glean'd, it is but squeezing\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='you, and Spundge you shall be dry againe'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Rosin. I vnderstand you not my Lord\\n\\n   Ham. I am glad of it: a knauish speech sleepes in a\\nfoolish eare\\n\\n   Rosin. My Lord, you must tell vs where the body is,\\nand go with vs to the King'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. The body is with the King, but the King is not\\nwith the body. The King, is a thing-\\n  Guild. A thing my Lord?\\n  Ham. Of nothing: bring me to him, hide Fox, and all\\nafter.\\n\\nExeunt.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Exeunt.\\n\\nEnter King.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"King. I haue sent to seeke him, and to find the bodie:\\nHow dangerous is it that this man goes loose:\\nYet must not we put the strong Law on him:\\nHee's loued of the distracted multitude,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Who like not in their iudgement, but their eyes:\\nAnd where 'tis so, th' Offenders scourge is weigh'd\\nBut neerer the offence: to beare all smooth, and euen,\\nThis sodaine sending him away, must seeme\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Deliberate pause, diseases desperate growne,\\nBy desperate appliance are releeued,\\nOr not at all.\\nEnter Rosincrane.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"How now? What hath befalne?\\n  Rosin. Where the dead body is bestow'd my Lord,\\nWe cannot get from him\\n\\n   King. But where is he?\\n  Rosin. Without my Lord, guarded to know your\\npleasure\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"King. Bring him before vs\\n\\n   Rosin. Hoa, Guildensterne? Bring in my Lord.\\nEnter Hamlet and Guildensterne.\\n\\n  King. Now Hamlet, where's Polonius?\\n  Ham. At Supper\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"King. At Supper? Where?\\n  Ham. Not where he eats, but where he is eaten, a certaine\\nconuocation of wormes are e'ne at him. Your worm\\nis your onely Emperor for diet. We fat all creatures else\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"to fat vs, and we fat our selfe for Magots. Your fat King,\\nand your leane Begger is but variable seruice to dishes,\\nbut to one Table that's the end\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='King. What dost thou meane by this?\\n  Ham. Nothing but to shew you how a King may go\\na Progresse through the guts of a Begger\\n\\n   King. Where is Polonius'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. In heauen, send thither to see. If your Messenger\\nfinde him not there, seeke him i'th other place your\\nselfe: but indeed, if you finde him not this moneth, you\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='shall nose him as you go vp the staires into the Lobby'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='King. Go seeke him there\\n\\n   Ham. He will stay till ye come'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='K. Hamlet, this deed of thine, for thine especial safety\\nWhich we do tender, as we deerely greeue\\nFor that which thou hast done, must send thee hence'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"With fierie Quicknesse. Therefore prepare thy selfe,\\nThe Barke is readie, and the winde at helpe,\\nTh' Associates tend, and euery thing at bent\\nFor England\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. For England?\\n  King. I Hamlet\\n\\n   Ham. Good\\n\\n   King. So is it, if thou knew'st our purposes\\n\\n   Ham. I see a Cherube that see's him: but come, for\\nEngland. Farewell deere Mother\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='King. Thy louing Father Hamlet\\n\\n   Hamlet. My Mother: Father and Mother is man and\\nwife: man & wife is one flesh, and so my mother. Come,\\nfor England.\\n\\nExit'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"King. Follow him at foote,\\nTempt him with speed aboord:\\nDelay it not, Ile haue him hence to night.\\nAway, for euery thing is Seal'd and done\\nThat else leanes on th' Affaire, pray you make hast.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='And England, if my loue thou holdst at ought,\\nAs my great power thereof may giue thee sense,\\nSince yet thy Cicatrice lookes raw and red\\nAfter the Danish Sword, and thy free awe'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Payes homage to vs; thou maist not coldly set\\nOur Soueraigne Processe, which imports at full\\nBy Letters coniuring to that effect\\nThe present death of Hamlet. Do it England,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"For like the Hecticke in my blood he rages,\\nAnd thou must cure me: Till I know 'tis done,\\nHow ere my happes, my ioyes were ne're begun.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Exit\\n\\nEnter Fortinbras with an Armie.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"For. Go Captaine, from me greet the Danish King,\\nTell him that by his license, Fortinbras\\nClaimes the conueyance of a promis'd March\\nOuer his Kingdome. You know the Rendeuous:\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='If that his Maiesty would ought with vs,\\nWe shall expresse our dutie in his eye,\\nAnd let him know so'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Cap. I will doo't, my Lord\\n\\n   For. Go safely on.\\nEnter.\\n\\nEnter Queene and Horatio.\\n\\n  Qu. I will not speake with her\\n\\n   Hor. She is importunate, indeed distract, her moode\\nwill needs be pittied\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Qu. What would she haue?\\n  Hor. She speakes much of her Father; saies she heares\\nThere's trickes i'th' world, and hems, and beats her heart,\\nSpurnes enuiously at Strawes, speakes things in doubt,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='That carry but halfe sense: Her speech is nothing,\\nYet the vnshaped vse of it doth moue\\nThe hearers to Collection; they ayme at it,\\nAnd botch the words vp fit to their owne thoughts,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Which as her winkes, and nods, and gestures yeeld them,\\nIndeed would make one thinke there would be thought,\\nThough nothing sure, yet much vnhappily'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Qu. 'Twere good she were spoken with,\\nFor she may strew dangerous coniectures\\nIn ill breeding minds. Let her come in.\\nTo my sicke soule (as sinnes true Nature is)\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Each toy seemes Prologue, to some great amisse,\\nSo full of Artlesse iealousie is guilt,\\nIt spill's it selfe, in fearing to be spilt.\\nEnter Ophelia distracted.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ophe. Where is the beauteous Maiesty of Denmark\\n\\n   Qu. How now Ophelia?\\n  Ophe. How should I your true loue know from another one?\\nBy his Cockle hat and staffe, and his Sandal shoone'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Qu. Alas sweet Lady: what imports this Song?\\n  Ophe. Say you? Nay pray you marke.\\nHe is dead and gone Lady, he is dead and gone,\\nAt his head a grasse-greene Turfe, at his heeles a stone.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Enter King.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Qu. Nay but Ophelia\\n\\n   Ophe. Pray you marke.\\nWhite his Shrow'd as the Mountaine Snow\\n\\n   Qu. Alas, looke heere my Lord\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ophe. Larded with sweet Flowers:\\nWhich bewept to the graue did not go,\\nWith true-loue showres'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"King. How do ye, pretty Lady?\\n  Ophe. Well, God dil'd you. They say the Owle was\\na Bakers daughter. Lord, wee know what we are, but\\nknow not what we may be. God be at your Table\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='King. Conceit vpon her Father'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ophe. Pray you let's haue no words of this: but when\\nthey aske you what it meanes, say you this:\\nTo morrow is S[aint]. Valentines day, all in the morning betime,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"And I a Maid at your Window, to be your Valentine.\\nThen vp he rose, & don'd his clothes, & dupt the chamber dore,\\nLet in the Maid, that out a Maid, neuer departed more\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='King. Pretty Ophelia'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ophe. Indeed la? without an oath Ile make an end ont.\\nBy gis, and by S[aint]. Charity,\\nAlacke, and fie for shame:\\nYong men wil doo't, if they come too't,\\nBy Cocke they are too blame.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Quoth she before you tumbled me,\\nYou promis'd me to Wed:\\nSo would I ha done by yonder Sunne,\\nAnd thou hadst not come to my bed\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='King. How long hath she bin thus?\\n  Ophe. I hope all will be well. We must bee patient,\\nbut I cannot choose but weepe, to thinke they should'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"lay him i'th' cold ground: My brother shall knowe of it,\\nand so I thanke you for your good counsell. Come, my\\nCoach: Goodnight Ladies: Goodnight sweet Ladies:\\nGoodnight, goodnight.\\nEnter.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='King. Follow her close,\\nGiue her good watch I pray you:\\nOh this is the poyson of deepe greefe, it springs\\nAll from her Fathers death. Oh Gertrude, Gertrude,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='When sorrowes comes, they come not single spies,\\nBut in Battalians. First, her Father slaine,\\nNext your Sonne gone, and he most violent Author\\nOf his owne iust remoue: the people muddied,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Thicke and vnwholsome in their thoughts, and whispers\\nFor good Polonius death; and we haue done but greenly\\nIn hugger mugger to interre him. Poore Ophelia'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Diuided from her selfe, and her faire Iudgement,\\nWithout the which we are Pictures, or meere Beasts.\\nLast, and as much containing as all these,\\nHer Brother is in secret come from France,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Keepes on his wonder, keepes himselfe in clouds,\\nAnd wants not Buzzers to infect his eare\\nWith pestilent Speeches of his Fathers death,\\nWhere in necessitie of matter Beggard,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Will nothing sticke our persons to Arraigne\\nIn eare and eare. O my deere Gertrude, this,\\nLike to a murdering Peece in many places,\\nGiues me superfluous death.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='A Noise within.\\n\\nEnter a Messenger.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Qu. Alacke, what noyse is this?\\n  King. Where are my Switzers?\\nLet them guard the doore. What is the matter?\\n  Mes. Saue your selfe, my Lord.\\nThe Ocean (ouer-peering of his List)'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Eates not the Flats with more impittious haste\\nThen young Laertes, in a Riotous head,\\nOre-beares your Officers, the rabble call him Lord,\\nAnd as the world were now but to begin,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Antiquity forgot, Custome not knowne,\\nThe Ratifiers and props of euery word,\\nThey cry choose we? Laertes shall be King,\\nCaps, hands, and tongues, applaud it to the clouds,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Laertes shall be King, Laertes King'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Qu. How cheerefully on the false Traile they cry,\\nOh this is Counter you false Danish Dogges.\\n\\nNoise within. Enter Laertes.\\n\\n  King. The doores are broke'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Laer. Where is the King, sirs? Stand you all without\\n\\n   All. No, let's come in\\n\\n   Laer. I pray you giue me leaue\\n\\n   Al. We will, we will\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Laer. I thanke you: Keepe the doore.\\nOh thou vilde King, giue me my Father\\n\\n   Qu. Calmely good Laertes'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Laer. That drop of blood, that calmes\\nProclaimes me Bastard:\\nCries Cuckold to my Father, brands the Harlot\\nEuen heere betweene the chaste vnsmirched brow\\nOf my true Mother'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"King. What is the cause Laertes,\\nThat thy Rebellion lookes so Gyant-like?\\nLet him go Gertrude: Do not feare our person:\\nThere's such Diuinity doth hedge a King,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='That Treason can but peepe to what it would,\\nActs little of his will. Tell me Laertes,\\nWhy thou art thus Incenst? Let him go Gertrude.\\nSpeake man'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Laer. Where's my Father?\\n  King. Dead\\n\\n   Qu. But not by him\\n\\n   King. Let him demand his fill\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Laer. How came he dead? Ile not be Iuggel'd with.\\nTo hell Allegeance: Vowes, to the blackest diuell.\\nConscience and Grace, to the profoundest Pit.\\nI dare Damnation: to this point I stand,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"That both the worlds I giue to negligence,\\nLet come what comes: onely Ile be reueng'd\\nMost throughly for my Father\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='King. Who shall stay you?\\n  Laer. My Will, not all the world,\\nAnd for my meanes, Ile husband them so well,\\nThey shall go farre with little'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='King. Good Laertes:\\nIf you desire to know the certaintie\\nOf your deere Fathers death, if writ in your reuenge,\\nThat Soop-stake you will draw both Friend and Foe,\\nWinner and Looser'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Laer. None but his Enemies\\n\\n   King. Will you know them then\\n\\n   La. To his good Friends, thus wide Ile ope my Armes:\\nAnd like the kinde Life-rend'ring Politician,\\nRepast them with my blood\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='King. Why now you speake\\nLike a good Childe, and a true Gentleman.\\nThat I am guiltlesse of your Fathers death,\\nAnd am most sensible in greefe for it,\\nIt shall as leuell to your Iudgement pierce'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"As day do's to your eye.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='A noise within. Let her come in.\\n\\nEnter Ophelia.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Laer. How now? what noise is that?\\nOh heate drie vp my Braines, teares seuen times salt,\\nBurne out the Sence and Vertue of mine eye.\\nBy Heauen, thy madnesse shall be payed by waight,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Till our Scale turnes the beame. Oh Rose of May,\\nDeere Maid, kinde Sister, sweet Ophelia:\\nOh Heauens, is't possible, a yong Maids wits,\\nShould be as mortall as an old mans life?\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Nature is fine in Loue, and where 'tis fine,\\nIt sends some precious instance of it selfe\\nAfter the thing it loues\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ophe. They bore him bare fac'd on the Beer,\\nHey non nony, nony, hey nony:\\nAnd on his graue raines many a teare,\\nFare you well my Doue\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Laer. Had'st thou thy wits, and did'st perswade Reuenge,\\nit could not moue thus\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ophe. You must sing downe a-downe, and you call\\nhim a-downe-a. Oh, how the wheele becomes it? It is\\nthe false Steward that stole his masters daughter\\n\\n   Laer. This nothings more then matter'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ophe. There's Rosemary, that's for Remembraunce.\\nPray loue remember: and there is Paconcies, that's for\\nThoughts\\n\\n   Laer. A document in madnesse, thoughts & remembrance\\nfitted\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ophe. There's Fennell for you, and Columbines: ther's\\nRew for you, and heere's some for me. Wee may call it\\nHerbe-Grace a Sundaies: Oh you must weare your Rew\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"with a difference. There's a Daysie, I would giue you\\nsome Violets, but they wither'd all when my Father dyed:\\nThey say, he made a good end;\\nFor bonny sweet Robin is all my ioy\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Laer. Thought, and Affliction, Passion, Hell it selfe:\\nShe turnes to Fauour, and to prettinesse'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ophe. And will he not come againe,\\nAnd will he not come againe:\\nNo, no, he is dead, go to thy Death-bed,\\nHe neuer wil come againe.\\nHis Beard as white as Snow,\\nAll Flaxen was his Pole:'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='He is gone, he is gone, and we cast away mone,\\nGramercy on his Soule.\\nAnd of all Christian Soules, I pray God.\\nGod buy ye.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Exeunt. Ophelia'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Laer. Do you see this, you Gods?\\n  King. Laertes, I must common with your greefe,\\nOr you deny me right: go but apart,\\nMake choice of whom your wisest Friends you will,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"And they shall heare and iudge 'twixt you and me;\\nIf by direct or by Colaterall hand\\nThey finde vs touch'd, we will our Kingdome giue,\\nOur Crowne, our Life, and all that we call Ours\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='To you in satisfaction. But if not,\\nBe you content to lend your patience to vs,\\nAnd we shall ioyntly labour with your soule\\nTo giue it due content'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Laer. Let this be so:\\nHis meanes of death, his obscure buriall;\\nNo Trophee, Sword, nor Hatchment o're his bones,\\nNo Noble rite, nor formall ostentation,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Cry to be heard, as 'twere from Heauen to Earth,\\nThat I must call in question\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"King. So you shall:\\nAnd where th' offence is, let the great Axe fall.\\nI pray you go with me.\\n\\nExeunt.\\n\\nEnter Horatio, with an Attendant.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Hora. What are they that would speake with me?\\n  Ser. Saylors sir, they say they haue Letters for you'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Hor. Let them come in,\\nI do not know from what part of the world\\nI should be greeted, if not from Lord Hamlet.\\nEnter Saylor.\\n\\n  Say. God blesse you Sir\\n\\n   Hor. Let him blesse thee too'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Say. Hee shall Sir, and't please him. There's a Letter\\nfor you Sir: It comes from th' Ambassadours that was\\nbound for England, if your name be Horatio, as I am let\\nto know it is.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Reads the Letter.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Horatio, When thou shalt haue ouerlook'd this, giue these\\nFellowes some meanes to the King: They haue Letters\\nfor him. Ere we were two dayes old at Sea, a Pyrate of very\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Warlicke appointment gaue vs Chace. Finding our selues too\\nslow of Saile, we put on a compelled Valour. In the Grapple, I\\nboorded them: On the instant they got cleare of our Shippe, so'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='I alone became their Prisoner. They haue dealt with mee, like\\nTheeues of Mercy, but they knew what they did. I am to doe\\na good turne for them. Let the King haue the Letters I haue'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='sent, and repaire thou to me with as much hast as thou wouldest\\nflye death. I haue words to speake in your eare, will make thee\\ndumbe, yet are they much too light for the bore of the Matter.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='These good Fellowes will bring thee where I am. Rosincrance\\nand Guildensterne, hold their course for England. Of them\\nI haue much to tell thee, Farewell.\\nHe that thou knowest thine,\\nHamlet.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Hamlet.\\nCome, I will giue you way for these your Letters,\\nAnd do't the speedier, that you may direct me\\nTo him from whom you brought them.\\nEnter.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Enter King and Laertes.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='King. Now must your conscience my acquittance seal,\\nAnd you must put me in your heart for Friend,\\nSith you haue heard, and with a knowing eare,\\nThat he which hath your Noble Father slaine,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Pursued my life'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Laer. It well appeares. But tell me,\\nWhy you proceeded not against these feates,\\nSo crimefull, and so Capitall in Nature,\\nAs by your Safety, Wisedome, all things else,\\nYou mainly were stirr'd vp?\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='King. O for two speciall Reasons,\\nWhich may to you (perhaps) seeme much vnsinnowed,\\nAnd yet to me they are strong. The Queen his Mother,\\nLiues almost by his lookes: and for my selfe,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"My Vertue or my Plague, be it either which,\\nShe's so coniunctiue to my life, and soule;\\nThat as the Starre moues not but in his Sphere,\\nI could not but by her. The other Motiue,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Why to a publike count I might not go,\\nIs the great loue the generall gender beare him,\\nWho dipping all his Faults in their affection,\\nWould like the Spring that turneth Wood to Stone,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Conuert his Gyues to Graces. So that my Arrowes\\nToo slightly timbred for so loud a Winde,\\nWould haue reuerted to my Bow againe,\\nAnd not where I had arm'd them\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Laer. And so haue I a Noble Father lost,\\nA Sister driuen into desperate tearmes,\\nWho was (if praises may go backe againe)\\nStood Challenger on mount of all the Age'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='For her perfections. But my reuenge will come'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='King. Breake not your sleepes for that,\\nYou must not thinke\\nThat we are made of stuffe, so flat, and dull,\\nThat we can let our Beard be shooke with danger,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"And thinke it pastime. You shortly shall heare more,\\nI lou'd your Father, and we loue our Selfe,\\nAnd that I hope will teach you to imagine-\\nEnter a Messenger.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='How now? What Newes?\\n  Mes. Letters my Lord from Hamlet, This to your\\nMaiesty: this to the Queene'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"King. From Hamlet? Who brought them?\\n  Mes. Saylors my Lord they say, I saw them not:\\nThey were giuen me by Claudio, he receiu'd them\\n\\n   King. Laertes you shall heare them:\\nLeaue vs.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Exit Messenger'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='High and Mighty, you shall know I am set naked on your\\nKingdome. To morrow shall I begge leaue to see your Kingly\\nEyes. When I shall (first asking your Pardon thereunto) recount'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"th' Occasions of my sodaine, and more strange returne.\\nHamlet.\\nWhat should this meane? Are all the rest come backe?\\nOr is it some abuse? Or no such thing?\\n  Laer. Know you the hand?\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Kin. 'Tis Hamlets Character, naked and in a Postscript\\nhere he sayes alone: Can you aduise me?\\n  Laer. I'm lost in it my Lord; but let him come,\\nIt warmes the very sicknesse in my heart,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='That I shall liue and tell him to his teeth;\\nThus diddest thou'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Kin. If it be so Laertes, as how should it be so:\\nHow otherwise will you be rul'd by me?\\n  Laer. If so you'l not o'rerule me to a peace\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Kin. To thine owne peace: if he be now return'd,\\nAs checking at his Voyage, and that he meanes\\nNo more to vndertake it; I will worke him\\nTo an exployt now ripe in my Deuice,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Vnder the which he shall not choose but fall;\\nAnd for his death no winde of blame shall breath,\\nBut euen his Mother shall vncharge the practice,\\nAnd call it accident: Some two Monthes hence'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Here was a Gentleman of Normandy,\\nI'ue seene my selfe, and seru'd against the French,\\nAnd they ran well on Horsebacke; but this Gallant\\nHad witchcraft in't; he grew into his Seat,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"And to such wondrous doing brought his Horse,\\nAs had he beene encorps't and demy-Natur'd\\nWith the braue Beast, so farre he past my thought,\\nThat I in forgery of shapes and trickes,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Come short of what he did'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Laer. A Norman was't?\\n  Kin. A Norman\\n\\n   Laer. Vpon my life Lamound\\n\\n   Kin. The very same\\n\\n   Laer. I know him well, he is the Brooch indeed,\\nAnd Iemme of all our Nation\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Kin. Hee mad confession of you,\\nAnd gaue you such a Masterly report,\\nFor Art and exercise in your defence;\\nAnd for your Rapier most especiall,\\nThat he cryed out, t'would be a sight indeed,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='If one could match you Sir. This report of his\\nDid Hamlet so envenom with his Enuy,\\nThat he could nothing doe but wish and begge,\\nYour sodaine comming ore to play with him;\\nNow out of this'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Laer. Why out of this, my Lord?\\n  Kin. Laertes was your Father deare to you?\\nOr are you like the painting of a sorrow,\\nA face without a heart?\\n  Laer. Why aske you this?'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Kin. Not that I thinke you did not loue your Father,\\nBut that I know Loue is begun by Time:\\nAnd that I see in passages of proofe,\\nTime qualifies the sparke and fire of it:'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Hamlet comes backe: what would you vndertake,\\nTo show your selfe your Fathers sonne indeed,\\nMore then in words?\\n  Laer. To cut his throat i'th' Church\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Kin. No place indeed should murder Sancturize;\\nReuenge should haue no bounds: but good Laertes\\nWill you doe this, keepe close within your Chamber,\\nHamlet return'd, shall know you are come home:\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Wee'l put on those shall praise your excellence,\\nAnd set a double varnish on the fame\\nThe Frenchman gaue you, bring you in fine together,\\nAnd wager on your heads, he being remisse,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Most generous, and free from all contriuing,\\nWill not peruse the Foiles? So that with ease,\\nOr with a little shuffling, you may choose\\nA Sword vnbaited, and in a passe of practice,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Requit him for your Father'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Laer. I will doo't.\\nAnd for that purpose Ile annoint my Sword:\\nI bought an Vnction of a Mountebanke\\nSo mortall, I but dipt a knife in it,\\nWhere it drawes blood, no Cataplasme so rare,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Collected from all Simples that haue Vertue\\nVnder the Moone, can saue the thing from death,\\nThat is but scratcht withall: Ile touch my point,\\nWith this contagion, that if I gall him slightly,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='It may be death'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Kin. Let's further thinke of this,\\nWeigh what conuenience both of time and meanes\\nMay fit vs to our shape, if this should faile;\\nAnd that our drift looke through our bad performance,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"'Twere better not assaid; therefore this Proiect\\nShould haue a backe or second, that might hold,\\nIf this should blast in proofe: Soft, let me see\\nWee'l make a solemne wager on your commings,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"I ha't: when in your motion you are hot and dry,\\nAs make your bowts more violent to the end,\\nAnd that he cals for drinke; Ile haue prepar'd him\\nA Challice for the nonce; whereon but sipping,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"If he by chance escape your venom'd stuck,\\nOur purpose may hold there; how sweet Queene.\\nEnter Queene.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Queen. One woe doth tread vpon anothers heele,\\nSo fast they'l follow: your Sister's drown'd Laertes\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Laer. Drown'd! O where?\\n  Queen. There is a Willow growes aslant a Brooke,\\nThat shewes his hore leaues in the glassie streame:\\nThere with fantasticke Garlands did she come,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Of Crow-flowers, Nettles, Daysies, and long Purples,\\nThat liberall Shepheards giue a grosser name;\\nBut our cold Maids doe Dead Mens Fingers call them:\\nThere on the pendant boughes, her Coronet weeds'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Clambring to hang; an enuious sliuer broke,\\nWhen downe the weedy Trophies, and her selfe,\\nFell in the weeping Brooke, her cloathes spred wide,\\nAnd Mermaid-like, a while they bore her vp,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Which time she chaunted snatches of old tunes,\\nAs one incapable of her owne distresse,\\nOr like a creature Natiue, and indued\\nVnto that Element: but long it could not be,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Till that her garments, heauy with her drinke,\\nPul'd the poore wretch from her melodious buy,\\nTo muddy death\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Laer. Alas then, is she drown'd?\\n  Queen. Drown'd, drown'd\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Laer. Too much of water hast thou poore Ophelia,\\nAnd therefore I forbid my teares: but yet\\nIt is our tricke, Nature her custome holds,\\nLet shame say what it will; when these are gone'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='The woman will be out: Adue my Lord,\\nI haue a speech of fire, that faine would blaze,\\nBut that this folly doubts it.\\nEnter.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Kin. Let's follow, Gertrude:\\nHow much I had to doe to calme his rage?\\nNow feare I this will giue it start againe;\\nTherefore let's follow.\\n\\nExeunt.\\n\\nEnter two Clownes.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Clown. Is she to bee buried in Christian buriall, that\\nwilfully seekes her owne saluation?\\n  Other. I tell thee she is, and therefore make her Graue'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='straight, the Crowner hath sate on her, and finds it Christian\\nburiall'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Clo. How can that be, vnlesse she drowned her selfe in\\nher owne defence?\\n  Other. Why 'tis found so\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Clo. It must be Se offendendo, it cannot bee else: for\\nheere lies the point; If I drowne my selfe wittingly, it argues\\nan Act: and an Act hath three branches. It is an'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Act to doe and to performe; argall she drown'd her selfe\\nwittingly\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Other. Nay but heare you Goodman Deluer'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Clown. Giue me leaue; heere lies the water; good:\\nheere stands the man; good: If the man goe to this water\\nand drowne himselfe; it is will he nill he, he goes;'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='marke you that? But if the water come to him & drowne\\nhim; hee drownes not himselfe. Argall, hee that is not\\nguilty of his owne death, shortens not his owne life'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Other. But is this law?\\n  Clo. I marry is't, Crowners Quest Law\\n\\n   Other. Will you ha the truth on't: if this had not\\nbeene a Gentlewoman, shee should haue beene buried\\nout of Christian Buriall\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Clo. Why there thou say'st. And the more pitty that\\ngreat folke should haue countenance in this world to\\ndrowne or hang themselues, more then their euen Christian.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Come, my Spade; there is no ancient Gentlemen,\\nbut Gardiners, Ditchers and Graue-makers; they hold vp\\nAdams Profession'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Other. Was he a Gentleman?\\n  Clo. He was the first that euer bore Armes\\n\\n   Other. Why he had none'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Clo. What, ar't a Heathen? how doth thou vnderstand\\nthe Scripture? the Scripture sayes Adam dig'd;\\ncould hee digge without Armes? Ile put another question\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='to thee; if thou answerest me not to the purpose, confesse\\nthy selfe-\\n  Other. Go too'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Clo. What is he that builds stronger then either the\\nMason, the Shipwright, or the Carpenter?\\n  Other. The Gallowes maker; for that Frame outliues a\\nthousand Tenants'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Clo. I like thy wit well in good faith, the Gallowes\\ndoes well; but how does it well? it does well to those\\nthat doe ill: now, thou dost ill to say the Gallowes is'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"built stronger then the Church: Argall, the Gallowes\\nmay doe well to thee. Too't againe, Come\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Other. Who builds stronger then a Mason, a Shipwright,\\nor a Carpenter?\\n  Clo. I, tell me that, and vnyoake\\n\\n   Other. Marry, now I can tell\\n\\n   Clo. Too't\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Clo. Too't\\n\\n   Other. Masse, I cannot tell.\\nEnter Hamlet and Horatio a farre off.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Clo. Cudgell thy braines no more about it; for your\\ndull Asse will not mend his pace with beating; and when\\nyou are ask't this question next, say a Graue-maker: the\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Houses that he makes, lasts till Doomesday: go, get thee\\nto Yaughan, fetch me a stoupe of Liquor.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Sings.\\n\\nIn youth when I did loue, did loue,\\nme thought it was very sweete:\\nTo contract O the time for a my behoue,\\nO me thought there was nothing meete'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Ha's this fellow no feeling of his businesse, that\\nhe sings at Graue-making?\\n  Hor. Custome hath made it in him a property of easinesse\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. 'Tis ee'n so; the hand of little Imployment hath\\nthe daintier sense\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Clowne sings. But Age with his stealing steps\\nhath caught me in his clutch:\\nAnd hath shipped me intill the Land,\\nas if I had neuer beene such'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. That Scull had a tongue in it, and could sing\\nonce: how the knaue iowles it to th' grownd, as if it\\nwere Caines Iaw-bone, that did the first murther: It\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"might be the Pate of a Polititian which this Asse o're Offices:\\none that could circumuent God, might it not?\\n  Hor. It might, my Lord\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Or of a Courtier, which could say, Good Morrow\\nsweet Lord: how dost thou, good Lord? this\\nmight be my Lord such a one, that prais'd my Lord such\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='a ones Horse, when he meant to begge it; might it not?\\n  Hor. I, my Lord'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Why ee'n so: and now my Lady Wormes,\\nChaplesse, and knockt about the Mazard with a Sextons\\nSpade; heere's fine Reuolution, if wee had the tricke to\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"see't. Did these bones cost no more the breeding, but\\nto play at Loggets with 'em? mine ake to thinke\\non't\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Clowne sings. A Pickhaxe and a Spade, a Spade,\\nfor and a shrowding-Sheete:\\nO a Pit of Clay for to be made,\\nfor such a Guest is meete'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. There's another: why might not that bee the\\nScull of a Lawyer? where be his Quiddits now? his\\nQuillets? his Cases? his Tenures, and his Tricks? why\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"doe's he suffer this rude knaue now to knocke him about\\nthe Sconce with a dirty Shouell, and will not tell him of\\nhis Action of Battery? hum. This fellow might be in's\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='time a great buyer of Land, with his Statutes, his Recognizances,\\nhis Fines, his double Vouchers, his Recoueries:\\nIs this the fine of his Fines, and the recouery of his Recoueries,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='to haue his fine Pate full of fine Dirt? will his\\nVouchers vouch him no more of his Purchases, and double\\nones too, then the length and breadth of a paire of'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Indentures? the very Conueyances of his Lands will\\nhardly lye in this Boxe; and must the Inheritor himselfe\\nhaue no more? ha?\\n  Hor. Not a iot more, my Lord'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. Is not Parchment made of Sheep-skinnes?\\n  Hor. I my Lord, and of Calue-skinnes too'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. They are Sheepe and Calues that seek out assurance\\nin that. I will speake to this fellow: whose Graue's\\nthis Sir?\\n  Clo. Mine Sir:\\nO a Pit of Clay for to be made,\\nfor such a Guest is meete\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. I thinke it be thine indeed: for thou liest in't\\n\\n   Clo. You lye out on't Sir, and therefore it is not yours:\\nfor my part, I doe not lye in't; and yet it is mine\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Thou dost lye in't, to be in't and say 'tis thine:\\n'tis for the dead, not for the quicke, therefore thou\\nlyest\\n\\n   Clo. 'Tis a quicke lye Sir, 'twill away againe from me\\nto you\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. What man dost thou digge it for?\\n  Clo. For no man Sir\\n\\n   Ham. What woman then?\\n  Clo. For none neither'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Who is to be buried in't?\\n  Clo. One that was a woman Sir; but rest her Soule,\\nshee's dead\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. How absolute the knaue is? wee must speake\\nby the Carde, or equiuocation will vndoe vs: by the\\nLord Horatio, these three yeares I haue taken note of it,'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='the Age is growne so picked, that the toe of the Pesant\\ncomes so neere the heeles of our Courtier, hee galls his\\nKibe. How long hast thou been a Graue-maker?'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Clo. Of all the dayes i'th' yeare, I came too't that day\\nthat our last King Hamlet o'recame Fortinbras\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. How long is that since?\\n  Clo. Cannot you tell that? euery foole can tell that:\\nIt was the very day, that young Hamlet was borne, hee\\nthat was mad, and sent into England'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. I marry, why was he sent into England?\\n  Clo. Why, because he was mad; hee shall recouer his\\nwits there; or if he do not, it's no great matter there\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Why?\\n  Clo. 'Twill not be seene in him, there the men are as\\nmad as he\\n\\n   Ham. How came he mad?\\n  Clo. Very strangely they say\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. How strangely?\\n  Clo. Faith e'ene with loosing his wits\\n\\n   Ham. Vpon what ground?\\n  Clo. Why heere in Denmarke: I haue bin sixeteene\\nheere, man and Boy thirty yeares\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. How long will a man lie i'th' earth ere he rot?\\n  Clo. Ifaith, if he be not rotten before he die (as we haue\\nmany pocky Coarses now adaies, that will scarce hold\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='the laying in) he will last you some eight yeare, or nine\\nyeare. A Tanner will last you nine yeare'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Why he, more then another?\\n  Clo. Why sir, his hide is so tan'd with his Trade, that\\nhe will keepe out water a great while. And your water,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='is a sore Decayer of your horson dead body. Heres a Scull\\nnow: this Scul, has laine in the earth three & twenty years'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. Whose was it?\\n  Clo. A whoreson mad Fellowes it was;\\nWhose doe you thinke it was?\\n  Ham. Nay, I know not'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Clo. A pestilence on him for a mad Rogue, a pour'd a\\nFlaggon of Renish on my head once. This same Scull\\nSir, this same Scull sir, was Yoricks Scull, the Kings Iester\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. This?\\n  Clo. E'ene that\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. Let me see. Alas poore Yorick, I knew him Horatio,\\na fellow of infinite Iest; of most excellent fancy, he\\nhath borne me on his backe a thousand times: And how'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='abhorred my Imagination is, my gorge rises at it. Heere\\nhung those lipps, that I haue kist I know not how oft.\\nWhere be your Iibes now? Your Gambals? Your'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Songs? Your flashes of Merriment that were wont to\\nset the Table on a Rore? No one now to mock your own\\nIeering? Quite chopfalne? Now get you to my Ladies'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Chamber, and tell her, let her paint an inch thicke, to this\\nfauour she must come. Make her laugh at that: prythee\\nHoratio tell me one thing'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Hor. What's that my Lord?\\n  Ham. Dost thou thinke Alexander lookt o'this fashion\\ni'th' earth?\\n  Hor. E'ene so\\n\\n   Ham. And smelt so? Puh\\n\\n   Hor. E'ene so, my Lord\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. To what base vses we may returne Horatio.\\nWhy may not Imagination trace the Noble dust of Alexander,\\ntill he find it stopping a bunghole'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Hor. 'Twere to consider: to curiously to consider so\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. No faith, not a iot. But to follow him thether\\nwith modestie enough, & likeliehood to lead it; as thus.\\nAlexander died: Alexander was buried: Alexander returneth'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"into dust; the dust is earth; of earth we make\\nLome, and why of that Lome (whereto he was conuerted)\\nmight they not stopp a Beere-barrell?\\nImperiall Caesar, dead and turn'd to clay,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Might stop a hole to keepe the winde away.\\nOh, that that earth, which kept the world in awe,\\nShould patch a Wall, t' expell the winters flaw.\\nBut soft, but soft, aside; heere comes the King.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Enter King, Queene, Laertes, and a Coffin, with Lords attendant.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"The Queene, the Courtiers. Who is that they follow,\\nAnd with such maimed rites? This doth betoken,\\nThe Coarse they follow, did with disperate hand,\\nFore do it owne life; 'twas some Estate.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Couch we a while, and mark'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Laer. What Cerimony else?\\n  Ham. That is Laertes, a very Noble youth: Marke'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Laer. What Cerimony else?\\n  Priest. Her Obsequies haue bin as farre inlarg'd.\\nAs we haue warrantie, her death was doubtfull,\\nAnd but that great Command, o're-swaies the order,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"She should in ground vnsanctified haue lodg'd,\\nTill the last Trumpet. For charitable praier,\\nShardes, Flints, and Peebles, should be throwne on her:\\nYet heere she is allowed her Virgin Rites,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Her Maiden strewments, and the bringing home\\nOf Bell and Buriall'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Laer. Must there no more be done ?\\n  Priest. No more be done:\\nWe should prophane the seruice of the dead,\\nTo sing sage Requiem, and such rest to her\\nAs to peace-parted Soules'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Laer. Lay her i'th' earth,\\nAnd from her faire and vnpolluted flesh,\\nMay Violets spring. I tell thee (churlish Priest)\\nA Ministring Angell shall my Sister be,\\nWhen thou liest howling?\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. What, the faire Ophelia?\\n  Queene. Sweets, to the sweet farewell.\\nI hop'd thou should'st haue bin my Hamlets wife:\\nI thought thy Bride-bed to haue deckt (sweet Maid)\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"And not t'haue strew'd thy Graue\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Laer. Oh terrible woer,\\nFall ten times trebble, on that cursed head\\nWhose wicked deed, thy most Ingenious sence\\nDepriu'd thee of. Hold off the earth a while,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Till I haue caught her once more in mine armes:'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Leaps in the graue.\\n\\nNow pile your dust, vpon the quicke, and dead,\\nTill of this flat a Mountaine you haue made,\\nTo o're top old Pelion, or the skyish head\\nOf blew Olympus\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. What is he, whose griefes\\nBeares such an Emphasis? whose phrase of Sorrow\\nConiure the wandring Starres, and makes them stand\\nLike wonder-wounded hearers? This is I,\\nHamlet the Dane'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Laer. The deuill take thy soule'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Thou prai'st not well,\\nI prythee take thy fingers from my throat;\\nSir though I am not Spleenatiue, and rash,\\nYet haue I something in me dangerous,\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Which let thy wisenesse feare. Away thy hand'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='King. Pluck them asunder\\n\\n   Qu. Hamlet, Hamlet\\n\\n   Gen. Good my Lord be quiet\\n\\n   Ham. Why I will fight with him vppon this Theme.\\nVntill my eielids will no longer wag'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Qu. Oh my Sonne, what Theame?\\n  Ham. I lou'd Ophelia; fortie thousand Brothers\\nCould not (with all there quantitie of Loue)\\nMake vp my summe. What wilt thou do for her?\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='King. Oh he is mad Laertes,\\n  Qu. For loue of God forbeare him'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Come show me what thou'lt doe.\\nWoo't weepe? Woo't fight? Woo't teare thy selfe?\\nWoo't drinke vp Esile, eate a Crocodile?\\nIle doo't. Dost thou come heere to whine;\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='To outface me with leaping in her Graue?\\nBe buried quicke with her, and so will I.\\nAnd if thou prate of Mountaines; let them throw\\nMillions of Akers on vs; till our ground'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Sindging his pate against the burning Zone,\\nMake Ossa like a wart. Nay, and thou'lt mouth,\\nIle rant as well as thou\"),\n",
       " ...]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=200,chunk_overlap=20)\n",
    "final_l=text_splitter.split_documents(fin_t)\n",
    "final_l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8e3a04b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"[The Tragedie of Hamlet by William Shakespeare 1599]\\n\\n\\nActus Primus. Scoena Prima.\\n\\nEnter Barnardo and Francisco two Centinels.\\n\\n  Barnardo. Who's there?\\n  Fran. Nay answer me: Stand & vnfold\\nyour selfe\\n\\n   Bar. Long liue the King\\n\\n   Fran. Barnardo?\\n  Bar. He\\n\\n   Fran. You come most carefully vpon your houre\\n\\n   Bar. 'Tis now strook twelue, get thee to bed Francisco\\n\\n   Fran. For this releefe much thankes: 'Tis bitter cold,\\nAnd I am sicke at heart\\n\\n   Barn. Haue you had quiet Guard?\\n  Fran. Not a Mouse stirring\\n\\n   Barn. Well, goodnight. If you do meet Horatio and\\nMarcellus, the Riuals of my Watch, bid them make hast.\\nEnter Horatio and Marcellus.\\n\\n  Fran. I thinke I heare them. Stand: who's there?\\n  Hor. Friends to this ground\\n\\n   Mar. And Leige-men to the Dane\\n\\n   Fran. Giue you good night\\n\\n   Mar. O farwel honest Soldier, who hath relieu'd you?\\n  Fra. Barnardo ha's my place: giue you goodnight.\\n\\nExit Fran.\\n\\n  Mar. Holla Barnardo\\n\\n   Bar. Say, what is Horatio there?\\n  Hor. A peece of him\\n\\n   Bar. Welcome Horatio, welcome good Marcellus\\n\\n   Mar. What, ha's this thing appear'd againe to night\\n\\n   Bar. I haue seene nothing\\n\\n   Mar. Horatio saies, 'tis but our Fantasie,\\nAnd will not let beleefe take hold of him\\nTouching this dreaded sight, twice seene of vs,\\nTherefore I haue intreated him along\\nWith vs, to watch the minutes of this Night,\\nThat if againe this Apparition come,\\nHe may approue our eyes, and speake to it\\n\\n   Hor. Tush, tush, 'twill not appeare\\n\\n   Bar. Sit downe a-while,\\nAnd let vs once againe assaile your eares,\\nThat are so fortified against our Story,\\nWhat we two Nights haue seene\\n\\n   Hor. Well, sit we downe,\\nAnd let vs heare Barnardo speake of this\\n\\n   Barn. Last night of all,\\nWhen yond same Starre that's Westward from the Pole\\nHad made his course t' illume that part of Heauen\\nWhere now it burnes, Marcellus and my selfe,\\nThe Bell then beating one\\n\\n   Mar. Peace, breake thee of:\\nEnter the Ghost.\\n\\nLooke where it comes againe\\n\\n   Barn. In the same figure, like the King that's dead\\n\\n   Mar. Thou art a Scholler; speake to it Horatio\\n\\n   Barn. Lookes it not like the King? Marke it Horatio\\n\\n   Hora. Most like: It harrowes me with fear & wonder\\n  Barn. It would be spoke too\\n\\n   Mar. Question it Horatio\\n\\n   Hor. What art thou that vsurp'st this time of night,\\nTogether with that Faire and Warlike forme\\nIn which the Maiesty of buried Denmarke\\nDid sometimes march: By Heauen I charge thee speake\\n\\n   Mar. It is offended\\n\\n   Barn. See, it stalkes away\\n\\n   Hor. Stay: speake; speake: I Charge thee, speake.\\n\\nExit the Ghost.\\n\\n  Mar. 'Tis gone, and will not answer\\n\\n   Barn. How now Horatio? You tremble & look pale:\\nIs not this something more then Fantasie?\\nWhat thinke you on't?\\n  Hor. Before my God, I might not this beleeue\\nWithout the sensible and true auouch\\nOf mine owne eyes\\n\\n   Mar. Is it not like the King?\\n  Hor. As thou art to thy selfe,\\nSuch was the very Armour he had on,\\nWhen th' Ambitious Norwey combatted:\\nSo frown'd he once, when in an angry parle\\nHe smot the sledded Pollax on the Ice.\\n'Tis strange\\n\\n   Mar. Thus twice before, and iust at this dead houre,\\nWith Martiall stalke, hath he gone by our Watch\\n\\n   Hor. In what particular thought to work, I know not:\\nBut in the grosse and scope of my Opinion,\\nThis boades some strange erruption to our State\\n\\n   Mar. Good now sit downe, & tell me he that knowes\\nWhy this same strict and most obseruant Watch,\\nSo nightly toyles the subiect of the Land,\\nAnd why such dayly Cast of Brazon Cannon\\nAnd Forraigne Mart for Implements of warre:\\nWhy such impresse of Ship-wrights, whose sore Taske\\nDo's not diuide the Sunday from the weeke,\\nWhat might be toward, that this sweaty hast\\nDoth make the Night ioynt-Labourer with the day:\\nWho is't that can informe me?\\n  Hor. That can I,\\nAt least the whisper goes so: Our last King,\\nWhose Image euen but now appear'd to vs,\\nWas (as you know) by Fortinbras of Norway,\\n(Thereto prick'd on by a most emulate Pride)\\nDar'd to the Combate. In which, our Valiant Hamlet,\\n(For so this side of our knowne world esteem'd him)\\nDid slay this Fortinbras: who by a Seal'd Compact,\\nWell ratified by Law, and Heraldrie,\\nDid forfeite (with his life) all those his Lands\\nWhich he stood seiz'd on, to the Conqueror:\\nAgainst the which, a Moity competent\\nWas gaged by our King: which had return'd\\nTo the Inheritance of Fortinbras,\\nHad he bin Vanquisher, as by the same Cou'nant\\nAnd carriage of the Article designe,\\nHis fell to Hamlet. Now sir, young Fortinbras,\\nOf vnimproued Mettle, hot and full,\\nHath in the skirts of Norway, heere and there,\\nShark'd vp a List of Landlesse Resolutes,\\nFor Foode and Diet, to some Enterprize\\nThat hath a stomacke in't: which is no other\\n(And it doth well appeare vnto our State)\\nBut to recouer of vs by strong hand\\nAnd termes Compulsatiue, those foresaid Lands\\nSo by his Father lost: and this (I take it)\\nIs the maine Motiue of our Preparations,\\nThe Sourse of this our Watch, and the cheefe head\\nOf this post-hast, and Romage in the Land.\\nEnter Ghost againe.\\n\\nBut soft, behold: Loe, where it comes againe:\\nIle crosse it, though it blast me. Stay Illusion:\\nIf thou hast any sound, or vse of Voyce,\\nSpeake to me. If there be any good thing to be done,\\nThat may to thee do ease, and grace to me; speak to me.\\nIf thou art priuy to thy Countries Fate\\n(Which happily foreknowing may auoyd) Oh speake.\\nOr, if thou hast vp-hoorded in thy life\\nExtorted Treasure in the wombe of Earth,\\n(For which, they say, you Spirits oft walke in death)\\nSpeake of it. Stay, and speake. Stop it Marcellus\\n\\n   Mar. Shall I strike at it with my Partizan?\\n  Hor. Do, if it will not stand\\n\\n   Barn. 'Tis heere\\n\\n   Hor. 'Tis heere\\n\\n   Mar. 'Tis gone.\\n\\nExit Ghost.\\n\\nWe do it wrong, being so Maiesticall\\nTo offer it the shew of Violence,\\nFor it is as the Ayre, invulnerable,\\nAnd our vaine blowes, malicious Mockery\\n\\n   Barn. It was about to speake, when the Cocke crew\\n\\n   Hor. And then it started, like a guilty thing\\nVpon a fearfull Summons. I haue heard,\\nThe Cocke that is the Trumpet to the day,\\nDoth with his lofty and shrill-sounding Throate\\nAwake the God of Day: and at his warning,\\nWhether in Sea, or Fire, in Earth, or Ayre,\\nTh' extrauagant, and erring Spirit, hyes\\nTo his Confine. And of the truth heerein,\\nThis present Obiect made probation\\n\\n   Mar. It faded on the crowing of the Cocke.\\nSome sayes, that euer 'gainst that Season comes\\nWherein our Sauiours Birch is celebrated,\\nThe Bird of Dawning singeth all night long:\\nAnd then (they say) no Spirit can walke abroad,\\nThe nights are wholsome, then no Planets strike,\\nNo Faiery talkes, nor Witch hath power to Charme:\\nSo hallow'd, and so gracious is the time\\n\\n   Hor. So haue I heard, and do in part beleeue it.\\nBut looke, the Morne in Russet mantle clad,\\nWalkes o're the dew of yon high Easterne Hill,\\nBreake we our Watch vp, and by my aduice\\nLet vs impart what we haue seene to night\\nVnto yong Hamlet. For vpon my life,\\nThis Spirit dumbe to vs, will speake to him:\\nDo you consent we shall acquaint him with it,\\nAs needfull in our Loues, fitting our Duty?\\n  Mar. Let do't I pray, and I this morning know\\nWhere we shall finde him most conueniently.\\n\\nExeunt.\\n\\nScena Secunda.\\n\\nEnter Claudius King of Denmarke, Gertrude the Queene, Hamlet,\\nPolonius,\\nLaertes, and his Sister Ophelia, Lords Attendant.\\n\\n  King. Though yet of Hamlet our deere Brothers death\\nThe memory be greene: and that it vs befitted\\nTo beare our hearts in greefe, and our whole Kingdome\\nTo be contracted in one brow of woe:\\nYet so farre hath Discretion fought with Nature,\\nThat we with wisest sorrow thinke on him,\\nTogether with remembrance of our selues.\\nTherefore our sometimes Sister, now our Queene,\\nTh' imperiall Ioyntresse of this warlike State,\\nHaue we, as 'twere, with a defeated ioy,\\nWith one Auspicious, and one Dropping eye,\\nWith mirth in Funerall, and with Dirge in Marriage,\\nIn equall Scale weighing Delight and Dole\\nTaken to Wife; nor haue we heerein barr'd\\nYour better Wisedomes, which haue freely gone\\nWith this affaire along, for all our Thankes.\\nNow followes, that you know young Fortinbras,\\nHolding a weake supposall of our worth;\\nOr thinking by our late deere Brothers death,\\nOur State to be disioynt, and out of Frame,\\nColleagued with the dreame of his Aduantage;\\nHe hath not fayl'd to pester vs with Message,\\nImporting the surrender of those Lands\\nLost by his Father: with all Bonds of Law\\nTo our most valiant Brother. So much for him.\\nEnter Voltemand and Cornelius.\\n\\nNow for our selfe, and for this time of meeting\\nThus much the businesse is. We haue heere writ\\nTo Norway, Vncle of young Fortinbras,\\nWho Impotent and Bedrid, scarsely heares\\nOf this his Nephewes purpose, to suppresse\\nHis further gate heerein. In that the Leuies,\\nThe Lists, and full proportions are all made\\nOut of his subiect: and we heere dispatch\\nYou good Cornelius, and you Voltemand,\\nFor bearing of this greeting to old Norway,\\nGiuing to you no further personall power\\nTo businesse with the King, more then the scope\\nOf these dilated Articles allow:\\nFarewell, and let your hast commend your duty\\n\\n   Volt. In that, and all things, will we shew our duty\\n\\n   King. We doubt it nothing, heartily farewell.\\n\\nExit Voltemand and Cornelius.\\n\\nAnd now Laertes, what's the newes with you?\\nYou told vs of some suite. What is't Laertes?\\nYou cannot speake of Reason to the Dane,\\nAnd loose your voyce. What would'st thou beg Laertes,\\nThat shall not be my Offer, not thy Asking?\\nThe Head is not more Natiue to the Heart,\\nThe Hand more instrumentall to the Mouth,\\nThen is the Throne of Denmarke to thy Father.\\nWhat would'st thou haue Laertes?\\n  Laer. Dread my Lord,\\nYour leaue and fauour to returne to France,\\nFrom whence, though willingly I came to Denmarke\\nTo shew my duty in your Coronation,\\nYet now I must confesse, that duty done,\\nMy thoughts and wishes bend againe towards France,\\nAnd bow them to your gracious leaue and pardon\\n\\n   King. Haue you your Fathers leaue?\\nWhat sayes Pollonius?\\n  Pol. He hath my Lord:\\nI do beseech you giue him leaue to go\\n\\n   King. Take thy faire houre Laertes, time be thine,\\nAnd thy best graces spend it at thy will:\\nBut now my Cosin Hamlet, and my Sonne?\\n  Ham. A little more then kin, and lesse then kinde\\n\\n   King. How is it that the Clouds still hang on you?\\n  Ham. Not so my Lord, I am too much i'th' Sun\\n\\n   Queen. Good Hamlet cast thy nightly colour off,\\nAnd let thine eye looke like a Friend on Denmarke.\\nDo not for euer with thy veyled lids\\nSeeke for thy Noble Father in the dust;\\nThou know'st 'tis common, all that liues must dye,\\nPassing through Nature, to Eternity\\n\\n   Ham. I Madam, it is common\\n\\n   Queen. If it be;\\nWhy seemes it so particular with thee\\n\\n   Ham. Seemes Madam? Nay, it is: I know not Seemes:\\n'Tis not alone my Inky Cloake (good Mother)\\nNor Customary suites of solemne Blacke,\\nNor windy suspiration of forc'd breath,\\nNo, nor the fruitfull Riuer in the Eye,\\nNor the deiected hauiour of the Visage,\\nTogether with all Formes, Moods, shewes of Griefe,\\nThat can denote me truly. These indeed Seeme,\\nFor they are actions that a man might play:\\nBut I haue that Within, which passeth show;\\nThese, but the Trappings, and the Suites of woe\\n\\n   King. 'Tis sweet and commendable\\nIn your Nature Hamlet,\\nTo giue these mourning duties to your Father:\\nBut you must know, your Father lost a Father,\\nThat Father lost, lost his, and the Suruiuer bound\\nIn filiall Obligation, for some terme\\nTo do obsequious Sorrow. But to perseuer\\nIn obstinate Condolement, is a course\\nOf impious stubbornnesse. 'Tis vnmanly greefe,\\nIt shewes a will most incorrect to Heauen,\\nA Heart vnfortified, a Minde impatient,\\nAn Vnderstanding simple, and vnschool'd:\\nFor, what we know must be, and is as common\\nAs any the most vulgar thing to sence,\\nWhy should we in our peeuish Opposition\\nTake it to heart? Fye, 'tis a fault to Heauen,\\nA fault against the Dead, a fault to Nature,\\nTo Reason most absurd, whose common Theame\\nIs death of Fathers, and who still hath cried,\\nFrom the first Coarse, till he that dyed to day,\\nThis must be so. We pray you throw to earth\\nThis vnpreuayling woe, and thinke of vs\\nAs of a Father; For let the world take note,\\nYou are the most immediate to our Throne,\\nAnd with no lesse Nobility of Loue,\\nThen that which deerest Father beares his Sonne,\\nDo I impart towards you. For your intent\\nIn going backe to Schoole in Wittenberg,\\nIt is most retrograde to our desire:\\nAnd we beseech you, bend you to remaine\\nHeere in the cheere and comfort of our eye,\\nOur cheefest Courtier Cosin, and our Sonne\\n\\n   Qu. Let not thy Mother lose her Prayers Hamlet:\\nI prythee stay with vs, go not to Wittenberg\\n\\n   Ham. I shall in all my best\\nObey you Madam\\n\\n   King. Why 'tis a louing, and a faire Reply,\\nBe as our selfe in Denmarke. Madam come,\\nThis gentle and vnforc'd accord of Hamlet\\nSits smiling to my heart; in grace whereof,\\nNo iocond health that Denmarke drinkes to day,\\nBut the great Cannon to the Clowds shall tell,\\nAnd the Kings Rouce, the Heauens shall bruite againe,\\nRespeaking earthly Thunder. Come away.\\n\\nExeunt.\\n\\nManet Hamlet.\\n\\n  Ham. Oh that this too too solid Flesh, would melt,\\nThaw, and resolue it selfe into a Dew:\\nOr that the Euerlasting had not fixt\\nHis Cannon 'gainst Selfe-slaughter. O God, O God!\\nHow weary, stale, flat, and vnprofitable\\nSeemes to me all the vses of this world?\\nFie on't? Oh fie, fie, 'tis an vnweeded Garden\\nThat growes to Seed: Things rank, and grosse in Nature\\nPossesse it meerely. That it should come to this:\\nBut two months dead: Nay, not so much; not two,\\nSo excellent a King, that was to this\\nHiperion to a Satyre: so louing to my Mother,\\nThat he might not beteene the windes of heauen\\nVisit her face too roughly. Heauen and Earth\\nMust I remember: why she would hang on him,\\nAs if encrease of Appetite had growne\\nBy what is fed on; and yet within a month?\\nLet me not thinke on't: Frailty, thy name is woman.\\nA little Month, or ere those shooes were old,\\nWith which she followed my poore Fathers body\\nLike Niobe, all teares. Why she, euen she.\\n(O Heauen! A beast that wants discourse of Reason\\nWould haue mourn'd longer) married with mine Vnkle,\\nMy Fathers Brother: but no more like my Father,\\nThen I to Hercules. Within a Moneth?\\nEre yet the salt of most vnrighteous Teares\\nHad left the flushing of her gauled eyes,\\nShe married. O most wicked speed, to post\\nWith such dexterity to Incestuous sheets:\\nIt is not, nor it cannot come to good.\\nBut breake my heart, for I must hold my tongue.\\nEnter Horatio, Barnardo, and Marcellus.\\n\\n  Hor. Haile to your Lordship\\n\\n   Ham. I am glad to see you well:\\nHoratio, or I do forget my selfe\\n\\n   Hor. The same my Lord,\\nAnd your poore Seruant euer\\n\\n   Ham. Sir my good friend,\\nIle change that name with you:\\nAnd what make you from Wittenberg Horatio?\\nMarcellus\\n\\n   Mar. My good Lord\\n\\n   Ham. I am very glad to see you: good euen Sir.\\nBut what in faith make you from Wittemberge?\\n  Hor. A truant disposition, good my Lord\\n\\n   Ham. I would not haue your Enemy say so;\\nNor shall you doe mine eare that violence,\\nTo make it truster of your owne report\\nAgainst your selfe. I know you are no Truant:\\nBut what is your affaire in Elsenour?\\nWee'l teach you to drinke deepe, ere you depart\\n\\n   Hor. My Lord, I came to see your Fathers Funerall\\n\\n   Ham. I pray thee doe not mock me (fellow Student)\\nI thinke it was to see my Mothers Wedding\\n\\n   Hor. Indeed my Lord, it followed hard vpon\\n\\n   Ham. Thrift thrift Horatio: the Funerall Bakt-meats\\nDid coldly furnish forth the Marriage Tables;\\nWould I had met my dearest foe in heauen,\\nEre I had euer seene that day Horatio.\\nMy father, me thinkes I see my father\\n\\n   Hor. Oh where my Lord?\\n  Ham. In my minds eye (Horatio)\\n  Hor. I saw him once; he was a goodly King\\n\\n   Ham. He was a man, take him for all in all:\\nI shall not look vpon his like againe\\n\\n   Hor. My Lord, I thinke I saw him yesternight\\n\\n   Ham. Saw? Who?\\n  Hor. My Lord, the King your Father\\n\\n   Ham. The King my Father?\\n  Hor. Season your admiration for a while\\nWith an attent eare; till I may deliuer\\nVpon the witnesse of these Gentlemen,\\nThis maruell to you\\n\\n   Ham. For Heauens loue let me heare\\n\\n   Hor. Two nights together, had these Gentlemen\\n(Marcellus and Barnardo) on their Watch\\nIn the dead wast and middle of the night\\nBeene thus encountred. A figure like your Father,\\nArm'd at all points exactly, Cap a Pe,\\nAppeares before them, and with sollemne march\\nGoes slow and stately: By them thrice he walkt,\\nBy their opprest and feare-surprized eyes,\\nWithin his Truncheons length; whilst they bestil'd\\nAlmost to Ielly with the Act of feare,\\nStand dumbe and speake not to him. This to me\\nIn dreadfull secrecie impart they did,\\nAnd I with them the third Night kept the Watch,\\nWhereas they had deliuer'd both in time,\\nForme of the thing; each word made true and good,\\nThe Apparition comes. I knew your Father:\\nThese hands are not more like\\n\\n   Ham. But where was this?\\n  Mar. My Lord vpon the platforme where we watcht\\n\\n   Ham. Did you not speake to it?\\n  Hor. My Lord, I did;\\nBut answere made it none: yet once me thought\\nIt lifted vp it head, and did addresse\\nIt selfe to motion, like as it would speake:\\nBut euen then, the Morning Cocke crew lowd;\\nAnd at the sound it shrunke in hast away,\\nAnd vanisht from our sight\\n\\n   Ham. Tis very strange\\n\\n   Hor. As I doe liue my honourd Lord 'tis true;\\nAnd we did thinke it writ downe in our duty\\nTo let you know of it\\n\\n   Ham. Indeed, indeed Sirs; but this troubles me.\\nHold you the watch to Night?\\n  Both. We doe my Lord\\n\\n   Ham. Arm'd, say you?\\n  Both. Arm'd, my Lord\\n\\n   Ham. From top to toe?\\n  Both. My Lord, from head to foote\\n\\n   Ham. Then saw you not his face?\\n  Hor. O yes, my Lord, he wore his Beauer vp\\n\\n   Ham. What, lookt he frowningly?\\n  Hor. A countenance more in sorrow then in anger\\n\\n   Ham. Pale, or red?\\n  Hor. Nay very pale\\n\\n   Ham. And fixt his eyes vpon you?\\n  Hor. Most constantly\\n\\n   Ham. I would I had beene there\\n\\n   Hor. It would haue much amaz'd you\\n\\n   Ham. Very like, very like: staid it long?\\n  Hor. While one with moderate hast might tell a hundred\\n\\n   All. Longer, longer\\n\\n   Hor. Not when I saw't\\n\\n   Ham. His Beard was grisly? no\\n\\n   Hor. It was, as I haue seene it in his life,\\nA Sable Siluer'd\\n\\n   Ham. Ile watch to Night; perchance 'twill wake againe\\n\\n   Hor. I warrant you it will\\n\\n   Ham. If it assume my noble Fathers person,\\nIle speake to it, though Hell it selfe should gape\\nAnd bid me hold my peace. I pray you all,\\nIf you haue hitherto conceald this sight;\\nLet it bee treble in your silence still:\\nAnd whatsoeuer els shall hap to night,\\nGiue it an vnderstanding but no tongue;\\nI will requite your loues; so fare ye well:\\nVpon the Platforme twixt eleuen and twelue,\\nIle visit you\\n\\n   All. Our duty to your Honour.\\n\\nExeunt\\n\\n   Ham. Your loue, as mine to you: farewell.\\nMy Fathers Spirit in Armes? All is not well:\\nI doubt some foule play: would the Night were come;\\nTill then sit still my soule; foule deeds will rise,\\nThough all the earth orewhelm them to mens eies.\\nEnter.\\n\\n\\nScena Tertia\\n\\n\\nEnter Laertes and Ophelia.\\n\\n  Laer. My necessaries are imbark't; Farewell:\\nAnd Sister, as the Winds giue Benefit,\\nAnd Conuoy is assistant; doe not sleepe,\\nBut let me heare from you\\n\\n   Ophel. Doe you doubt that?\\n  Laer. For Hamlet, and the trifling of his fauours,\\nHold it a fashion and a toy in Bloude;\\nA Violet in the youth of Primy Nature;\\nFroward, not permanent; sweet not lasting\\nThe suppliance of a minute? No more\\n\\n   Ophel. No more but so\\n\\n   Laer. Thinke it no more:\\nFor nature cressant does not grow alone,\\nIn thewes and Bulke: but as his Temple waxes,\\nThe inward seruice of the Minde and Soule\\nGrowes wide withall. Perhaps he loues you now,\\nAnd now no soyle nor cautell doth besmerch\\nThe vertue of his feare: but you must feare\\nHis greatnesse weigh'd, his will is not his owne;\\nFor hee himselfe is subiect to his Birth:\\nHee may not, as vnuallued persons doe,\\nCarue for himselfe; for, on his choyce depends\\nThe sanctity and health of the whole State.\\nAnd therefore must his choyce be circumscrib'd\\nVnto the voyce and yeelding of that Body,\\nWhereof he is the Head. Then if he sayes he loues you,\\nIt fits your wisedome so farre to beleeue it;\\nAs he in his peculiar Sect and force\\nMay giue his saying deed: which is no further,\\nThen the maine voyce of Denmarke goes withall.\\nThen weight what losse your Honour may sustaine,\\nIf with too credent eare you list his Songs;\\nOr lose your Heart; or your chast Treasure open\\nTo his vnmastred importunity.\\nFeare it Ophelia, feare it my deare Sister,\\nAnd keepe within the reare of your Affection;\\nOut of the shot and danger of Desire.\\nThe chariest Maid is Prodigall enough,\\nIf she vnmaske her beauty to the Moone:\\nVertue it selfe scapes not calumnious stroakes,\\nThe Canker Galls, the Infants of the Spring\\nToo oft before the buttons be disclos'd,\\nAnd in the Morne and liquid dew of Youth,\\nContagious blastments are most imminent.\\nBe wary then, best safety lies in feare;\\nYouth to it selfe rebels, though none else neere\\n\\n   Ophe. I shall th' effect of this good Lesson keepe,\\nAs watchmen to my heart: but good my Brother\\nDoe not as some vngracious Pastors doe,\\nShew me the steepe and thorny way to Heauen;\\nWhilst like a puft and recklesse Libertine\\nHimselfe, the Primrose path of dalliance treads,\\nAnd reaks not his owne reade\\n\\n   Laer. Oh, feare me not.\\nEnter Polonius.\\n\\nI stay too long; but here my Father comes:\\nA double blessing is a double grace;\\nOccasion smiles vpon a second leaue\\n\\n   Polon. Yet heere Laertes? Aboord, aboord for shame,\\nThe winde sits in the shoulder of your saile,\\nAnd you are staid for there: my blessing with you;\\nAnd these few Precepts in thy memory,\\nSee thou Character. Giue thy thoughts no tongue,\\nNor any vnproportion'd thoughts his Act:\\nBe thou familiar; but by no meanes vulgar:\\nThe friends thou hast, and their adoption tride,\\nGrapple them to thy Soule, with hoopes of Steele:\\nBut doe not dull thy palme, with entertainment\\nOf each vnhatch't, vnfledg'd Comrade. Beware\\nOf entrance to a quarrell: but being in\\nBear't that th' opposed may beware of thee.\\nGiue euery man thine eare; but few thy voyce:\\nTake each mans censure; but reserue thy iudgement:\\nCostly thy habit as thy purse can buy;\\nBut not exprest in fancie; rich, not gawdie:\\nFor the Apparell oft proclaimes the man.\\nAnd they in France of the best ranck and station,\\nAre of a most select and generous cheff in that.\\nNeither a borrower, nor a lender be;\\nFor lone oft loses both it selfe and friend:\\nAnd borrowing duls the edge of Husbandry.\\nThis aboue all; to thine owne selfe be true:\\nAnd it must follow, as the Night the Day,\\nThou canst not then be false to any man.\\nFarewell: my Blessing season this in thee\\n\\n   Laer. Most humbly doe I take my leaue, my Lord\\n\\n   Polon. The time inuites you, goe, your seruants tend\\n\\n   Laer. Farewell Ophelia, and remember well\\nWhat I haue said to you\\n\\n   Ophe. Tis in my memory lockt,\\nAnd you your selfe shall keepe the key of it\\n\\n   Laer. Farewell.\\n\\nExit Laer.\\n\\n  Polon. What ist Ophelia he hath said to you?\\n  Ophe. So please you, somthing touching the L[ord]. Hamlet\\n\\n   Polon. Marry, well bethought:\\nTis told me he hath very oft of late\\nGiuen priuate time to you; and you your selfe\\nHaue of your audience beene most free and bounteous.\\nIf it be so, as so tis put on me;\\nAnd that in way of caution: I must tell you,\\nYou doe not vnderstand your selfe so cleerely,\\nAs it behoues my Daughter, and your Honour.\\nWhat is betweene you, giue me vp the truth?\\n  Ophe. He hath my Lord of late, made many tenders\\nOf his affection to me\\n\\n   Polon. Affection, puh. You speake like a greene Girle,\\nVnsifted in such perillous Circumstance.\\nDoe you beleeue his tenders, as you call them?\\n  Ophe. I do not know, my Lord, what I should thinke\\n\\n   Polon. Marry Ile teach you; thinke your selfe a Baby,\\nThat you haue tane his tenders for true pay,\\nWhich are not starling. Tender your selfe more dearly;\\nOr not to crack the winde of the poore Phrase,\\nRoaming it thus, you'l tender me a foole\\n\\n   Ophe. My Lord, he hath importun'd me with loue,\\nIn honourable fashion\\n\\n   Polon. I, fashion you may call it, go too, go too\\n\\n   Ophe. And hath giuen countenance to his speech,\\nMy Lord, with all the vowes of Heauen\\n\\n   Polon. I, Springes to catch Woodcocks. I doe know\\nWhen the Bloud burnes, how Prodigall the Soule\\nGiues the tongue vowes: these blazes, Daughter,\\nGiuing more light then heate; extinct in both,\\nEuen in their promise, as it is a making;\\nYou must not take for fire. For this time Daughter,\\nBe somewhat scanter of your Maiden presence;\\nSet your entreatments at a higher rate,\\nThen a command to parley. For Lord Hamlet,\\nBeleeue so much in him, that he is young,\\nAnd with a larger tether may he walke,\\nThen may be giuen you. In few, Ophelia,\\nDoe not beleeue his vowes; for they are Broakers,\\nNot of the eye, which their Inuestments show:\\nBut meere implorators of vnholy Sutes,\\nBreathing like sanctified and pious bonds,\\nThe better to beguile. This is for all:\\nI would not, in plaine tearmes, from this time forth,\\nHaue you so slander any moment leisure,\\nAs to giue words or talke with the Lord Hamlet:\\nLooke too't, I charge you; come your wayes\\n\\n   Ophe. I shall obey my Lord.\\n\\nExeunt.\\n\\nEnter Hamlet, Horatio, Marcellus.\\n\\n  Ham. The Ayre bites shrewdly: is it very cold?\\n  Hor. It is a nipping and an eager ayre\\n\\n   Ham. What hower now?\\n  Hor. I thinke it lacks of twelue\\n\\n   Mar. No, it is strooke\\n\\n   Hor. Indeed I heard it not: then it drawes neere the season,\\nWherein the Spirit held his wont to walke.\\nWhat does this meane my Lord?\\n  Ham. The King doth wake to night, and takes his rouse,\\nKeepes wassels and the swaggering vpspring reeles,\\nAnd as he dreines his draughts of Renish downe,\\nThe kettle Drum and Trumpet thus bray out\\nThe triumph of his Pledge\\n\\n   Horat. Is it a custome?\\n  Ham. I marry ist;\\nAnd to my mind, though I am natiue heere,\\nAnd to the manner borne: It is a Custome\\nMore honour'd in the breach, then the obseruance.\\nEnter Ghost.\\n\\n  Hor. Looke my Lord, it comes\\n\\n   Ham. Angels and Ministers of Grace defend vs:\\nBe thou a Spirit of health, or Goblin damn'd,\\nBring with thee ayres from Heauen, or blasts from Hell,\\nBe thy euents wicked or charitable,\\nThou com'st in such a questionable shape\\nThat I will speake to thee. Ile call thee Hamlet,\\nKing, Father, Royall Dane: Oh, oh, answer me,\\nLet me not burst in Ignorance; but tell\\nWhy thy Canoniz'd bones Hearsed in death,\\nHaue burst their cerments, why the Sepulcher\\nWherein we saw thee quietly enurn'd,\\nHath op'd his ponderous and Marble iawes,\\nTo cast thee vp againe? What may this meane?\\nThat thou dead Coarse againe in compleat steele,\\nReuisits thus the glimpses of the Moone,\\nMaking Night hidious? And we fooles of Nature,\\nSo horridly to shake our disposition,\\nWith thoughts beyond thee; reaches of our Soules,\\nSay, why is this? wherefore? what should we doe?\\n\\nGhost beckens Hamlet.\\n\\n  Hor. It beckons you to goe away with it,\\nAs if it some impartment did desire\\nTo you alone\\n\\n   Mar. Looke with what courteous action\\nIt wafts you to a more remoued ground:\\nBut doe not goe with it\\n\\n   Hor. No, by no meanes\\n\\n   Ham. It will not speake: then will I follow it\\n\\n   Hor. Doe not my Lord\\n\\n   Ham. Why, what should be the feare?\\nI doe not set my life at a pins fee;\\nAnd for my Soule, what can it doe to that?\\nBeing a thing immortall as it selfe:\\nIt waues me forth againe; Ile follow it\\n\\n   Hor. What if it tempt you toward the Floud my Lord?\\nOr to the dreadfull Sonnet of the Cliffe,\\nThat beetles o're his base into the Sea,\\nAnd there assumes some other horrible forme,\\nWhich might depriue your Soueraignty of Reason,\\nAnd draw you into madnesse thinke of it?\\n  Ham. It wafts me still: goe on, Ile follow thee\\n\\n   Mar. You shall not goe my Lord\\n\\n   Ham. Hold off your hand\\n\\n   Hor. Be rul'd, you shall not goe\\n\\n   Ham. My fate cries out,\\nAnd makes each petty Artire in this body,\\nAs hardy as the Nemian Lions nerue:\\nStill am I cal'd? Vnhand me Gentlemen:\\nBy Heau'n, Ile make a Ghost of him that lets me:\\nI say away, goe on, Ile follow thee.\\n\\nExeunt. Ghost & Hamlet.\\n\\n  Hor. He waxes desperate with imagination\\n\\n   Mar. Let's follow; 'tis not fit thus to obey him\\n\\n   Hor. Haue after, to what issue will this come?\\n  Mar. Something is rotten in the State of Denmarke\\n\\n   Hor. Heauen will direct it\\n\\n   Mar. Nay, let's follow him.\\n\\nExeunt.\\n\\nEnter Ghost and Hamlet.\\n\\n  Ham. Where wilt thou lead me? speak; Ile go no further\\n\\n   Gho. Marke me\\n\\n   Ham. I will\\n\\n   Gho. My hower is almost come,\\nWhen I to sulphurous and tormenting Flames\\nMust render vp my selfe\\n\\n   Ham. Alas poore Ghost\\n\\n   Gho. Pitty me not, but lend thy serious hearing\\nTo what I shall vnfold\\n\\n   Ham. Speake, I am bound to heare\\n\\n   Gho. So art thou to reuenge, when thou shalt heare\\n\\n   Ham. What?\\n  Gho. I am thy Fathers Spirit,\\nDoom'd for a certaine terme to walke the night;\\nAnd for the day confin'd to fast in Fiers,\\nTill the foule crimes done in my dayes of Nature\\nAre burnt and purg'd away? But that I am forbid\\nTo tell the secrets of my Prison-House;\\nI could a Tale vnfold, whose lightest word\\nWould harrow vp thy soule, freeze thy young blood,\\nMake thy two eyes like Starres, start from their Spheres,\\nThy knotty and combined lockes to part,\\nAnd each particular haire to stand an end,\\nLike Quilles vpon the fretfull Porpentine:\\nBut this eternall blason must not be\\nTo eares of flesh and bloud; list Hamlet, oh list,\\nIf thou didst euer thy deare Father loue\\n\\n   Ham. Oh Heauen!\\n  Gho. Reuenge his foule and most vnnaturall Murther\\n\\n   Ham. Murther?\\n  Ghost. Murther most foule, as in the best it is;\\nBut this most foule, strange, and vnnaturall\\n\\n   Ham. Hast, hast me to know it,\\nThat with wings as swift\\nAs meditation, or the thoughts of Loue,\\nMay sweepe to my Reuenge\\n\\n   Ghost. I finde thee apt,\\nAnd duller should'st thou be then the fat weede\\nThat rots it selfe in ease, on Lethe Wharfe,\\nWould'st thou not stirre in this. Now Hamlet heare:\\nIt's giuen out, that sleeping in mine Orchard,\\nA Serpent stung me: so the whole eare of Denmarke,\\nIs by a forged processe of my death\\nRankly abus'd: But know thou Noble youth,\\nThe Serpent that did sting thy Fathers life,\\nNow weares his Crowne\\n\\n   Ham. O my Propheticke soule: mine Vncle?\\n  Ghost. I that incestuous, that adulterate Beast\\nWith witchcraft of his wits, hath Traitorous guifts.\\nOh wicked Wit, and Gifts, that haue the power\\nSo to seduce? Won to this shamefull Lust\\nThe will of my most seeming vertuous Queene:\\nOh Hamlet, what a falling off was there,\\nFrom me, whose loue was of that dignity,\\nThat it went hand in hand, euen with the Vow\\nI made to her in Marriage; and to decline\\nVpon a wretch, whose Naturall gifts were poore\\nTo those of mine. But Vertue, as it neuer wil be moued,\\nThough Lewdnesse court it in a shape of Heauen:\\nSo Lust, though to a radiant Angell link'd,\\nWill sate it selfe in a Celestiall bed, & prey on Garbage.\\nBut soft, me thinkes I sent the Mornings Ayre;\\nBriefe let me be: Sleeping within mine Orchard,\\nMy custome alwayes in the afternoone;\\nVpon my secure hower thy Vncle stole\\nWith iuyce of cursed Hebenon in a Violl,\\nAnd in the Porches of mine eares did poure\\nThe leaperous Distilment; whose effect\\nHolds such an enmity with bloud of Man,\\nThat swift as Quick-siluer, it courses through\\nThe naturall Gates and Allies of the body;\\nAnd with a sodaine vigour it doth posset\\nAnd curd, like Aygre droppings into Milke,\\nThe thin and wholsome blood: so did it mine;\\nAnd a most instant Tetter bak'd about,\\nMost Lazar-like, with vile and loathsome crust,\\nAll my smooth Body.\\nThus was I, sleeping, by a Brothers hand,\\nOf Life, of Crowne, and Queene at once dispatcht;\\nCut off euen in the Blossomes of my Sinne,\\nVnhouzzled, disappointed, vnnaneld,\\nNo reckoning made, but sent to my account\\nWith all my imperfections on my head;\\nOh horrible Oh horrible, most horrible:\\nIf thou hast nature in thee beare it not;\\nLet not the Royall Bed of Denmarke be\\nA Couch for Luxury and damned Incest.\\nBut howsoeuer thou pursuest this Act,\\nTaint not thy mind; nor let thy Soule contriue\\nAgainst thy Mother ought; leaue her to heauen,\\nAnd to those Thornes that in her bosome lodge,\\nTo pricke and sting her. Fare thee well at once;\\nThe Glow-worme showes the Matine to be neere,\\nAnd gins to pale his vneffectuall Fire:\\nAdue, adue, Hamlet: remember me.\\nEnter.\\n\\n  Ham. Oh all you host of Heauen! Oh Earth; what els?\\nAnd shall I couple Hell? Oh fie: hold my heart;\\nAnd you my sinnewes, grow not instant Old;\\nBut beare me stiffely vp: Remember thee?\\nI, thou poore Ghost, while memory holds a seate\\nIn this distracted Globe: Remember thee?\\nYea, from the Table of my Memory,\\nIle wipe away all triuiall fond Records,\\nAll sawes of Bookes, all formes, all presures past,\\nThat youth and obseruation coppied there;\\nAnd thy Commandment all alone shall liue\\nWithin the Booke and Volume of my Braine,\\nVnmixt with baser matter; yes yes, by Heauen:\\nOh most pernicious woman!\\nOh Villaine, Villaine, smiling damned Villaine!\\nMy Tables, my Tables; meet it is I set it downe,\\nThat one may smile, and smile and be a Villaine;\\nAt least I'm sure it may be so in Denmarke;\\nSo Vnckle there you are: now to my word;\\nIt is; Adue, Adue, Remember me: I haue sworn't\\n\\n   Hor. & Mar. within. My Lord, my Lord.\\nEnter Horatio and Marcellus.\\n\\n  Mar. Lord Hamlet\\n\\n   Hor. Heauen secure him\\n\\n   Mar. So be it\\n\\n   Hor. Illo, ho, ho, my Lord\\n\\n   Ham. Hillo, ho, ho, boy; come bird, come\\n\\n   Mar. How ist my Noble Lord?\\n  Hor. What newes, my Lord?\\n  Ham. Oh wonderfull!\\n  Hor. Good my Lord tell it\\n\\n   Ham. No you'l reueale it\\n\\n   Hor. Not I, my Lord, by Heauen\\n\\n   Mar. Nor I, my Lord\\n\\n   Ham. How say you then, would heart of man once think it?\\nBut you'l be secret?\\n  Both. I, by Heau'n, my Lord\\n\\n   Ham. There's nere a villaine dwelling in all Denmarke\\nBut hee's an arrant knaue\\n\\n   Hor. There needs no Ghost my Lord, come from the\\nGraue, to tell vs this\\n\\n   Ham. Why right, you are i'th' right;\\nAnd so, without more circumstance at all,\\nI hold it fit that we shake hands, and part:\\nYou, as your busines and desires shall point you:\\nFor euery man ha's businesse and desire,\\nSuch as it is: and for mine owne poore part,\\nLooke you, Ile goe pray\\n\\n   Hor. These are but wild and hurling words, my Lord\\n\\n   Ham. I'm sorry they offend you heartily:\\nYes faith, heartily\\n\\n   Hor. There's no offence my Lord\\n\\n   Ham. Yes, by Saint Patricke, but there is my Lord,\\nAnd much offence too, touching this Vision heere:\\nIt is an honest Ghost, that let me tell you:\\nFor your desire to know what is betweene vs,\\nO'remaster't as you may. And now good friends,\\nAs you are Friends, Schollers and Soldiers,\\nGiue me one poore request\\n\\n   Hor. What is't my Lord? we will\\n\\n   Ham. Neuer make known what you haue seen to night\\n\\n   Both. My Lord, we will not\\n\\n   Ham. Nay, but swear't\\n\\n   Hor. Infaith my Lord, not I\\n\\n   Mar. Nor I my Lord: in faith\\n\\n   Ham. Vpon my sword\\n\\n   Marcell. We haue sworne my Lord already\\n\\n   Ham. Indeed, vpon my sword, Indeed\\n\\n   Gho. Sweare.\\n\\nGhost cries vnder the Stage.\\n\\n  Ham. Ah ha boy, sayest thou so. Art thou there truepenny?\\nCome one you here this fellow in the selleredge\\nConsent to sweare\\n\\n   Hor. Propose the Oath my Lord\\n\\n   Ham. Neuer to speake of this that you haue seene.\\nSweare by my sword\\n\\n   Gho. Sweare\\n\\n   Ham. Hic & vbique? Then wee'l shift for grownd,\\nCome hither Gentlemen,\\nAnd lay your hands againe vpon my sword,\\nNeuer to speake of this that you haue heard:\\nSweare by my Sword\\n\\n   Gho. Sweare\\n\\n   Ham. Well said old Mole, can'st worke i'th' ground so fast?\\nA worthy Pioner, once more remoue good friends\\n\\n   Hor. Oh day and night: but this is wondrous strange\\n\\n   Ham. And therefore as a stranger giue it welcome.\\nThere are more things in Heauen and Earth, Horatio,\\nThen are dream't of in our Philosophy. But come,\\nHere as before, neuer so helpe you mercy,\\nHow strange or odde so ere I beare my selfe;\\n(As I perchance heereafter shall thinke meet\\nTo put an Anticke disposition on:)\\nThat you at such time seeing me, neuer shall\\nWith Armes encombred thus, or thus, head shake;\\nOr by pronouncing of some doubtfull Phrase;\\nAs well, we know, or we could and if we would,\\nOr if we list to speake; or there be and if there might,\\nOr such ambiguous giuing out to note,\\nThat you know ought of me; this not to doe:\\nSo grace and mercy at your most neede helpe you:\\nSweare\\n\\n   Ghost. Sweare\\n\\n   Ham. Rest, rest perturbed Spirit: so Gentlemen,\\nWith all my loue I doe commend me to you;\\nAnd what so poore a man as Hamlet is,\\nMay doe t' expresse his loue and friending to you,\\nGod willing shall not lacke: let vs goe in together,\\nAnd still your fingers on your lippes I pray,\\nThe time is out of ioynt: Oh cursed spight,\\nThat euer I was borne to set it right.\\nNay, come let's goe together.\\n\\nExeunt.\\n\\n\\nActus Secundus.\\n\\nEnter Polonius, and Reynoldo.\\n\\n  Polon. Giue him his money, and these notes Reynoldo\\n\\n   Reynol. I will my Lord\\n\\n   Polon. You shall doe maruels wisely: good Reynoldo,\\nBefore you visite him you make inquiry\\nOf his behauiour\\n\\n   Reynol. My Lord, I did intend it\\n\\n   Polon. Marry, well said;\\nVery well said. Looke you Sir,\\nEnquire me first what Danskers are in Paris;\\nAnd how, and who; what meanes; and where they keepe:\\nWhat company, at what expence: and finding\\nBy this encompassement and drift of question,\\nThat they doe know my sonne: Come you more neerer\\nThen your particular demands will touch it,\\nTake you as 'twere some distant knowledge of him,\\nAnd thus I know his father and his friends,\\nAnd in part him. Doe you marke this Reynoldo?\\n  Reynol. I, very well my Lord\\n\\n   Polon. And in part him, but you may say not well;\\nBut if't be hee I meane, hees very wilde;\\nAddicted so and so; and there put on him\\nWhat forgeries you please; marry, none so ranke,\\nAs may dishonour him; take heed of that:\\nBut Sir, such wanton, wild, and vsuall slips,\\nAs are Companions noted and most knowne\\nTo youth and liberty\\n\\n   Reynol. As gaming my Lord\\n\\n   Polon. I, or drinking, fencing, swearing,\\nQuarelling, drabbing. You may goe so farre\\n\\n   Reynol. My Lord that would dishonour him\\n\\n   Polon. Faith no, as you may season it in the charge;\\nYou must not put another scandall on him,\\nThat hee is open to Incontinencie;\\nThat's not my meaning: but breath his faults so quaintly,\\nThat they may seeme the taints of liberty;\\nThe flash and out-breake of a fiery minde,\\nA sauagenes in vnreclaim'd bloud of generall assault\\n\\n   Reynol. But my good Lord\\n\\n   Polon. Wherefore should you doe this?\\n  Reynol. I my Lord, I would know that\\n\\n   Polon. Marry Sir, heere's my drift,\\nAnd I belieue it is a fetch of warrant:\\nYou laying these slight sulleyes on my Sonne,\\nAs 'twere a thing a little soil'd i'th' working:\\nMarke you your party in conuerse; him you would sound,\\nHauing euer seene. In the prenominate crimes,\\nThe youth you breath of guilty, be assur'd\\nHe closes with you in this consequence:\\nGood sir, or so, or friend, or Gentleman.\\nAccording to the Phrase and the Addition,\\nOf man and Country\\n\\n   Reynol. Very good my Lord\\n\\n   Polon. And then Sir does he this?\\nHe does: what was I about to say?\\nI was about say somthing: where did I leaue?\\n  Reynol. At closes in the consequence:\\nAt friend, or so, and Gentleman\\n\\n   Polon. At closes in the consequence, I marry,\\nHe closes with you thus. I know the Gentleman,\\nI saw him yesterday, or tother day;\\nOr then or then, with such and such; and as you say,\\nThere was he gaming, there o'retooke in's Rouse,\\nThere falling out at Tennis; or perchance,\\nI saw him enter such a house of saile;\\nVidelicet, a Brothell, or so forth. See you now;\\nYour bait of falshood, takes this Cape of truth;\\nAnd thus doe we of wisedome and of reach\\nWith windlesses, and with assaies of Bias,\\nBy indirections finde directions out:\\nSo by my former Lecture and aduice\\nShall you my Sonne; you haue me, haue you not?\\n  Reynol. My Lord I haue\\n\\n   Polon. God buy you; fare you well\\n\\n   Reynol. Good my Lord\\n\\n   Polon. Obserue his inclination in your selfe\\n\\n   Reynol. I shall my Lord\\n\\n   Polon. And let him plye his Musicke\\n\\n   Reynol. Well, my Lord.\\nEnter.\\n\\nEnter Ophelia.\\n\\n  Polon. Farewell:\\nHow now Ophelia, what's the matter?\\n  Ophe. Alas my Lord, I haue beene so affrighted\\n\\n   Polon. With what, in the name of Heauen?\\n  Ophe. My Lord, as I was sowing in my Chamber,\\nLord Hamlet with his doublet all vnbrac'd,\\nNo hat vpon his head, his stockings foul'd,\\nVngartred, and downe giued to his Anckle,\\nPale as his shirt, his knees knocking each other,\\nAnd with a looke so pitious in purport,\\nAs if he had been loosed out of hell,\\nTo speake of horrors: he comes before me\\n\\n   Polon. Mad for thy Loue?\\n  Ophe. My Lord, I doe not know: but truly I do feare it\\n\\n   Polon. What said he?\\n  Ophe. He tooke me by the wrist, and held me hard;\\nThen goes he to the length of all his arme;\\nAnd with his other hand thus o're his brow,\\nHe fals to such perusall of my face,\\nAs he would draw it. Long staid he so,\\nAt last, a little shaking of mine Arme:\\nAnd thrice his head thus wauing vp and downe;\\nHe rais'd a sigh, so pittious and profound,\\nThat it did seeme to shatter all his bulke,\\nAnd end his being. That done, he lets me goe,\\nAnd with his head ouer his shoulders turn'd,\\nHe seem'd to finde his way without his eyes,\\nFor out adores he went without their helpe;\\nAnd to the last, bended their light on me\\n\\n   Polon. Goe with me, I will goe seeke the King,\\nThis is the very extasie of Loue,\\nWhose violent property foredoes it selfe,\\nAnd leads the will to desperate Vndertakings,\\nAs oft as any passion vnder Heauen,\\nThat does afflict our Natures. I am sorrie,\\nWhat haue you giuen him any hard words of late?\\n  Ophe. No my good Lord: but as you did command,\\nI did repell his Letters, and deny'de\\nHis accesse to me\\n\\n   Pol. That hath made him mad.\\nI am sorrie that with better speed and iudgement\\nI had not quoted him. I feare he did but trifle,\\nAnd meant to wracke thee: but beshrew my iealousie:\\nIt seemes it is as proper to our Age,\\nTo cast beyond our selues in our Opinions,\\nAs it is common for the yonger sort\\nTo lacke discretion. Come, go we to the King,\\nThis must be knowne, being kept close might moue\\nMore greefe to hide, then hate to vtter loue.\\n\\nExeunt.\\n\\n\\nScena Secunda.\\n\\nEnter King, Queene, Rosincrane, and Guildensterne Cum alijs.\\n\\n  King. Welcome deere Rosincrance and Guildensterne.\\nMoreouer, that we much did long to see you,\\nThe neede we haue to vse you, did prouoke\\nOur hastie sending. Something haue you heard\\nOf Hamlets transformation: so I call it,\\nSince not th' exterior, nor the inward man\\nResembles that it was. What it should bee\\nMore then his Fathers death, that thus hath put him\\nSo much from th' vnderstanding of himselfe,\\nI cannot deeme of. I intreat you both,\\nThat being of so young dayes brought vp with him:\\nAnd since so Neighbour'd to his youth, and humour,\\nThat you vouchsafe your rest heere in our Court\\nSome little time: so by your Companies\\nTo draw him on to pleasures, and to gather\\nSo much as from Occasions you may gleane,\\nThat open'd lies within our remedie\\n\\n   Qu. Good Gentlemen, he hath much talk'd of you,\\nAnd sure I am, two men there are not liuing,\\nTo whom he more adheres. If it will please you\\nTo shew vs so much Gentrie, and good will,\\nAs to expend your time with vs a-while,\\nFor the supply and profit of our Hope,\\nYour Visitation shall receiue such thankes\\nAs fits a Kings remembrance\\n\\n   Rosin. Both your Maiesties\\nMight by the Soueraigne power you haue of vs,\\nPut your dread pleasures, more into Command\\nThen to Entreatie\\n\\n   Guil. We both obey,\\nAnd here giue vp our selues, in the full bent,\\nTo lay our Seruices freely at your feete,\\nTo be commanded\\n\\n   King. Thankes Rosincrance, and gentle Guildensterne\\n\\n   Qu. Thankes Guildensterne and gentle Rosincrance.\\nAnd I beseech you instantly to visit\\nMy too much changed Sonne.\\nGo some of ye,\\nAnd bring the Gentlemen where Hamlet is\\n\\n   Guil. Heauens make our presence and our practises\\nPleasant and helpfull to him.\\nEnter.\\n\\n  Queene. Amen.\\nEnter Polonius.\\n\\n  Pol. Th' Ambassadors from Norwey, my good Lord,\\nAre ioyfully return'd\\n\\n   King. Thou still hast bin the father of good Newes\\n\\n   Pol. Haue I, my Lord? Assure you, my good Liege,\\nI hold my dutie, as I hold my Soule,\\nBoth to my God, one to my gracious King:\\nAnd I do thinke, or else this braine of mine\\nHunts not the traile of Policie, so sure\\nAs I haue vs'd to do: that I haue found\\nThe very cause of Hamlets Lunacie\\n\\n   King. Oh speake of that, that I do long to heare\\n\\n   Pol. Giue first admittance to th' Ambassadors,\\nMy Newes shall be the Newes to that great Feast\\n\\n   King. Thy selfe do grace to them, and bring them in.\\nHe tels me my sweet Queene, that he hath found\\nThe head and sourse of all your Sonnes distemper\\n\\n   Qu. I doubt it is no other, but the maine,\\nHis Fathers death, and our o're-hasty Marriage.\\nEnter Polonius, Voltumand, and Cornelius.\\n\\n  King. Well, we shall sift him. Welcome good Frends:\\nSay Voltumand, what from our Brother Norwey?\\n  Volt. Most faire returne of Greetings, and Desires.\\nVpon our first, he sent out to suppresse\\nHis Nephewes Leuies, which to him appear'd\\nTo be a preparation 'gainst the Poleak:\\nBut better look'd into, he truly found\\nIt was against your Highnesse, whereat greeued,\\nThat so his Sicknesse, Age, and Impotence\\nWas falsely borne in hand, sends out Arrests\\nOn Fortinbras, which he (in breefe) obeyes,\\nReceiues rebuke from Norwey: and in fine,\\nMakes Vow before his Vnkle, neuer more\\nTo giue th' assay of Armes against your Maiestie.\\nWhereon old Norwey, ouercome with ioy,\\nGiues him three thousand Crownes in Annuall Fee,\\nAnd his Commission to imploy those Soldiers\\nSo leuied as before, against the Poleak:\\nWith an intreaty heerein further shewne,\\nThat it might please you to giue quiet passe\\nThrough your Dominions, for his Enterprize,\\nOn such regards of safety and allowance,\\nAs therein are set downe\\n\\n   King. It likes vs well:\\nAnd at our more consider'd time wee'l read,\\nAnswer, and thinke vpon this Businesse.\\nMeane time we thanke you, for your well-tooke Labour.\\nGo to your rest, at night wee'l Feast together.\\nMost welcome home.\\n\\nExit Ambass.\\n\\n  Pol. This businesse is very well ended.\\nMy Liege, and Madam, to expostulate\\nWhat Maiestie should be, what Dutie is,\\nWhy day is day; night, night; and time is time,\\nWere nothing but to waste Night, Day, and Time.\\nTherefore, since Breuitie is the Soule of Wit,\\nAnd tediousnesse, the limbes and outward flourishes,\\nI will be breefe. Your Noble Sonne is mad:\\nMad call I it; for to define true Madnesse,\\nWhat is't, but to be nothing else but mad.\\nBut let that go\\n\\n   Qu. More matter, with lesse Art\\n\\n   Pol. Madam, I sweare I vse no Art at all:\\nThat he is mad, 'tis true: 'Tis true 'tis pittie,\\nAnd pittie it is true: A foolish figure,\\nBut farewell it: for I will vse no Art.\\nMad let vs grant him then: and now remaines\\nThat we finde out the cause of this effect,\\nOr rather say, the cause of this defect;\\nFor this effect defectiue, comes by cause,\\nThus it remaines, and the remainder thus. Perpend,\\nI haue a daughter: haue, whil'st she is mine,\\nWho in her Dutie and Obedience, marke,\\nHath giuen me this: now gather, and surmise.\\n\\nThe Letter.\\n\\nTo the Celestiall, and my Soules Idoll, the most beautifed Ophelia.\\nThat's an ill Phrase, a vilde Phrase, beautified is a vilde\\nPhrase: but you shall heare these in her excellent white\\nbosome, these\\n\\n   Qu. Came this from Hamlet to her\\n\\n   Pol. Good Madam stay awhile, I will be faithfull.\\nDoubt thou, the Starres are fire,\\nDoubt, that the Sunne doth moue:\\nDoubt Truth to be a Lier,\\nBut neuer Doubt, I loue.\\nO deere Ophelia, I am ill at these Numbers: I haue not Art to\\nreckon my grones; but that I loue thee best, oh most Best beleeue\\nit. Adieu.\\nThine euermore most deere Lady, whilst this\\nMachine is to him, Hamlet.\\nThis in Obedience hath my daughter shew'd me:\\nAnd more aboue hath his soliciting,\\nAs they fell out by Time, by Meanes, and Place,\\nAll giuen to mine eare\\n\\n   King. But how hath she receiu'd his Loue?\\n  Pol. What do you thinke of me?\\n  King. As of a man, faithfull and Honourable\\n\\n   Pol. I wold faine proue so. But what might you think?\\nWhen I had seene this hot loue on the wing,\\nAs I perceiued it, I must tell you that\\nBefore my Daughter told me what might you\\nOr my deere Maiestie your Queene heere, think,\\nIf I had playd the Deske or Table-booke,\\nOr giuen my heart a winking, mute and dumbe,\\nOr look'd vpon this Loue, with idle sight,\\nWhat might you thinke? No, I went round to worke,\\nAnd (my yong Mistris) thus I did bespeake\\nLord Hamlet is a Prince out of thy Starre,\\nThis must not be: and then, I Precepts gaue her,\\nThat she should locke her selfe from his Resort,\\nAdmit no Messengers, receiue no Tokens:\\nWhich done, she tooke the Fruites of my Aduice,\\nAnd he repulsed. A short Tale to make,\\nFell into a Sadnesse, then into a Fast,\\nThence to a Watch, thence into a Weaknesse,\\nThence to a Lightnesse, and by this declension\\nInto the Madnesse whereon now he raues,\\nAnd all we waile for\\n\\n   King. Do you thinke 'tis this?\\n  Qu. It may be very likely\\n\\n   Pol. Hath there bene such a time, I'de fain know that,\\nThat I haue possitiuely said, 'tis so,\\nWhen it prou'd otherwise?\\n  King. Not that I know\\n\\n   Pol. Take this from this; if this be otherwise,\\nIf Circumstances leade me, I will finde\\nWhere truth is hid, though it were hid indeede\\nWithin the Center\\n\\n   King. How may we try it further?\\n  Pol. You know sometimes\\nHe walkes foure houres together, heere\\nIn the Lobby\\n\\n   Qu. So he ha's indeed\\n\\n   Pol. At such a time Ile loose my Daughter to him,\\nBe you and I behinde an Arras then,\\nMarke the encounter: If he loue her not,\\nAnd be not from his reason falne thereon;\\nLet me be no Assistant for a State,\\nAnd keepe a Farme and Carters\\n\\n   King. We will try it.\\nEnter Hamlet reading on a Booke.\\n\\n  Qu. But looke where sadly the poore wretch\\nComes reading\\n\\n   Pol. Away I do beseech you, both away,\\nIle boord him presently.\\n\\nExit King & Queen.\\n\\nOh giue me leaue. How does my good Lord Hamlet?\\n  Ham. Well, God-a-mercy\\n\\n   Pol. Do you know me, my Lord?\\n  Ham. Excellent, excellent well: y'are a Fishmonger\\n\\n   Pol. Not I my Lord\\n\\n   Ham. Then I would you were so honest a man\\n\\n   Pol. Honest, my Lord?\\n  Ham. I sir, to be honest as this world goes, is to bee\\none man pick'd out of two thousand\\n\\n   Pol. That's very true, my Lord\\n\\n   Ham. For if the Sun breed Magots in a dead dogge,\\nbeing a good kissing Carrion-\\nHaue you a daughter?\\n  Pol. I haue my Lord\\n\\n   Ham. Let her not walke i'thSunne: Conception is a\\nblessing, but not as your daughter may conceiue. Friend\\nlooke too't\\n\\n   Pol. How say you by that? Still harping on my daughter:\\nyet he knew me not at first; he said I was a Fishmonger:\\nhe is farre gone, farre gone: and truly in my youth,\\nI suffred much extreamity for loue: very neere this. Ile\\nspeake to him againe. What do you read my Lord?\\n  Ham. Words, words, words\\n\\n   Pol. What is the matter, my Lord?\\n  Ham. Betweene who?\\n  Pol. I meane the matter you meane, my Lord\\n\\n   Ham. Slanders Sir: for the Satyricall slaue saies here,\\nthat old men haue gray Beards; that their faces are wrinkled;\\ntheir eyes purging thicke Amber, or Plum-Tree\\nGumme: and that they haue a plentifull locke of Wit,\\ntogether with weake Hammes. All which Sir, though I\\nmost powerfully, and potently beleeue; yet I holde it\\nnot Honestie to haue it thus set downe: For you your\\nselfe Sir, should be old as I am, if like a Crab you could\\ngo backward\\n\\n   Pol. Though this be madnesse,\\nYet there is Method in't: will you walke\\nOut of the ayre my Lord?\\n  Ham. Into my Graue?\\n  Pol. Indeed that is out o'th' Ayre:\\nHow pregnant (sometimes) his Replies are?\\nA happinesse,\\nThat often Madnesse hits on,\\nWhich Reason and Sanitie could not\\nSo prosperously be deliuer'd of.\\nI will leaue him,\\nAnd sodainely contriue the meanes of meeting\\nBetweene him, and my daughter.\\nMy Honourable Lord, I will most humbly\\nTake my leaue of you\\n\\n   Ham. You cannot Sir take from me any thing, that I\\nwill more willingly part withall, except my life, my\\nlife\\n\\n   Polon. Fare you well my Lord\\n\\n   Ham. These tedious old fooles\\n\\n   Polon. You goe to seeke my Lord Hamlet; there\\nhee is.\\nEnter Rosincran and Guildensterne.\\n\\n  Rosin. God saue you Sir\\n\\n   Guild. Mine honour'd Lord?\\n  Rosin. My most deare Lord?\\n  Ham. My excellent good friends? How do'st thou\\nGuildensterne? Oh, Rosincrane; good Lads: How doe ye\\nboth?\\n  Rosin. As the indifferent Children of the earth\\n\\n   Guild. Happy, in that we are not ouer-happy: on Fortunes\\nCap, we are not the very Button\\n\\n   Ham. Nor the Soales of her Shoo?\\n  Rosin. Neither my Lord\\n\\n   Ham. Then you liue about her waste, or in the middle\\nof her fauour?\\n  Guil. Faith, her priuates, we\\n\\n   Ham. In the secret parts of Fortune? Oh, most true:\\nshe is a Strumpet. What's the newes?\\n  Rosin. None my Lord; but that the World's growne\\nhonest\\n\\n   Ham. Then is Doomesday neere: But your newes is\\nnot true. Let me question more in particular: what haue\\nyou my good friends, deserued at the hands of Fortune,\\nthat she sends you to Prison hither?\\n  Guil. Prison, my Lord?\\n  Ham. Denmark's a Prison\\n\\n   Rosin. Then is the World one\\n\\n   Ham. A goodly one, in which there are many Confines,\\nWards, and Dungeons; Denmarke being one o'th'\\nworst\\n\\n   Rosin. We thinke not so my Lord\\n\\n   Ham. Why then 'tis none to you; for there is nothing\\neither good or bad, but thinking makes it so: to me it is\\na prison\\n\\n   Rosin. Why then your Ambition makes it one: 'tis\\ntoo narrow for your minde\\n\\n   Ham. O God, I could be bounded in a nutshell, and\\ncount my selfe a King of infinite space; were it not that\\nI haue bad dreames\\n\\n   Guil. Which dreames indeed are Ambition: for the\\nvery substance of the Ambitious, is meerely the shadow\\nof a Dreame\\n\\n   Ham. A dreame it selfe is but a shadow\\n\\n   Rosin. Truely, and I hold Ambition of so ayry and\\nlight a quality, that it is but a shadowes shadow\\n\\n   Ham. Then are our Beggers bodies; and our Monarchs\\nand out-stretcht Heroes the Beggers Shadowes:\\nshall wee to th' Court: for, by my fey I cannot reason?\\n  Both. Wee'l wait vpon you\\n\\n   Ham. No such matter. I will not sort you with the\\nrest of my seruants: for to speake to you like an honest\\nman: I am most dreadfully attended; but in the beaten\\nway of friendship, What make you at Elsonower?\\n  Rosin. To visit you my Lord, no other occasion\\n\\n   Ham. Begger that I am, I am euen poore in thankes;\\nbut I thanke you: and sure deare friends my thanks\\nare too deare a halfepeny; were you not sent for? Is it\\nyour owne inclining? Is it a free visitation? Come,\\ndeale iustly with me: come, come; nay speake\\n\\n   Guil. What should we say my Lord?\\n  Ham. Why any thing. But to the purpose; you were\\nsent for; and there is a kinde confession in your lookes;\\nwhich your modesties haue not craft enough to color,\\nI know the good King & Queene haue sent for you\\n\\n   Rosin. To what end my Lord?\\n  Ham. That you must teach me: but let mee coniure\\nyou by the rights of our fellowship, by the consonancy of\\nour youth, by the Obligation of our euer-preserued loue,\\nand by what more deare, a better proposer could charge\\nyou withall; be euen and direct with me, whether you\\nwere sent for or no\\n\\n   Rosin. What say you?\\n  Ham. Nay then I haue an eye of you: if you loue me\\nhold not off\\n\\n   Guil. My Lord, we were sent for\\n\\n   Ham. I will tell you why; so shall my anticipation\\npreuent your discouery of your secricie to the King and\\nQueene: moult no feather, I haue of late, but wherefore\\nI know not, lost all my mirth, forgone all custome of exercise;\\nand indeed, it goes so heauenly with my disposition;\\nthat this goodly frame the Earth, seemes to me a sterrill\\nPromontory; this most excellent Canopy the Ayre,\\nlook you, this braue ore-hanging, this Maiesticall Roofe,\\nfretted with golden fire: why, it appeares no other thing\\nto mee, then a foule and pestilent congregation of vapours.\\nWhat a piece of worke is a man! how Noble in\\nReason? how infinite in faculty? in forme and mouing\\nhow expresse and admirable? in Action, how like an Angel?\\nin apprehension, how like a God? the beauty of the\\nworld, the Parragon of Animals; and yet to me, what is\\nthis Quintessence of Dust? Man delights not me; no,\\nnor Woman neither; though by your smiling you seeme\\nto say so\\n\\n   Rosin. My Lord, there was no such stuffe in my\\nthoughts\\n\\n   Ham. Why did you laugh, when I said, Man delights\\nnot me?\\n  Rosin. To thinke, my Lord, if you delight not in Man,\\nwhat Lenton entertainment the Players shall receiue\\nfrom you: wee coated them on the way, and hither are\\nthey comming to offer you Seruice\\n\\n   Ham. He that playes the King shall be welcome; his\\nMaiesty shall haue Tribute of mee: the aduenturous\\nKnight shal vse his Foyle and Target: the Louer shall\\nnot sigh gratis, the humorous man shall end his part in\\npeace: the Clowne shall make those laugh whose lungs\\nare tickled a'th' sere: and the Lady shall say her minde\\nfreely; or the blanke Verse shall halt for't: what Players\\nare they?\\n  Rosin. Euen those you were wont to take delight in\\nthe Tragedians of the City\\n\\n   Ham. How chances it they trauaile? their residence\\nboth in reputation and profit was better both\\nwayes\\n\\n   Rosin. I thinke their Inhibition comes by the meanes\\nof the late Innouation?\\n  Ham. Doe they hold the same estimation they did\\nwhen I was in the City? Are they so follow'd?\\n  Rosin. No indeed, they are not\\n\\n   Ham. How comes it? doe they grow rusty?\\n  Rosin. Nay, their indeauour keepes in the wonted\\npace; But there is Sir an ayrie of Children, little\\nYases, that crye out on the top of question; and\\nare most tyrannically clap't for't: these are now the\\nfashion, and so be-ratled the common Stages (so they\\ncall them) that many wearing Rapiers, are affraide of\\nGoose-quils, and dare scarse come thither\\n\\n   Ham. What are they Children? Who maintains 'em?\\nHow are they escorted? Will they pursue the Quality no\\nlonger then they can sing? Will they not say afterwards\\nif they should grow themselues to common Players (as\\nit is most like if their meanes are not better) their Writers\\ndo them wrong, to make them exclaim against their\\nowne Succession\\n\\n   Rosin. Faith there ha's bene much to do on both sides:\\nand the Nation holds it no sinne, to tarre them to Controuersie.\\nThere was for a while, no mony bid for argument,\\nvnlesse the Poet and the Player went to Cuffes in\\nthe Question\\n\\n   Ham. Is't possible?\\n  Guild. Oh there ha's beene much throwing about of\\nBraines\\n\\n   Ham. Do the Boyes carry it away?\\n  Rosin. I that they do my Lord. Hercules & his load too\\n\\n   Ham. It is not strange: for mine Vnckle is King of\\nDenmarke, and those that would make mowes at him\\nwhile my Father liued; giue twenty, forty, an hundred\\nDucates a peece, for his picture in Little. There is something\\nin this more then Naturall, if Philosophie could\\nfinde it out.\\n\\nFlourish for the Players.\\n\\n  Guil. There are the Players\\n\\n   Ham. Gentlemen, you are welcom to Elsonower: your\\nhands, come: The appurtenance of Welcome, is Fashion\\nand Ceremony. Let me comply with you in the Garbe,\\nlest my extent to the Players (which I tell you must shew\\nfairely outward) should more appeare like entertainment\\nthen yours. You are welcome: but my Vnckle Father,\\nand Aunt Mother are deceiu'd\\n\\n   Guil. In what my deere Lord?\\n  Ham. I am but mad North, North-West: when the\\nWinde is Southerly, I know a Hawke from a Handsaw.\\nEnter Polonius.\\n\\n  Pol. Well be with you Gentlemen\\n\\n   Ham. Hearke you Guildensterne, and you too: at each\\neare a hearer: that great Baby you see there, is not yet\\nout of his swathing clouts\\n\\n   Rosin. Happily he's the second time come to them: for\\nthey say, an old man is twice a childe\\n\\n   Ham. I will Prophesie. Hee comes to tell me of the\\nPlayers. Mark it, you say right Sir: for a Monday morning\\n'twas so indeed\\n\\n   Pol. My Lord, I haue Newes to tell you\\n\\n   Ham. My Lord, I haue Newes to tell you.\\nWhen Rossius an Actor in Rome-\\n  Pol. The Actors are come hither my Lord\\n\\n   Ham. Buzze, buzze\\n\\n   Pol. Vpon mine Honor\\n\\n   Ham. Then can each Actor on his Asse-\\n  Polon. The best Actors in the world, either for Tragedie,\\nComedie, Historie, Pastorall:\\nPastoricall-Comicall-Historicall-Pastorall:\\nTragicall-Historicall: Tragicall-Comicall-Historicall-Pastorall:\\nScene indiuidible: or Poem\\nvnlimited. Seneca cannot be too heauy, nor Plautus\\ntoo light, for the law of Writ, and the Liberty. These are\\nthe onely men\\n\\n   Ham. O Iephta Iudge of Israel, what a Treasure had'st\\nthou?\\n  Pol. What a Treasure had he, my Lord?\\n  Ham. Why one faire Daughter, and no more,\\nThe which he loued passing well\\n\\n   Pol. Still on my Daughter\\n\\n   Ham. Am I not i'th' right old Iephta?\\n  Polon. If you call me Iephta my Lord, I haue a daughter\\nthat I loue passing well\\n\\n   Ham. Nay that followes not\\n\\n   Polon. What followes then, my Lord?\\n  Ha. Why, As by lot, God wot: and then you know, It\\ncame to passe, as most like it was: The first rowe of the\\nPons Chanson will shew you more. For looke where my\\nAbridgements come.\\nEnter foure or fiue Players.\\n\\nY'are welcome Masters, welcome all. I am glad to see\\nthee well: Welcome good Friends. Oh my olde Friend?\\nThy face is valiant since I saw thee last: Com'st thou to\\nbeard me in Denmarke? What, my yong Lady and Mistris?\\nByrlady your Ladiship is neerer Heauen then when\\nI saw you last, by the altitude of a Choppine. Pray God\\nyour voice like a peece of vncurrant Gold be not crack'd\\nwithin the ring. Masters, you are all welcome: wee'l e'ne\\nto't like French Faulconers, flie at any thing we see: wee'l\\nhaue a Speech straight. Come giue vs a tast of your quality:\\ncome, a passionate speech\\n\\n   1.Play. What speech, my Lord?\\n  Ham. I heard thee speak me a speech once, but it was\\nneuer Acted: or if it was, not aboue once, for the Play I\\nremember pleas'd not the Million, 'twas Cauiarie to the\\nGenerall: but it was (as I receiu'd it, and others, whose\\niudgement in such matters, cried in the top of mine) an\\nexcellent Play; well digested in the Scoenes, set downe\\nwith as much modestie, as cunning. I remember one said,\\nthere was no Sallets in the lines, to make the matter sauory;\\nnor no matter in the phrase, that might indite the\\nAuthor of affectation, but cal'd it an honest method. One\\ncheefe Speech in it, I cheefely lou'd, 'twas Aeneas Tale\\nto Dido, and thereabout of it especially, where he speaks\\nof Priams slaughter. If it liue in your memory, begin at\\nthis Line, let me see, let me see: The rugged Pyrrhus like\\nth'Hyrcanian Beast. It is not so: it begins with Pyrrhus\\nThe rugged Pyrrhus, he whose Sable Armes\\nBlacke as his purpose, did the night resemble\\nWhen he lay couched in the Ominous Horse,\\nHath now this dread and blacke Complexion smear'd\\nWith Heraldry more dismall: Head to foote\\nNow is he to take Geulles, horridly Trick'd\\nWith blood of Fathers, Mothers, Daughters, Sonnes,\\nBak'd and impasted with the parching streets,\\nThat lend a tyrannous, and damned light\\nTo their vilde Murthers, roasted in wrath and fire,\\nAnd thus o're-sized with coagulate gore,\\nWith eyes like Carbuncles, the hellish Pyrrhus\\nOlde Grandsire Priam seekes\\n\\n   Pol. Fore God, my Lord, well spoken, with good accent,\\nand good discretion\\n\\n   1.Player. Anon he findes him,\\nStriking too short at Greekes. His anticke Sword,\\nRebellious to his Arme, lyes where it falles\\nRepugnant to command: vnequall match,\\nPyrrhus at Priam driues, in Rage strikes wide:\\nBut with the whiffe and winde of his fell Sword,\\nTh' vnnerued Father fals. Then senselesse Illium,\\nSeeming to feele his blow, with flaming top\\nStoopes to his Bace, and with a hideous crash\\nTakes Prisoner Pyrrhus eare. For loe, his Sword\\nWhich was declining on the Milkie head\\nOf Reuerend Priam, seem'd i'th' Ayre to sticke:\\nSo as a painted Tyrant Pyrrhus stood,\\nAnd like a Newtrall to his will and matter, did nothing.\\nBut as we often see against some storme,\\nA silence in the Heauens, the Racke stand still,\\nThe bold windes speechlesse, and the Orbe below\\nAs hush as death: Anon the dreadfull Thunder\\nDoth rend the Region. So after Pyrrhus pause,\\nA rowsed Vengeance sets him new a-worke,\\nAnd neuer did the Cyclops hammers fall\\nOn Mars his Armours, forg'd for proofe Eterne,\\nWith lesse remorse then Pyrrhus bleeding sword\\nNow falles on Priam.\\nOut, out, thou Strumpet-Fortune, all you Gods,\\nIn generall Synod take away her power:\\nBreake all the Spokes and Fallies from her wheele,\\nAnd boule the round Naue downe the hill of Heauen,\\nAs low as to the Fiends\\n\\n   Pol. This is too long\\n\\n   Ham. It shall to'th Barbars, with your beard. Prythee\\nsay on: He's for a Iigge, or a tale of Baudry, or hee\\nsleepes. Say on; come to Hecuba\\n\\n   1.Play. But who, O who, had seen the inobled Queen\\n\\n   Ham. The inobled Queene?\\n  Pol. That's good: Inobled Queene is good\\n\\n   1.Play. Run bare-foot vp and downe,\\nThreatning the flame\\nWith Bisson Rheume: A clout about that head,\\nWhere late the Diadem stood, and for a Robe\\nAbout her lanke and all ore-teamed Loines,\\nA blanket in th' Alarum of feare caught vp.\\nWho this had seene, with tongue in Venome steep'd,\\n'Gainst Fortunes State, would Treason haue pronounc'd?\\nBut if the Gods themselues did see her then,\\nWhen she saw Pyrrhus make malicious sport\\nIn mincing with his Sword her Husbands limbes,\\nThe instant Burst of Clamour that she made\\n(Vnlesse things mortall moue them not at all)\\nWould haue made milche the Burning eyes of Heauen,\\nAnd passion in the Gods\\n\\n   Pol. Looke where he ha's not turn'd his colour, and\\nha's teares in's eyes. Pray you no more\\n\\n   Ham. 'Tis well, Ile haue thee speake out the rest,\\nsoone. Good my Lord, will you see the Players wel bestow'd.\\nDo ye heare, let them be well vs'd: for they are\\nthe Abstracts and breefe Chronicles of the time. After\\nyour death, you were better haue a bad Epitaph, then\\ntheir ill report while you liued\\n\\n   Pol. My Lord, I will vse them according to their desart\\n\\n   Ham. Gods bodykins man, better. Vse euerie man\\nafter his desart, and who should scape whipping: vse\\nthem after your own Honor and Dignity. The lesse they\\ndeserue, the more merit is in your bountie. Take them\\nin\\n\\n   Pol. Come sirs.\\n\\nExit Polon.\\n\\n  Ham. Follow him Friends: wee'l heare a play to morrow.\\nDost thou heare me old Friend, can you play the\\nmurther of Gonzago?\\n  Play. I my Lord\\n\\n   Ham. Wee'l ha't to morrow night. You could for a\\nneed study a speech of some dosen or sixteene lines, which\\nI would set downe, and insert in't? Could ye not?\\n  Play. I my Lord\\n\\n   Ham. Very well. Follow that Lord, and looke you\\nmock him not. My good Friends, Ile leaue you til night\\nyou are welcome to Elsonower?\\n  Rosin. Good my Lord.\\n\\nExeunt.\\n\\nManet Hamlet.\\n\\n  Ham. I so, God buy'ye: Now I am alone.\\nOh what a Rogue and Pesant slaue am I?\\nIs it not monstrous that this Player heere,\\nBut in a Fixion, in a dreame of Passion,\\nCould force his soule so to his whole conceit,\\nThat from her working, all his visage warm'd;\\nTeares in his eyes, distraction in's Aspect,\\nA broken voyce, and his whole Function suiting\\nWith Formes, to his Conceit? And all for nothing?\\nFor Hecuba?\\nWhat's Hecuba to him, or he to Hecuba,\\nThat he should weepe for her? What would he doe,\\nHad he the Motiue and the Cue for passion\\nThat I haue? He would drowne the Stage with teares,\\nAnd cleaue the generall eare with horrid speech:\\nMake mad the guilty, and apale the free,\\nConfound the ignorant, and amaze indeed,\\nThe very faculty of Eyes and Eares. Yet I,\\nA dull and muddy-metled Rascall, peake\\nLike Iohn a-dreames, vnpregnant of my cause,\\nAnd can say nothing: No, not for a King,\\nVpon whose property, and most deere life,\\nA damn'd defeate was made. Am I a Coward?\\nWho calles me Villaine? breakes my pate a-crosse?\\nPluckes off my Beard, and blowes it in my face?\\nTweakes me by'th' Nose? giues me the Lye i'th' Throate,\\nAs deepe as to the Lungs? Who does me this?\\nHa? Why I should take it: for it cannot be,\\nBut I am Pigeon-Liuer'd, and lacke Gall\\nTo make Oppression bitter, or ere this,\\nI should haue fatted all the Region Kites\\nWith this Slaues Offall, bloudy: a Bawdy villaine,\\nRemorselesse, Treacherous, Letcherous, kindles villaine!\\nOh Vengeance!\\nWho? What an Asse am I? I sure, this is most braue,\\nThat I, the Sonne of the Deere murthered,\\nPrompted to my Reuenge by Heauen, and Hell,\\nMust (like a Whore) vnpacke my heart with words,\\nAnd fall a Cursing like a very Drab.\\nA Scullion? Fye vpon't: Foh. About my Braine.\\nI haue heard, that guilty Creatures sitting at a Play,\\nHaue by the very cunning of the Scoene,\\nBene strooke so to the soule, that presently\\nThey haue proclaim'd their Malefactions.\\nFor Murther, though it haue no tongue, will speake\\nWith most myraculous Organ. Ile haue these Players,\\nPlay something like the murder of my Father,\\nBefore mine Vnkle. Ile obserue his lookes,\\nIle rent him to the quicke: If he but blench\\nI know my course. The Spirit that I haue seene\\nMay be the Diuell, and the Diuel hath power\\nT' assume a pleasing shape, yea and perhaps\\nOut of my Weaknesse, and my Melancholly,\\nAs he is very potent with such Spirits,\\nAbuses me to damne me. Ile haue grounds\\nMore Relatiue then this: The Play's the thing,\\nWherein Ile catch the Conscience of the King.\\n\\nExit\\n\\nEnter King, Queene, Polonius, Ophelia, Rosincrance,\\nGuildenstern, and\\nLords.\\n\\n  King. And can you by no drift of circumstance\\nGet from him why he puts on this Confusion:\\nGrating so harshly all his dayes of quiet\\nWith turbulent and dangerous Lunacy\\n\\n   Rosin. He does confesse he feeles himselfe distracted,\\nBut from what cause he will by no meanes speake\\n\\n   Guil. Nor do we finde him forward to be sounded,\\nBut with a crafty Madnesse keepes aloofe:\\nWhen we would bring him on to some Confession\\nOf his true state\\n\\n   Qu. Did he receiue you well?\\n  Rosin. Most like a Gentleman\\n\\n   Guild. But with much forcing of his disposition\\n\\n   Rosin. Niggard of question, but of our demands\\nMost free in his reply\\n\\n   Qu. Did you assay him to any pastime?\\n  Rosin. Madam, it so fell out, that certaine Players\\nWe ore-wrought on the way: of these we told him,\\nAnd there did seeme in him a kinde of ioy\\nTo heare of it: They are about the Court,\\nAnd (as I thinke) they haue already order\\nThis night to play before him\\n\\n   Pol. 'Tis most true:\\nAnd he beseech'd me to intreate your Maiesties\\nTo heare, and see the matter\\n\\n   King. With all my heart, and it doth much content me\\nTo heare him so inclin'd. Good Gentlemen,\\nGiue him a further edge, and driue his purpose on\\nTo these delights\\n\\n   Rosin. We shall my Lord.\\n\\nExeunt.\\n\\n  King. Sweet Gertrude leaue vs too,\\nFor we haue closely sent for Hamlet hither,\\nThat he, as 'twere by accident, may there\\nAffront Ophelia. Her Father, and my selfe (lawful espials)\\nWill so bestow our selues, that seeing vnseene\\nWe may of their encounter frankely iudge,\\nAnd gather by him, as he is behaued,\\nIf't be th' affliction of his loue, or no.\\nThat thus he suffers for\\n\\n   Qu. I shall obey you,\\nAnd for your part Ophelia, I do wish\\nThat your good Beauties be the happy cause\\nOf Hamlets wildenesse: so shall I hope your Vertues\\nWill bring him to his wonted way againe,\\nTo both your Honors\\n\\n   Ophe. Madam, I wish it may\\n\\n   Pol. Ophelia, walke you heere. Gracious so please ye\\nWe will bestow our selues: Reade on this booke,\\nThat shew of such an exercise may colour\\nYour lonelinesse. We are oft too blame in this,\\n'Tis too much prou'd, that with Deuotions visage,\\nAnd pious Action, we do surge o're\\nThe diuell himselfe\\n\\n   King. Oh 'tis true:\\nHow smart a lash that speech doth giue my Conscience?\\nThe Harlots Cheeke beautied with plaist'ring Art\\nIs not more vgly to the thing that helpes it,\\nThen is my deede, to my most painted word.\\nOh heauie burthen!\\n  Pol. I heare him comming, let's withdraw my Lord.\\n\\nExeunt.\\n\\nEnter Hamlet.\\n\\n  Ham. To be, or not to be, that is the Question:\\nWhether 'tis Nobler in the minde to suffer\\nThe Slings and Arrowes of outragious Fortune,\\nOr to take Armes against a Sea of troubles,\\nAnd by opposing end them: to dye, to sleepe\\nNo more; and by a sleepe, to say we end\\nThe Heart-ake, and the thousand Naturall shockes\\nThat Flesh is heyre too? 'Tis a consummation\\nDeuoutly to be wish'd. To dye to sleepe,\\nTo sleepe, perchance to Dreame; I, there's the rub,\\nFor in that sleepe of death, what dreames may come,\\nWhen we haue shuffel'd off this mortall coile,\\nMust giue vs pawse. There's the respect\\nThat makes Calamity of so long life:\\nFor who would beare the Whips and Scornes of time,\\nThe Oppressors wrong, the poore mans Contumely,\\nThe pangs of dispriz'd Loue, the Lawes delay,\\nThe insolence of Office, and the Spurnes\\nThat patient merit of the vnworthy takes,\\nWhen he himselfe might his Quietus make\\nWith a bare Bodkin? Who would these Fardles beare\\nTo grunt and sweat vnder a weary life,\\nBut that the dread of something after death,\\nThe vndiscouered Countrey, from whose Borne\\nNo Traueller returnes, Puzels the will,\\nAnd makes vs rather beare those illes we haue,\\nThen flye to others that we know not of.\\nThus Conscience does make Cowards of vs all,\\nAnd thus the Natiue hew of Resolution\\nIs sicklied o're, with the pale cast of Thought,\\nAnd enterprizes of great pith and moment,\\nWith this regard their Currants turne away,\\nAnd loose the name of Action. Soft you now,\\nThe faire Ophelia? Nimph, in thy Orizons\\nBe all my sinnes remembred\\n\\n   Ophe. Good my Lord,\\nHow does your Honor for this many a day?\\n  Ham. I humbly thanke you: well, well, well\\n\\n   Ophe. My Lord, I haue Remembrances of yours,\\nThat I haue longed long to re-deliuer.\\nI pray you now, receiue them\\n\\n   Ham. No, no, I neuer gaue you ought\\n\\n   Ophe. My honor'd Lord, I know right well you did,\\nAnd with them words of so sweet breath compos'd,\\nAs made the things more rich, then perfume left:\\nTake these againe, for to the Noble minde\\nRich gifts wax poore, when giuers proue vnkinde.\\nThere my Lord\\n\\n   Ham. Ha, ha: Are you honest?\\n  Ophe. My Lord\\n\\n   Ham. Are you faire?\\n  Ophe. What meanes your Lordship?\\n  Ham. That if you be honest and faire, your Honesty\\nshould admit no discourse to your Beautie\\n\\n   Ophe. Could Beautie my Lord, haue better Comerce\\nthen your Honestie?\\n  Ham. I trulie: for the power of Beautie, will sooner\\ntransforme Honestie from what is, to a Bawd, then the\\nforce of Honestie can translate Beautie into his likenesse.\\nThis was sometime a Paradox, but now the time giues it\\nproofe. I did loue you once\\n\\n   Ophe. Indeed my Lord, you made me beleeue so\\n\\n   Ham. You should not haue beleeued me. For vertue\\ncannot so innocculate our old stocke, but we shall rellish\\nof it. I loued you not\\n\\n   Ophe. I was the more deceiued\\n\\n   Ham. Get thee to a Nunnerie. Why would'st thou\\nbe a breeder of Sinners? I am my selfe indifferent honest,\\nbut yet I could accuse me of such things, that it were better\\nmy Mother had not borne me. I am very prowd, reuengefull,\\nAmbitious, with more offences at my becke,\\nthen I haue thoughts to put them in imagination, to giue\\nthem shape, or time to acte them in. What should such\\nFellowes as I do, crawling betweene Heauen and Earth.\\nWe are arrant Knaues all, beleeue none of vs. Goe thy\\nwayes to a Nunnery. Where's your Father?\\n  Ophe. At home, my Lord\\n\\n   Ham. Let the doores be shut vpon him, that he may\\nplay the Foole no way, but in's owne house. Farewell\\n\\n   Ophe. O helpe him, you sweet Heauens\\n\\n   Ham. If thou doest Marry, Ile giue thee this Plague\\nfor thy Dowrie. Be thou as chast as Ice, as pure as Snow,\\nthou shalt not escape Calumny. Get thee to a Nunnery.\\nGo, Farewell. Or if thou wilt needs Marry, marry a fool:\\nfor Wise men know well enough, what monsters you\\nmake of them. To a Nunnery go, and quickly too. Farwell\\n\\n   Ophe. O heauenly Powers, restore him\\n\\n   Ham. I haue heard of your pratlings too wel enough.\\nGod has giuen you one pace, and you make your selfe another:\\nyou gidge, you amble, and you lispe, and nickname\\nGods creatures, and make your Wantonnesse, your Ignorance.\\nGo too, Ile no more on't, it hath made me mad.\\nI say, we will haue no more Marriages. Those that are\\nmarried already, all but one shall liue, the rest shall keep\\nas they are. To a Nunnery, go.\\n\\nExit Hamlet.\\n\\n  Ophe. O what a Noble minde is heere o're-throwne?\\nThe Courtiers, Soldiers, Schollers: Eye, tongue, sword,\\nTh' expectansie and Rose of the faire State,\\nThe glasse of Fashion, and the mould of Forme,\\nTh' obseru'd of all Obseruers, quite, quite downe.\\nHaue I of Ladies most deiect and wretched,\\nThat suck'd the Honie of his Musicke Vowes:\\nNow see that Noble, and most Soueraigne Reason,\\nLike sweet Bels iangled out of tune, and harsh,\\nThat vnmatch'd Forme and Feature of blowne youth,\\nBlasted with extasie. Oh woe is me,\\nT'haue seene what I haue seene: see what I see.\\nEnter King, and Polonius.\\n\\n  King. Loue? His affections do not that way tend,\\nNor what he spake, though it lack'd Forme a little,\\nWas not like Madnesse. There's something in his soule?\\nO're which his Melancholly sits on brood,\\nAnd I do doubt the hatch, and the disclose\\nWill be some danger, which to preuent\\nI haue in quicke determination\\nThus set it downe. He shall with speed to England\\nFor the demand of our neglected Tribute:\\nHaply the Seas and Countries different\\nWith variable Obiects, shall expell\\nThis something setled matter in his heart:\\nWhereon his Braines still beating, puts him thus\\nFrom fashion of himselfe. What thinke you on't?\\n  Pol. It shall do well. But yet do I beleeue\\nThe Origin and Commencement of this greefe\\nSprung from neglected loue. How now Ophelia?\\nYou neede not tell vs, what Lord Hamlet saide,\\nWe heard it all. My Lord, do as you please,\\nBut if you hold it fit after the Play,\\nLet his Queene Mother all alone intreat him\\nTo shew his Greefes: let her be round with him,\\nAnd Ile be plac'd so, please you in the eare\\nOf all their Conference. If she finde him not,\\nTo England send him: Or confine him where\\nYour wisedome best shall thinke\\n\\n   King. It shall be so:\\nMadnesse in great Ones, must not vnwatch'd go.\\n\\nExeunt.\\n\\nEnter Hamlet, and two or three of the Players.\\n\\n  Ham. Speake the Speech I pray you, as I pronounc'd\\nit to you trippingly on the Tongue: But if you mouth it,\\nas many of your Players do, I had as liue the Town-Cryer\\nhad spoke my Lines: Nor do not saw the Ayre too much\\nyour hand thus, but vse all gently; for in the verie Torrent,\\nTempest, and (as I say) the Whirle-winde of\\nPassion, you must acquire and beget a Temperance that\\nmay giue it Smoothnesse. O it offends mee to the Soule,\\nto see a robustious Pery-wig-pated Fellow, teare a Passion\\nto tatters, to verie ragges, to split the eares of the\\nGroundlings: who (for the most part) are capeable of\\nnothing, but inexplicable dumbe shewes, & noise: I could\\nhaue such a Fellow whipt for o're-doing Termagant: it\\noutHerod's Herod. Pray you auoid it\\n\\n   Player. I warrant your Honor\\n\\n   Ham. Be not too tame neyther: but let your owne\\nDiscretion be your Tutor. Sute the Action to the Word,\\nthe Word to the Action, with this speciall obseruance:\\nThat you ore-stop not the modestie of Nature; for any\\nthing so ouer-done, is fro[m] the purpose of Playing, whose\\nend both at the first and now, was and is, to hold as 'twer\\nthe Mirrour vp to Nature; to shew Vertue her owne\\nFeature, Scorne her owne Image, and the verie Age and\\nBodie of the Time, his forme and pressure. Now, this\\nouer-done, or come tardie off, though it make the vnskilfull\\nlaugh, cannot but make the Iudicious greeue; The\\ncensure of the which One, must in your allowance o'reway\\na whole Theater of Others. Oh, there bee Players\\nthat I haue seene Play, and heard others praise, and that\\nhighly (not to speake it prophanely) that neyther hauing\\nthe accent of Christians, nor the gate of Christian, Pagan,\\nor Norman, haue so strutted and bellowed, that I haue\\nthought some of Natures Iouerney-men had made men,\\nand not made them well, they imitated Humanity so abhominably\\n\\n   Play. I hope we haue reform'd that indifferently with\\nvs, Sir\\n\\n   Ham. O reforme it altogether. And let those that\\nplay your Clownes, speake no more then is set downe for\\nthem. For there be of them, that will themselues laugh,\\nto set on some quantitie of barren Spectators to laugh\\ntoo, though in the meane time, some necessary Question\\nof the Play be then to be considered: that's Villanous, &\\nshewes a most pittifull Ambition in the Foole that vses\\nit. Go make you readie.\\n\\nExit Players.\\n\\nEnter Polonius, Rosincrance, and Guildensterne.\\n\\nHow now my Lord,\\nWill the King heare this peece of Worke?\\n  Pol. And the Queene too, and that presently\\n\\n   Ham. Bid the Players make hast.\\n\\nExit Polonius.\\n\\nWill you two helpe to hasten them?\\n  Both. We will my Lord.\\n\\nExeunt.\\n\\nEnter Horatio.\\n\\n  Ham. What hoa, Horatio?\\n  Hora. Heere sweet Lord, at your Seruice\\n\\n   Ham. Horatio, thou art eene as iust a man\\nAs ere my Conuersation coap'd withall\\n\\n   Hora. O my deere Lord\\n\\n   Ham. Nay, do not thinke I flatter:\\nFor what aduancement may I hope from thee,\\nThat no Reuennew hast, but thy good spirits\\nTo feed & cloath thee. Why shold the poor be flatter'd?\\nNo, let the Candied tongue, like absurd pompe,\\nAnd crooke the pregnant Hindges of the knee,\\nWhere thrift may follow faining? Dost thou heare,\\nSince my deere Soule was Mistris of my choyse,\\nAnd could of men distinguish, her election\\nHath seal'd thee for her selfe. For thou hast bene\\nAs one in suffering all, that suffers nothing.\\nA man that Fortunes buffets, and Rewards\\nHath 'tane with equall Thankes. And blest are those,\\nWhose Blood and Iudgement are so well co-mingled,\\nThat they are not a Pipe for Fortunes finger.\\nTo sound what stop she please. Giue me that man,\\nThat is not Passions Slaue, and I will weare him\\nIn my hearts Core. I, in my Heart of heart,\\nAs I do thee. Something too much of this.\\nThere is a Play to night to before the King.\\nOne Scoene of it comes neere the Circumstance\\nWhich I haue told thee, of my Fathers death.\\nI prythee, when thou see'st that Acte a-foot,\\nEuen with the verie Comment of my Soule\\nObserue mine Vnkle: If his occulted guilt,\\nDo not it selfe vnkennell in one speech,\\nIt is a damned Ghost that we haue seene:\\nAnd my Imaginations are as foule\\nAs Vulcans Stythe. Giue him needfull note,\\nFor I mine eyes will riuet to his Face:\\nAnd after we will both our iudgements ioyne,\\nTo censure of his seeming\\n\\n   Hora. Well my Lord.\\nIf he steale ought the whil'st this Play is Playing,\\nAnd scape detecting, I will pay the Theft.\\nEnter King, Queene, Polonius, Ophelia, Rosincrance,\\nGuildensterne, and\\nother Lords attendant with his Guard carrying Torches. Danish\\nMarch. Sound\\na Flourish.\\n\\n  Ham. They are comming to the Play: I must be idle.\\nGet you a place\\n\\n   King. How fares our Cosin Hamlet?\\n  Ham. Excellent Ifaith, of the Camelions dish: I eate\\nthe Ayre promise-cramm'd, you cannot feed Capons so\\n\\n   King. I haue nothing with this answer Hamlet, these\\nwords are not mine\\n\\n   Ham. No, nor mine. Now my Lord, you plaid once\\ni'th' Vniuersity, you say?\\n  Polon. That I did my Lord, and was accounted a good\\nActor\\n\\n   Ham. And what did you enact?\\n  Pol. I did enact Iulius Caesar, I was kill'd i'th' Capitol:\\nBrutus kill'd me\\n\\n   Ham. It was a bruite part of him, to kill so Capitall a\\nCalfe there. Be the Players ready?\\n  Rosin. I my Lord, they stay vpon your patience\\n\\n   Qu. Come hither my good Hamlet, sit by me\\n\\n   Ha. No good Mother, here's Mettle more attractiue\\n\\n   Pol. Oh ho, do you marke that?\\n  Ham. Ladie, shall I lye in your Lap?\\n  Ophe. No my Lord\\n\\n   Ham. I meane, my Head vpon your Lap?\\n  Ophe. I my Lord\\n\\n   Ham. Do you thinke I meant Country matters?\\n  Ophe. I thinke nothing, my Lord\\n\\n   Ham. That's a faire thought to ly betweene Maids legs\\n  Ophe. What is my Lord?\\n  Ham. Nothing\\n\\n   Ophe. You are merrie, my Lord?\\n  Ham. Who I?\\n  Ophe. I my Lord\\n\\n   Ham. Oh God, your onely Iigge-maker: what should\\na man do, but be merrie. For looke you how cheerefully\\nmy Mother lookes, and my Father dyed within's two\\nHoures\\n\\n   Ophe. Nay, 'tis twice two moneths, my Lord\\n\\n   Ham. So long? Nay then let the Diuel weare blacke,\\nfor Ile haue a suite of Sables. Oh Heauens! dye two moneths\\nago, and not forgotten yet? Then there's hope, a\\ngreat mans Memorie, may out-liue his life halfe a yeare:\\nBut byrlady he must builde Churches then: or else shall\\nhe suffer not thinking on, with the Hoby-horsse, whose\\nEpitaph is, For o, For o, the Hoby-horse is forgot.\\n\\nHoboyes play. The dumbe shew enters.\\n\\nEnter a King and Queene, very louingly; the Queene embracing\\nhim. She\\nkneeles, and makes shew of Protestation vnto him. He takes her\\nvp, and\\ndeclines his head vpon her neck. Layes him downe vpon a Banke\\nof Flowers.\\nShe seeing him a-sleepe, leaues him. Anon comes in a Fellow,\\ntakes off his\\nCrowne, kisses it, and powres poyson in the Kings eares, and\\nExits. The\\nQueene returnes, findes the King dead, and makes passionate\\nAction. The\\nPoysoner, with some two or three Mutes comes in againe, seeming\\nto lament\\nwith her. The dead body is carried away: The Poysoner Wooes the\\nQueene with\\nGifts, she seemes loath and vnwilling awhile, but in the end,\\naccepts his\\nloue.\\n\\nExeunt.\\n\\n  Ophe. What meanes this, my Lord?\\n  Ham. Marry this is Miching Malicho, that meanes\\nMischeefe\\n\\n   Ophe. Belike this shew imports the Argument of the\\nPlay?\\n  Ham. We shall know by these Fellowes: the Players\\ncannot keepe counsell, they'l tell all\\n\\n   Ophe. Will they tell vs what this shew meant?\\n  Ham. I, or any shew that you'l shew him. Bee not\\nyou asham'd to shew, hee'l not shame to tell you what it\\nmeanes\\n\\n   Ophe. You are naught, you are naught, Ile marke the\\nPlay.\\nEnter Prologue.\\n\\nFor vs, and for our Tragedie,\\nHeere stooping to your Clemencie:\\nWe begge your hearing Patientlie\\n\\n   Ham. Is this a Prologue, or the Poesie of a Ring?\\n  Ophe. 'Tis briefe my Lord\\n\\n   Ham. As Womans loue.\\nEnter King and his Queene.\\n\\n  King. Full thirtie times hath Phoebus Cart gon round,\\nNeptunes salt Wash, and Tellus Orbed ground:\\nAnd thirtie dozen Moones with borrowed sheene,\\nAbout the World haue times twelue thirties beene,\\nSince loue our hearts, and Hymen did our hands\\nVnite comutuall, in most sacred Bands\\n\\n   Bap. So many iournies may the Sunne and Moone\\nMake vs againe count o're, ere loue be done.\\nBut woe is me, you are so sicke of late,\\nSo farre from cheere, and from your former state,\\nThat I distrust you: yet though I distrust,\\nDiscomfort you (my Lord) it nothing must:\\nFor womens Feare and Loue, holds quantitie,\\nIn neither ought, or in extremity:\\nNow what my loue is, proofe hath made you know,\\nAnd as my Loue is siz'd, my Feare is so\\n\\n   King. Faith I must leaue thee Loue, and shortly too:\\nMy operant Powers my Functions leaue to do:\\nAnd thou shalt liue in this faire world behinde,\\nHonour'd, belou'd, and haply, one as kinde.\\nFor Husband shalt thou-\\n  Bap. Oh confound the rest:\\nSuch Loue, must needs be Treason in my brest:\\nIn second Husband, let me be accurst,\\nNone wed the second, but who kill'd the first\\n\\n   Ham. Wormwood, Wormwood\\n\\n   Bapt. The instances that second Marriage moue,\\nAre base respects of Thrift, but none of Loue.\\nA second time, I kill my Husband dead,\\nWhen second Husband kisses me in Bed\\n\\n   King. I do beleeue you. Think what now you speak:\\nBut what we do determine, oft we breake:\\nPurpose is but the slaue to Memorie,\\nOf violent Birth, but poore validitie:\\nWhich now like Fruite vnripe stickes on the Tree,\\nBut fall vnshaken, when they mellow bee.\\nMost necessary 'tis, that we forget\\nTo pay our selues, what to our selues is debt:\\nWhat to our selues in passion we propose,\\nThe passion ending, doth the purpose lose.\\nThe violence of other Greefe or Ioy,\\nTheir owne ennactors with themselues destroy:\\nWhere Ioy most Reuels, Greefe doth most lament;\\nGreefe ioyes, Ioy greeues on slender accident.\\nThis world is not for aye, nor 'tis not strange\\nThat euen our Loues should with our Fortunes change.\\nFor 'tis a question left vs yet to proue,\\nWhether Loue lead Fortune, or else Fortune Loue.\\nThe great man downe, you marke his fauourites flies,\\nThe poore aduanc'd, makes Friends of Enemies:\\nAnd hitherto doth Loue on Fortune tend,\\nFor who not needs, shall neuer lacke a Frend:\\nAnd who in want a hollow Friend doth try,\\nDirectly seasons him his Enemie.\\nBut orderly to end, where I begun,\\nOur Willes and Fates do so contrary run,\\nThat our Deuices still are ouerthrowne,\\nOur thoughts are ours, their ends none of our owne.\\nSo thinke thou wilt no second Husband wed.\\nBut die thy thoughts, when thy first Lord is dead\\n\\n   Bap. Nor Earth to giue me food, nor Heauen light,\\nSport and repose locke from me day and night:\\nEach opposite that blankes the face of ioy,\\nMeet what I would haue well, and it destroy:\\nBoth heere, and hence, pursue me lasting strife,\\nIf once a Widdow, euer I be Wife\\n\\n   Ham. If she should breake it now\\n\\n   King. 'Tis deepely sworne:\\nSweet, leaue me heere a while,\\nMy spirits grow dull, and faine I would beguile\\nThe tedious day with sleepe\\n\\n   Qu. Sleepe rocke thy Braine,\\n\\nSleepes\\n\\nAnd neuer come mischance betweene vs twaine.\\n\\nExit\\n\\n  Ham. Madam, how like you this Play?\\n  Qu. The Lady protests to much me thinkes\\n\\n   Ham. Oh but shee'l keepe her word\\n\\n   King. Haue you heard the Argument, is there no Offence\\nin't?\\n  Ham. No, no, they do but iest, poyson in iest, no Offence\\ni'th' world\\n\\n   King. What do you call the Play?\\n  Ham. The Mouse-trap: Marry how? Tropically:\\nThis Play is the Image of a murder done in Vienna: Gonzago\\nis the Dukes name, his wife Baptista: you shall see\\nanon: 'tis a knauish peece of worke: But what o'that?\\nYour Maiestie, and wee that haue free soules, it touches\\nvs not: let the gall'd iade winch: our withers are vnrung.\\nEnter Lucianus.\\n\\nThis is one Lucianus nephew to the King\\n\\n   Ophe. You are a good Chorus, my Lord\\n\\n   Ham. I could interpret betweene you and your loue:\\nif I could see the Puppets dallying\\n\\n   Ophe. You are keene my Lord, you are keene\\n\\n   Ham. It would cost you a groaning, to take off my\\nedge\\n\\n   Ophe. Still better and worse\\n\\n   Ham. So you mistake Husbands.\\nBegin Murderer. Pox, leaue thy damnable Faces, and\\nbegin. Come, the croaking Rauen doth bellow for Reuenge\\n\\n   Lucian. Thoughts blacke, hands apt,\\nDrugges fit, and Time agreeing:\\nConfederate season, else, no Creature seeing:\\nThou mixture ranke, of Midnight Weeds collected,\\nWith Hecats Ban, thrice blasted, thrice infected,\\nThy naturall Magicke, and dire propertie,\\nOn wholsome life, vsurpe immediately.\\n\\nPowres the poyson in his eares.\\n\\n  Ham. He poysons him i'th' Garden for's estate: His\\nname's Gonzago: the Story is extant and writ in choyce\\nItalian. You shall see anon how the Murtherer gets the\\nloue of Gonzago's wife\\n\\n   Ophe. The King rises\\n\\n   Ham. What, frighted with false fire\\n\\n   Qu. How fares my Lord?\\n  Pol. Giue o're the Play\\n\\n   King. Giue me some Light. Away\\n\\n   All. Lights, Lights, Lights.\\n\\nExeunt.\\n\\nManet Hamlet & Horatio.\\n\\n  Ham. Why let the strucken Deere go weepe,\\nThe Hart vngalled play:\\nFor some must watch, while some must sleepe;\\nSo runnes the world away.\\nWould not this Sir, and a Forrest of Feathers, if the rest of\\nmy Fortunes turne Turke with me; with two Prouinciall\\nRoses on my rac'd Shooes, get me a Fellowship in a crie\\nof Players sir\\n\\n   Hor. Halfe a share\\n\\n   Ham. A whole one I,\\nFor thou dost know: Oh Damon deere,\\nThis Realme dismantled was of Ioue himselfe,\\nAnd now reignes heere.\\nA verie verie Paiocke\\n\\n   Hora. You might haue Rim'd\\n\\n   Ham. Oh good Horatio, Ile take the Ghosts word for\\na thousand pound. Did'st perceiue?\\n  Hora. Verie well my Lord\\n\\n   Ham. Vpon the talke of the poysoning?\\n  Hora. I did verie well note him.\\nEnter Rosincrance and Guildensterne.\\n\\n  Ham. Oh, ha? Come some Musick. Come y Recorders:\\nFor if the King like not the Comedie,\\nWhy then belike he likes it not perdie.\\nCome some Musicke\\n\\n   Guild. Good my Lord, vouchsafe me a word with you\\n\\n   Ham. Sir, a whole History\\n\\n   Guild. The King, sir\\n\\n   Ham. I sir, what of him?\\n  Guild. Is in his retyrement, maruellous distemper'd\\n\\n   Ham. With drinke Sir?\\n  Guild. No my Lord, rather with choller\\n\\n   Ham. Your wisedome should shew it selfe more richer,\\nto signifie this to his Doctor: for for me to put him\\nto his Purgation, would perhaps plundge him into farre\\nmore Choller\\n\\n   Guild. Good my Lord put your discourse into some\\nframe, and start not so wildely from my affayre\\n\\n   Ham. I am tame Sir, pronounce\\n\\n   Guild. The Queene your Mother, in most great affliction\\nof spirit, hath sent me to you\\n\\n   Ham. You are welcome\\n\\n   Guild. Nay, good my Lord, this courtesie is not of\\nthe right breed. If it shall please you to make me a wholsome\\nanswer, I will doe your Mothers command'ment:\\nif not, your pardon, and my returne shall bee the end of\\nmy Businesse\\n\\n   Ham. Sir, I cannot\\n\\n   Guild. What, my Lord?\\n  Ham. Make you a wholsome answere: my wits diseas'd.\\nBut sir, such answers as I can make, you shal command:\\nor rather you say, my Mother: therfore no more\\nbut to the matter. My Mother you say\\n\\n   Rosin. Then thus she sayes: your behauior hath stroke\\nher into amazement, and admiration\\n\\n   Ham. Oh wonderfull Sonne, that can so astonish a\\nMother. But is there no sequell at the heeles of this Mothers\\nadmiration?\\n  Rosin. She desires to speake with you in her Closset,\\nere you go to bed\\n\\n   Ham. We shall obey, were she ten times our Mother.\\nHaue you any further Trade with vs?\\n  Rosin. My Lord, you once did loue me\\n\\n   Ham. So I do still, by these pickers and stealers\\n\\n   Rosin. Good my Lord, what is your cause of distemper?\\nYou do freely barre the doore of your owne Libertie,\\nif you deny your greefes to your Friend\\n\\n   Ham. Sir I lacke Aduancement\\n\\n   Rosin. How can that be, when you haue the voyce of\\nthe King himselfe, for your Succession in Denmarke?\\n  Ham. I, but while the grasse growes, the Prouerbe is\\nsomething musty.\\nEnter one with a Recorder.\\n\\nO the Recorder. Let me see, to withdraw with you, why\\ndo you go about to recouer the winde of mee, as if you\\nwould driue me into a toyle?\\n  Guild. O my Lord, if my Dutie be too bold, my loue\\nis too vnmannerly\\n\\n   Ham. I do not well vnderstand that. Will you play\\nvpon this Pipe?\\n  Guild. My Lord, I cannot\\n\\n   Ham. I pray you\\n\\n   Guild. Beleeue me, I cannot\\n\\n   Ham. I do beseech you\\n\\n   Guild. I know no touch of it, my Lord\\n\\n   Ham. 'Tis as easie as lying: gouerne these Ventiges\\nwith your finger and thumbe, giue it breath with your\\nmouth, and it will discourse most excellent Musicke.\\nLooke you, these are the stoppes\\n\\n   Guild. But these cannot I command to any vtterance\\nof hermony, I haue not the skill\\n\\n   Ham. Why looke you now, how vnworthy a thing\\nyou make of me: you would play vpon mee; you would\\nseeme to know my stops: you would pluck out the heart\\nof my Mysterie; you would sound mee from my lowest\\nNote, to the top of my Compasse: and there is much Musicke,\\nexcellent Voice, in this little Organe, yet cannot\\nyou make it. Why do you thinke, that I am easier to bee\\nplaid on, then a Pipe? Call me what Instrument you will,\\nthough you can fret me, you cannot play vpon me. God\\nblesse you Sir.\\nEnter Polonius.\\n\\n  Polon. My Lord; the Queene would speak with you,\\nand presently\\n\\n   Ham. Do you see that Clowd? that's almost in shape\\nlike a Camell\\n\\n   Polon. By'th' Masse, and it's like a Camell indeed\\n\\n   Ham. Me thinkes it is like a Weazell\\n\\n   Polon. It is back'd like a Weazell\\n\\n   Ham. Or like a Whale?\\n  Polon. Verie like a Whale\\n\\n   Ham. Then will I come to my Mother, by and by:\\nThey foole me to the top of my bent.\\nI will come by and by\\n\\n   Polon. I will say so.\\nEnter.\\n\\n  Ham. By and by, is easily said. Leaue me Friends:\\n'Tis now the verie witching time of night,\\nWhen Churchyards yawne, and Hell it selfe breaths out\\nContagion to this world. Now could I drink hot blood,\\nAnd do such bitter businesse as the day\\nWould quake to looke on. Soft now, to my Mother:\\nOh Heart, loose not thy Nature; let not euer\\nThe Soule of Nero, enter this firme bosome:\\nLet me be cruell, not vnnaturall,\\nI will speake Daggers to her, but vse none:\\nMy Tongue and Soule in this be Hypocrites.\\nHow in my words someuer she be shent,\\nTo giue them Seales, neuer my Soule consent.\\nEnter King, Rosincrance, and Guildensterne.\\n\\n  King. I like him not, nor stands it safe with vs,\\nTo let his madnesse range. Therefore prepare you,\\nI your Commission will forthwith dispatch,\\nAnd he to England shall along with you:\\nThe termes of our estate, may not endure\\nHazard so dangerous as doth hourely grow\\nOut of his Lunacies\\n\\n   Guild. We will our selues prouide:\\nMost holie and Religious feare it is\\nTo keepe those many many bodies safe\\nThat liue and feede vpon your Maiestie\\n\\n   Rosin. The single\\nAnd peculiar life is bound\\nWith all the strength and Armour of the minde,\\nTo keepe it selfe from noyance: but much more,\\nThat Spirit, vpon whose spirit depends and rests\\nThe liues of many, the cease of Maiestie\\nDies not alone; but like a Gulfe doth draw\\nWhat's neere it, with it. It is a massie wheele\\nFixt on the Somnet of the highest Mount.\\nTo whose huge Spoakes, ten thousand lesser things\\nAre mortiz'd and adioyn'd: which when it falles,\\nEach small annexment, pettie consequence\\nAttends the boystrous Ruine. Neuer alone\\nDid the King sighe, but with a generall grone\\n\\n   King. Arme you, I pray you to this speedie Voyage;\\nFor we will Fetters put vpon this feare,\\nWhich now goes too free-footed\\n\\n   Both. We will haste vs.\\n\\nExeunt. Gent.\\n\\nEnter Polonius.\\n\\n  Pol. My Lord, he's going to his Mothers Closset:\\nBehinde the Arras Ile conuey my selfe\\nTo heare the Processe. Ile warrant shee'l tax him home,\\nAnd as you said, and wisely was it said,\\n'Tis meete that some more audience then a Mother,\\nSince Nature makes them partiall, should o're-heare\\nThe speech of vantage. Fare you well my Liege,\\nIle call vpon you ere you go to bed,\\nAnd tell you what I know\\n\\n   King. Thankes deere my Lord.\\nOh my offence is ranke, it smels to heauen,\\nIt hath the primall eldest curse vpon't,\\nA Brothers murther. Pray can I not,\\nThough inclination be as sharpe as will:\\nMy stronger guilt, defeats my strong intent,\\nAnd like a man to double businesse bound,\\nI stand in pause where I shall first begin,\\nAnd both neglect; what if this cursed hand\\nWere thicker then it selfe with Brothers blood,\\nIs there not Raine enough in the sweet Heauens\\nTo wash it white as Snow? Whereto serues mercy,\\nBut to confront the visage of Offence?\\nAnd what's in Prayer, but this two-fold force,\\nTo be fore-stalled ere we come to fall,\\nOr pardon'd being downe? Then Ile looke vp,\\nMy fault is past. But oh, what forme of Prayer\\nCan serue my turne? Forgiue me my foule Murther:\\nThat cannot be, since I am still possest\\nOf those effects for which I did the Murther.\\nMy Crowne, mine owne Ambition, and my Queene:\\nMay one be pardon'd, and retaine th' offence?\\nIn the corrupted currants of this world,\\nOffences gilded hand may shoue by Iustice,\\nAnd oft 'tis seene, the wicked prize it selfe\\nBuyes out the Law; but 'tis not so aboue,\\nThere is no shuffling, there the Action lyes\\nIn his true Nature, and we our selues compell'd\\nEuen to the teeth and forehead of our faults,\\nTo giue in euidence. What then? What rests?\\nTry what Repentance can. What can it not?\\nYet what can it, when one cannot repent?\\nOh wretched state! Oh bosome, blacke as death!\\nOh limed soule, that strugling to be free,\\nArt more ingag'd: Helpe Angels, make assay:\\nBow stubborne knees, and heart with strings of Steele,\\nBe soft as sinewes of the new-borne Babe,\\nAll may be well.\\nEnter Hamlet.\\n\\n  Ham. Now might I do it pat, now he is praying,\\nAnd now Ile doo't, and so he goes to Heauen,\\nAnd so am I reueng'd: that would be scann'd,\\nA Villaine killes my Father, and for that\\nI his foule Sonne, do this same Villaine send\\nTo heauen. Oh this is hyre and Sallery, not Reuenge.\\nHe tooke my Father grossely, full of bread,\\nWith all his Crimes broad blowne, as fresh as May,\\nAnd how his Audit stands, who knowes, saue Heauen:\\nBut in our circumstance and course of thought\\n'Tis heauie with him: and am I then reueng'd,\\nTo take him in the purging of his Soule,\\nWhen he is fit and season'd for his passage? No.\\nVp Sword, and know thou a more horrid hent\\nWhen he is drunke asleepe: or in his Rage,\\nOr in th' incestuous pleasure of his bed,\\nAt gaming, swearing, or about some acte\\nThat ha's no rellish of Saluation in't,\\nThen trip him, that his heeles may kicke at Heauen,\\nAnd that his Soule may be as damn'd and blacke\\nAs Hell, whereto it goes. My Mother stayes,\\nThis Physicke but prolongs thy sickly dayes.\\nEnter.\\n\\n  King. My words flye vp, my thoughts remain below,\\nWords without thoughts, neuer to Heauen go.\\nEnter.\\n\\nEnter Queene and Polonius.\\n\\n  Pol. He will come straight:\\nLooke you lay home to him,\\nTell him his prankes haue been too broad to beare with,\\nAnd that your Grace hath screen'd, and stoode betweene\\nMuch heate, and him. Ile silence me e'ene heere:\\nPray you be round with him\\n\\n   Ham. within. Mother, mother, mother\\n\\n   Qu. Ile warrant you, feare me not.\\nWithdraw, I heare him coming.\\nEnter Hamlet.\\n\\n  Ham. Now Mother, what's the matter?\\n  Qu. Hamlet, thou hast thy Father much offended\\n\\n\\n   Ham. Mother, you haue my Father much offended\\n\\n   Qu. Come, come, you answer with an idle tongue\\n\\n   Ham. Go, go, you question with an idle tongue\\n\\n   Qu. Why how now Hamlet?\\n  Ham. Whats the matter now?\\n  Qu. Haue you forgot me?\\n  Ham. No by the Rood, not so:\\nYou are the Queene, your Husbands Brothers wife,\\nBut would you were not so. You are my Mother\\n\\n   Qu. Nay, then Ile set those to you that can speake\\n\\n   Ham. Come, come, and sit you downe, you shall not\\nboudge:\\nYou go not till I set you vp a glasse,\\nWhere you may see the inmost part of you?\\n  Qu. What wilt thou do? thou wilt not murther me?\\nHelpe, helpe, hoa\\n\\n   Pol. What hoa, helpe, helpe, helpe\\n\\n   Ham. How now, a Rat? dead for a Ducate, dead\\n\\n   Pol. Oh I am slaine.\\n\\nKilles Polonius\\n\\n   Qu. Oh me, what hast thou done?\\n  Ham. Nay I know not, is it the King?\\n  Qu. Oh what a rash, and bloody deed is this?\\n  Ham. A bloody deed, almost as bad good Mother,\\nAs kill a King, and marrie with his Brother\\n\\n   Qu. As kill a King?\\n  Ham. I Lady, 'twas my word.\\nThou wretched, rash, intruding foole farewell,\\nI tooke thee for thy Betters, take thy Fortune,\\nThou find'st to be too busie, is some danger.\\nLeaue wringing of your hands, peace, sit you downe,\\nAnd let me wring your heart, for so I shall\\nIf it be made of penetrable stuffe;\\nIf damned Custome haue not braz'd it so,\\nThat it is proofe and bulwarke against Sense\\n\\n   Qu. What haue I done, that thou dar'st wag thy tong,\\nIn noise so rude against me?\\n  Ham. Such an Act\\nThat blurres the grace and blush of Modestie,\\nCals Vertue Hypocrite, takes off the Rose\\nFrom the faire forehead of an innocent loue,\\nAnd makes a blister there. Makes marriage vowes\\nAs false as Dicers Oathes. Oh such a deed,\\nAs from the body of Contraction pluckes\\nThe very soule, and sweete Religion makes\\nA rapsidie of words. Heauens face doth glow,\\nYea this solidity and compound masse,\\nWith tristfull visage as against the doome,\\nIs thought-sicke at the act\\n\\n   Qu. Aye me; what act, that roares so lowd, & thunders\\nin the Index\\n\\n   Ham. Looke heere vpon this Picture, and on this,\\nThe counterfet presentment of two Brothers:\\nSee what a grace was seated on his Brow,\\nHyperions curles, the front of Ioue himselfe,\\nAn eye like Mars, to threaten or command\\nA Station, like the Herald Mercurie\\nNew lighted on a heauen-kissing hill:\\nA Combination, and a forme indeed,\\nWhere euery God did seeme to set his Seale,\\nTo giue the world assurance of a man.\\nThis was your Husband. Looke you now what followes.\\nHeere is your Husband, like a Mildew'd eare\\nBlasting his wholsom breath. Haue you eyes?\\nCould you on this faire Mountaine leaue to feed,\\nAnd batten on this Moore? Ha? Haue you eyes?\\nYou cannot call it Loue: For at your age,\\nThe hey-day in the blood is tame, it's humble,\\nAnd waites vpon the Iudgement: and what Iudgement\\nWould step from this, to this? What diuell was't,\\nThat thus hath cousend you at hoodman-blinde?\\nO Shame! where is thy Blush? Rebellious Hell,\\nIf thou canst mutine in a Matrons bones,\\nTo flaming youth, let Vertue be as waxe.\\nAnd melt in her owne fire. Proclaime no shame,\\nWhen the compulsiue Ardure giues the charge,\\nSince Frost it selfe, as actiuely doth burne,\\nAs Reason panders Will\\n\\n   Qu. O Hamlet, speake no more.\\nThou turn'st mine eyes into my very soule,\\nAnd there I see such blacke and grained spots,\\nAs will not leaue their Tinct\\n\\n   Ham. Nay, but to liue\\nIn the ranke sweat of an enseamed bed,\\nStew'd in Corruption; honying and making loue\\nOuer the nasty Stye\\n\\n   Qu. Oh speake to me, no more,\\nThese words like Daggers enter in mine eares.\\nNo more sweet Hamlet\\n\\n   Ham. A Murderer, and a Villaine:\\nA Slaue, that is not twentieth part the tythe\\nOf your precedent Lord. A vice of Kings,\\nA Cutpurse of the Empire and the Rule.\\nThat from a shelfe, the precious Diadem stole,\\nAnd put it in his Pocket\\n\\n   Qu. No more.\\nEnter Ghost.\\n\\n  Ham. A King of shreds and patches.\\nSaue me; and houer o're me with your wings\\nYou heauenly Guards. What would your gracious figure?\\n  Qu. Alas he's mad\\n\\n   Ham. Do you not come your tardy Sonne to chide,\\nThat laps't in Time and Passion, lets go by\\nTh' important acting of your dread command? Oh say\\n\\n   Ghost. Do not forget: this Visitation\\nIs but to whet thy almost blunted purpose.\\nBut looke, Amazement on thy Mother sits;\\nO step betweene her, and her fighting Soule,\\nConceit in weakest bodies, strongest workes.\\nSpeake to her Hamlet\\n\\n   Ham. How is it with you Lady?\\n  Qu. Alas, how is't with you?\\nThat you bend your eye on vacancie,\\nAnd with their corporall ayre do hold discourse.\\nForth at your eyes, your spirits wildely peepe,\\nAnd as the sleeping Soldiours in th' Alarme,\\nYour bedded haire, like life in excrements,\\nStart vp, and stand an end. Oh gentle Sonne,\\nVpon the heate and flame of thy distemper\\nSprinkle coole patience. Whereon do you looke?\\n  Ham. On him, on him: look you how pale he glares,\\nHis forme and cause conioyn'd, preaching to stones,\\nWould make them capeable. Do not looke vpon me,\\nLeast with this pitteous action you conuert\\nMy sterne effects: then what I haue to do,\\nWill want true colour; teares perchance for blood\\n\\n   Qu. To who do you speake this?\\n  Ham. Do you see nothing there?\\n  Qu. Nothing at all, yet all that is I see\\n\\n   Ham. Nor did you nothing heare?\\n  Qu. No, nothing but our selues\\n\\n   Ham. Why look you there: looke how it steals away:\\nMy Father in his habite, as he liued,\\nLooke where he goes euen now out at the Portall.\\nEnter.\\n\\n  Qu. This is the very coynage of your Braine,\\nThis bodilesse Creation extasie is very cunning in\\n\\n   Ham. Extasie?\\nMy Pulse as yours doth temperately keepe time,\\nAnd makes as healthfull Musicke. It is not madnesse\\nThat I haue vttered; bring me to the Test\\nAnd I the matter will re-word: which madnesse\\nWould gamboll from. Mother, for loue of Grace,\\nLay not a flattering Vnction to your soule,\\nThat not your trespasse, but my madnesse speakes:\\nIt will but skin and filme the Vlcerous place,\\nWhil'st ranke Corruption mining all within,\\nInfects vnseene. Confesse your selfe to Heauen,\\nRepent what's past, auoyd what is to come,\\nAnd do not spred the Compost on the Weedes,\\nTo make them ranke. Forgiue me this my Vertue,\\nFor in the fatnesse of this pursie times,\\nVertue it selfe, of Vice must pardon begge,\\nYea courb, and woe, for leaue to do him good\\n\\n   Qu. Oh Hamlet,\\nThou hast cleft my heart in twaine\\n\\n   Ham. O throw away the worser part of it,\\nAnd liue the purer with the other halfe.\\nGood night, but go not to mine Vnkles bed,\\nAssume a Vertue, if you haue it not, refraine to night,\\nAnd that shall lend a kinde of easinesse\\nTo the next abstinence. Once more goodnight,\\nAnd when you are desirous to be blest,\\nIle blessing begge of you. For this same Lord,\\nI do repent: but heauen hath pleas'd it so,\\nTo punish me with this, and this with me,\\nThat I must be their Scourge and Minister.\\nI will bestow him, and will answer well\\nThe death I gaue him: so againe, good night.\\nI must be cruell, onely to be kinde;\\nThus bad begins and worse remaines behinde\\n\\n   Qu. What shall I do?\\n  Ham. Not this by no meanes that I bid you do:\\nLet the blunt King tempt you againe to bed,\\nPinch Wanton on your cheeke, call you his Mouse,\\nAnd let him for a paire of reechie kisses,\\nOr padling in your necke with his damn'd Fingers,\\nMake you to rauell all this matter out,\\nThat I essentially am not in madnesse,\\nBut made in craft. 'Twere good you let him know,\\nFor who that's but a Queene, faire, sober, wise,\\nWould from a Paddocke, from a Bat, a Gibbe,\\nSuch deere concernings hide, Who would do so,\\nNo in despight of Sense and Secrecie,\\nVnpegge the Basket on the houses top:\\nLet the Birds flye, and like the famous Ape\\nTo try Conclusions in the Basket, creepe\\nAnd breake your owne necke downe\\n\\n   Qu. Be thou assur'd, if words be made of breath,\\nAnd breath of life: I haue no life to breath\\nWhat thou hast saide to me\\n\\n   Ham. I must to England, you know that?\\n  Qu. Alacke I had forgot: 'Tis so concluded on\\n\\n   Ham. This man shall set me packing:\\nIle lugge the Guts into the Neighbor roome,\\nMother goodnight. Indeede this Counsellor\\nIs now most still, most secret, and most graue,\\nWho was in life, a foolish prating Knaue.\\nCome sir, to draw toward an end with you.\\nGood night Mother.\\nExit Hamlet tugging in Polonius.\\n\\nEnter King.\\n\\n  King. There's matters in these sighes.\\nThese profound heaues\\nYou must translate; Tis fit we vnderstand them.\\nWhere is your Sonne?\\n  Qu. Ah my good Lord, what haue I seene to night?\\n  King. What Gertrude? How do's Hamlet?\\n  Qu. Mad as the Seas, and winde, when both contend\\nWhich is the Mightier, in his lawlesse fit\\nBehinde the Arras, hearing something stirre,\\nHe whips his Rapier out, and cries a Rat, a Rat,\\nAnd in his brainish apprehension killes\\nThe vnseene good old man\\n\\n   King. Oh heauy deed:\\nIt had bin so with vs had we beene there:\\nHis Liberty is full of threats to all,\\nTo you your selfe, to vs, to euery one.\\nAlas, how shall this bloody deede be answered?\\nIt will be laide to vs, whose prouidence\\nShould haue kept short, restrain'd, and out of haunt,\\nThis mad yong man. But so much was our loue,\\nWe would not vnderstand what was most fit,\\nBut like the Owner of a foule disease,\\nTo keepe it from divulging, let's it feede\\nEuen on the pith of life. Where is he gone?\\n  Qu. To draw apart the body he hath kild,\\nO're whom his very madnesse like some Oare\\nAmong a Minerall of Mettels base\\nShewes it selfe pure. He weepes for what is done\\n\\n   King. Oh Gertrude, come away:\\nThe Sun no sooner shall the Mountaines touch,\\nBut we will ship him hence, and this vilde deed,\\nWe must with all our Maiesty and Skill\\nBoth countenance, and excuse.\\nEnter Ros. & Guild.\\n\\nHo Guildenstern:\\nFriends both go ioyne you with some further ayde:\\nHamlet in madnesse hath Polonius slaine,\\nAnd from his Mother Clossets hath he drag'd him.\\nGo seeke him out, speake faire, and bring the body\\nInto the Chappell. I pray you hast in this.\\nExit Gent.\\n\\nCome Gertrude, wee'l call vp our wisest friends,\\nTo let them know both what we meane to do,\\nAnd what's vntimely done. Oh come away,\\nMy soule is full of discord and dismay.\\n\\nExeunt.\\n\\nEnter Hamlet.\\n\\n  Ham. Safely stowed\\n\\n   Gentlemen within. Hamlet, Lord Hamlet\\n\\n   Ham. What noise? Who cals on Hamlet?\\nOh heere they come.\\nEnter Ros. and Guildensterne.\\n\\n  Ro. What haue you done my Lord with the dead body?\\n  Ham. Compounded it with dust, whereto 'tis Kinne\\n\\n   Rosin. Tell vs where 'tis, that we may take it thence,\\nAnd beare it to the Chappell\\n\\n   Ham. Do not beleeue it\\n\\n   Rosin. Beleeue what?\\n  Ham. That I can keepe your counsell, and not mine\\nowne. Besides, to be demanded of a Spundge, what replication\\nshould be made by the Sonne of a King\\n\\n   Rosin. Take you me for a Spundge, my Lord?\\n  Ham. I sir, that sokes vp the Kings Countenance, his\\nRewards, his Authorities (but such Officers do the King\\nbest seruice in the end. He keepes them like an Ape in\\nthe corner of his iaw, first mouth'd to be last swallowed,\\nwhen he needes what you haue glean'd, it is but squeezing\\nyou, and Spundge you shall be dry againe\\n\\n   Rosin. I vnderstand you not my Lord\\n\\n   Ham. I am glad of it: a knauish speech sleepes in a\\nfoolish eare\\n\\n   Rosin. My Lord, you must tell vs where the body is,\\nand go with vs to the King\\n\\n   Ham. The body is with the King, but the King is not\\nwith the body. The King, is a thing-\\n  Guild. A thing my Lord?\\n  Ham. Of nothing: bring me to him, hide Fox, and all\\nafter.\\n\\nExeunt.\\n\\nEnter King.\\n\\n  King. I haue sent to seeke him, and to find the bodie:\\nHow dangerous is it that this man goes loose:\\nYet must not we put the strong Law on him:\\nHee's loued of the distracted multitude,\\nWho like not in their iudgement, but their eyes:\\nAnd where 'tis so, th' Offenders scourge is weigh'd\\nBut neerer the offence: to beare all smooth, and euen,\\nThis sodaine sending him away, must seeme\\nDeliberate pause, diseases desperate growne,\\nBy desperate appliance are releeued,\\nOr not at all.\\nEnter Rosincrane.\\n\\nHow now? What hath befalne?\\n  Rosin. Where the dead body is bestow'd my Lord,\\nWe cannot get from him\\n\\n   King. But where is he?\\n  Rosin. Without my Lord, guarded to know your\\npleasure\\n\\n   King. Bring him before vs\\n\\n   Rosin. Hoa, Guildensterne? Bring in my Lord.\\nEnter Hamlet and Guildensterne.\\n\\n  King. Now Hamlet, where's Polonius?\\n  Ham. At Supper\\n\\n   King. At Supper? Where?\\n  Ham. Not where he eats, but where he is eaten, a certaine\\nconuocation of wormes are e'ne at him. Your worm\\nis your onely Emperor for diet. We fat all creatures else\\nto fat vs, and we fat our selfe for Magots. Your fat King,\\nand your leane Begger is but variable seruice to dishes,\\nbut to one Table that's the end\\n\\n   King. What dost thou meane by this?\\n  Ham. Nothing but to shew you how a King may go\\na Progresse through the guts of a Begger\\n\\n   King. Where is Polonius\\n\\n   Ham. In heauen, send thither to see. If your Messenger\\nfinde him not there, seeke him i'th other place your\\nselfe: but indeed, if you finde him not this moneth, you\\nshall nose him as you go vp the staires into the Lobby\\n\\n   King. Go seeke him there\\n\\n   Ham. He will stay till ye come\\n\\n   K. Hamlet, this deed of thine, for thine especial safety\\nWhich we do tender, as we deerely greeue\\nFor that which thou hast done, must send thee hence\\nWith fierie Quicknesse. Therefore prepare thy selfe,\\nThe Barke is readie, and the winde at helpe,\\nTh' Associates tend, and euery thing at bent\\nFor England\\n\\n   Ham. For England?\\n  King. I Hamlet\\n\\n   Ham. Good\\n\\n   King. So is it, if thou knew'st our purposes\\n\\n   Ham. I see a Cherube that see's him: but come, for\\nEngland. Farewell deere Mother\\n\\n   King. Thy louing Father Hamlet\\n\\n   Hamlet. My Mother: Father and Mother is man and\\nwife: man & wife is one flesh, and so my mother. Come,\\nfor England.\\n\\nExit\\n\\n  King. Follow him at foote,\\nTempt him with speed aboord:\\nDelay it not, Ile haue him hence to night.\\nAway, for euery thing is Seal'd and done\\nThat else leanes on th' Affaire, pray you make hast.\\nAnd England, if my loue thou holdst at ought,\\nAs my great power thereof may giue thee sense,\\nSince yet thy Cicatrice lookes raw and red\\nAfter the Danish Sword, and thy free awe\\nPayes homage to vs; thou maist not coldly set\\nOur Soueraigne Processe, which imports at full\\nBy Letters coniuring to that effect\\nThe present death of Hamlet. Do it England,\\nFor like the Hecticke in my blood he rages,\\nAnd thou must cure me: Till I know 'tis done,\\nHow ere my happes, my ioyes were ne're begun.\\n\\nExit\\n\\nEnter Fortinbras with an Armie.\\n\\n  For. Go Captaine, from me greet the Danish King,\\nTell him that by his license, Fortinbras\\nClaimes the conueyance of a promis'd March\\nOuer his Kingdome. You know the Rendeuous:\\nIf that his Maiesty would ought with vs,\\nWe shall expresse our dutie in his eye,\\nAnd let him know so\\n\\n   Cap. I will doo't, my Lord\\n\\n   For. Go safely on.\\nEnter.\\n\\nEnter Queene and Horatio.\\n\\n  Qu. I will not speake with her\\n\\n   Hor. She is importunate, indeed distract, her moode\\nwill needs be pittied\\n\\n   Qu. What would she haue?\\n  Hor. She speakes much of her Father; saies she heares\\nThere's trickes i'th' world, and hems, and beats her heart,\\nSpurnes enuiously at Strawes, speakes things in doubt,\\nThat carry but halfe sense: Her speech is nothing,\\nYet the vnshaped vse of it doth moue\\nThe hearers to Collection; they ayme at it,\\nAnd botch the words vp fit to their owne thoughts,\\nWhich as her winkes, and nods, and gestures yeeld them,\\nIndeed would make one thinke there would be thought,\\nThough nothing sure, yet much vnhappily\\n\\n   Qu. 'Twere good she were spoken with,\\nFor she may strew dangerous coniectures\\nIn ill breeding minds. Let her come in.\\nTo my sicke soule (as sinnes true Nature is)\\nEach toy seemes Prologue, to some great amisse,\\nSo full of Artlesse iealousie is guilt,\\nIt spill's it selfe, in fearing to be spilt.\\nEnter Ophelia distracted.\\n\\n  Ophe. Where is the beauteous Maiesty of Denmark\\n\\n   Qu. How now Ophelia?\\n  Ophe. How should I your true loue know from another one?\\nBy his Cockle hat and staffe, and his Sandal shoone\\n\\n   Qu. Alas sweet Lady: what imports this Song?\\n  Ophe. Say you? Nay pray you marke.\\nHe is dead and gone Lady, he is dead and gone,\\nAt his head a grasse-greene Turfe, at his heeles a stone.\\nEnter King.\\n\\n  Qu. Nay but Ophelia\\n\\n   Ophe. Pray you marke.\\nWhite his Shrow'd as the Mountaine Snow\\n\\n   Qu. Alas, looke heere my Lord\\n\\n   Ophe. Larded with sweet Flowers:\\nWhich bewept to the graue did not go,\\nWith true-loue showres\\n\\n   King. How do ye, pretty Lady?\\n  Ophe. Well, God dil'd you. They say the Owle was\\na Bakers daughter. Lord, wee know what we are, but\\nknow not what we may be. God be at your Table\\n\\n   King. Conceit vpon her Father\\n\\n   Ophe. Pray you let's haue no words of this: but when\\nthey aske you what it meanes, say you this:\\nTo morrow is S[aint]. Valentines day, all in the morning betime,\\nAnd I a Maid at your Window, to be your Valentine.\\nThen vp he rose, & don'd his clothes, & dupt the chamber dore,\\nLet in the Maid, that out a Maid, neuer departed more\\n\\n   King. Pretty Ophelia\\n\\n   Ophe. Indeed la? without an oath Ile make an end ont.\\nBy gis, and by S[aint]. Charity,\\nAlacke, and fie for shame:\\nYong men wil doo't, if they come too't,\\nBy Cocke they are too blame.\\nQuoth she before you tumbled me,\\nYou promis'd me to Wed:\\nSo would I ha done by yonder Sunne,\\nAnd thou hadst not come to my bed\\n\\n   King. How long hath she bin thus?\\n  Ophe. I hope all will be well. We must bee patient,\\nbut I cannot choose but weepe, to thinke they should\\nlay him i'th' cold ground: My brother shall knowe of it,\\nand so I thanke you for your good counsell. Come, my\\nCoach: Goodnight Ladies: Goodnight sweet Ladies:\\nGoodnight, goodnight.\\nEnter.\\n\\n  King. Follow her close,\\nGiue her good watch I pray you:\\nOh this is the poyson of deepe greefe, it springs\\nAll from her Fathers death. Oh Gertrude, Gertrude,\\nWhen sorrowes comes, they come not single spies,\\nBut in Battalians. First, her Father slaine,\\nNext your Sonne gone, and he most violent Author\\nOf his owne iust remoue: the people muddied,\\nThicke and vnwholsome in their thoughts, and whispers\\nFor good Polonius death; and we haue done but greenly\\nIn hugger mugger to interre him. Poore Ophelia\\nDiuided from her selfe, and her faire Iudgement,\\nWithout the which we are Pictures, or meere Beasts.\\nLast, and as much containing as all these,\\nHer Brother is in secret come from France,\\nKeepes on his wonder, keepes himselfe in clouds,\\nAnd wants not Buzzers to infect his eare\\nWith pestilent Speeches of his Fathers death,\\nWhere in necessitie of matter Beggard,\\nWill nothing sticke our persons to Arraigne\\nIn eare and eare. O my deere Gertrude, this,\\nLike to a murdering Peece in many places,\\nGiues me superfluous death.\\n\\nA Noise within.\\n\\nEnter a Messenger.\\n\\n  Qu. Alacke, what noyse is this?\\n  King. Where are my Switzers?\\nLet them guard the doore. What is the matter?\\n  Mes. Saue your selfe, my Lord.\\nThe Ocean (ouer-peering of his List)\\nEates not the Flats with more impittious haste\\nThen young Laertes, in a Riotous head,\\nOre-beares your Officers, the rabble call him Lord,\\nAnd as the world were now but to begin,\\nAntiquity forgot, Custome not knowne,\\nThe Ratifiers and props of euery word,\\nThey cry choose we? Laertes shall be King,\\nCaps, hands, and tongues, applaud it to the clouds,\\nLaertes shall be King, Laertes King\\n\\n   Qu. How cheerefully on the false Traile they cry,\\nOh this is Counter you false Danish Dogges.\\n\\nNoise within. Enter Laertes.\\n\\n  King. The doores are broke\\n\\n   Laer. Where is the King, sirs? Stand you all without\\n\\n   All. No, let's come in\\n\\n   Laer. I pray you giue me leaue\\n\\n   Al. We will, we will\\n\\n   Laer. I thanke you: Keepe the doore.\\nOh thou vilde King, giue me my Father\\n\\n   Qu. Calmely good Laertes\\n\\n   Laer. That drop of blood, that calmes\\nProclaimes me Bastard:\\nCries Cuckold to my Father, brands the Harlot\\nEuen heere betweene the chaste vnsmirched brow\\nOf my true Mother\\n\\n   King. What is the cause Laertes,\\nThat thy Rebellion lookes so Gyant-like?\\nLet him go Gertrude: Do not feare our person:\\nThere's such Diuinity doth hedge a King,\\nThat Treason can but peepe to what it would,\\nActs little of his will. Tell me Laertes,\\nWhy thou art thus Incenst? Let him go Gertrude.\\nSpeake man\\n\\n   Laer. Where's my Father?\\n  King. Dead\\n\\n   Qu. But not by him\\n\\n   King. Let him demand his fill\\n\\n   Laer. How came he dead? Ile not be Iuggel'd with.\\nTo hell Allegeance: Vowes, to the blackest diuell.\\nConscience and Grace, to the profoundest Pit.\\nI dare Damnation: to this point I stand,\\nThat both the worlds I giue to negligence,\\nLet come what comes: onely Ile be reueng'd\\nMost throughly for my Father\\n\\n   King. Who shall stay you?\\n  Laer. My Will, not all the world,\\nAnd for my meanes, Ile husband them so well,\\nThey shall go farre with little\\n\\n   King. Good Laertes:\\nIf you desire to know the certaintie\\nOf your deere Fathers death, if writ in your reuenge,\\nThat Soop-stake you will draw both Friend and Foe,\\nWinner and Looser\\n\\n   Laer. None but his Enemies\\n\\n   King. Will you know them then\\n\\n   La. To his good Friends, thus wide Ile ope my Armes:\\nAnd like the kinde Life-rend'ring Politician,\\nRepast them with my blood\\n\\n   King. Why now you speake\\nLike a good Childe, and a true Gentleman.\\nThat I am guiltlesse of your Fathers death,\\nAnd am most sensible in greefe for it,\\nIt shall as leuell to your Iudgement pierce\\nAs day do's to your eye.\\n\\nA noise within. Let her come in.\\n\\nEnter Ophelia.\\n\\n  Laer. How now? what noise is that?\\nOh heate drie vp my Braines, teares seuen times salt,\\nBurne out the Sence and Vertue of mine eye.\\nBy Heauen, thy madnesse shall be payed by waight,\\nTill our Scale turnes the beame. Oh Rose of May,\\nDeere Maid, kinde Sister, sweet Ophelia:\\nOh Heauens, is't possible, a yong Maids wits,\\nShould be as mortall as an old mans life?\\nNature is fine in Loue, and where 'tis fine,\\nIt sends some precious instance of it selfe\\nAfter the thing it loues\\n\\n   Ophe. They bore him bare fac'd on the Beer,\\nHey non nony, nony, hey nony:\\nAnd on his graue raines many a teare,\\nFare you well my Doue\\n\\n   Laer. Had'st thou thy wits, and did'st perswade Reuenge,\\nit could not moue thus\\n\\n   Ophe. You must sing downe a-downe, and you call\\nhim a-downe-a. Oh, how the wheele becomes it? It is\\nthe false Steward that stole his masters daughter\\n\\n   Laer. This nothings more then matter\\n\\n   Ophe. There's Rosemary, that's for Remembraunce.\\nPray loue remember: and there is Paconcies, that's for\\nThoughts\\n\\n   Laer. A document in madnesse, thoughts & remembrance\\nfitted\\n\\n   Ophe. There's Fennell for you, and Columbines: ther's\\nRew for you, and heere's some for me. Wee may call it\\nHerbe-Grace a Sundaies: Oh you must weare your Rew\\nwith a difference. There's a Daysie, I would giue you\\nsome Violets, but they wither'd all when my Father dyed:\\nThey say, he made a good end;\\nFor bonny sweet Robin is all my ioy\\n\\n   Laer. Thought, and Affliction, Passion, Hell it selfe:\\nShe turnes to Fauour, and to prettinesse\\n\\n   Ophe. And will he not come againe,\\nAnd will he not come againe:\\nNo, no, he is dead, go to thy Death-bed,\\nHe neuer wil come againe.\\nHis Beard as white as Snow,\\nAll Flaxen was his Pole:\\nHe is gone, he is gone, and we cast away mone,\\nGramercy on his Soule.\\nAnd of all Christian Soules, I pray God.\\nGod buy ye.\\n\\nExeunt. Ophelia\\n\\n  Laer. Do you see this, you Gods?\\n  King. Laertes, I must common with your greefe,\\nOr you deny me right: go but apart,\\nMake choice of whom your wisest Friends you will,\\nAnd they shall heare and iudge 'twixt you and me;\\nIf by direct or by Colaterall hand\\nThey finde vs touch'd, we will our Kingdome giue,\\nOur Crowne, our Life, and all that we call Ours\\nTo you in satisfaction. But if not,\\nBe you content to lend your patience to vs,\\nAnd we shall ioyntly labour with your soule\\nTo giue it due content\\n\\n   Laer. Let this be so:\\nHis meanes of death, his obscure buriall;\\nNo Trophee, Sword, nor Hatchment o're his bones,\\nNo Noble rite, nor formall ostentation,\\nCry to be heard, as 'twere from Heauen to Earth,\\nThat I must call in question\\n\\n   King. So you shall:\\nAnd where th' offence is, let the great Axe fall.\\nI pray you go with me.\\n\\nExeunt.\\n\\nEnter Horatio, with an Attendant.\\n\\n  Hora. What are they that would speake with me?\\n  Ser. Saylors sir, they say they haue Letters for you\\n\\n   Hor. Let them come in,\\nI do not know from what part of the world\\nI should be greeted, if not from Lord Hamlet.\\nEnter Saylor.\\n\\n  Say. God blesse you Sir\\n\\n   Hor. Let him blesse thee too\\n\\n   Say. Hee shall Sir, and't please him. There's a Letter\\nfor you Sir: It comes from th' Ambassadours that was\\nbound for England, if your name be Horatio, as I am let\\nto know it is.\\n\\nReads the Letter.\\n\\nHoratio, When thou shalt haue ouerlook'd this, giue these\\nFellowes some meanes to the King: They haue Letters\\nfor him. Ere we were two dayes old at Sea, a Pyrate of very\\nWarlicke appointment gaue vs Chace. Finding our selues too\\nslow of Saile, we put on a compelled Valour. In the Grapple, I\\nboorded them: On the instant they got cleare of our Shippe, so\\nI alone became their Prisoner. They haue dealt with mee, like\\nTheeues of Mercy, but they knew what they did. I am to doe\\na good turne for them. Let the King haue the Letters I haue\\nsent, and repaire thou to me with as much hast as thou wouldest\\nflye death. I haue words to speake in your eare, will make thee\\ndumbe, yet are they much too light for the bore of the Matter.\\nThese good Fellowes will bring thee where I am. Rosincrance\\nand Guildensterne, hold their course for England. Of them\\nI haue much to tell thee, Farewell.\\nHe that thou knowest thine,\\nHamlet.\\nCome, I will giue you way for these your Letters,\\nAnd do't the speedier, that you may direct me\\nTo him from whom you brought them.\\nEnter.\\n\\nEnter King and Laertes.\\n\\n  King. Now must your conscience my acquittance seal,\\nAnd you must put me in your heart for Friend,\\nSith you haue heard, and with a knowing eare,\\nThat he which hath your Noble Father slaine,\\nPursued my life\\n\\n   Laer. It well appeares. But tell me,\\nWhy you proceeded not against these feates,\\nSo crimefull, and so Capitall in Nature,\\nAs by your Safety, Wisedome, all things else,\\nYou mainly were stirr'd vp?\\n  King. O for two speciall Reasons,\\nWhich may to you (perhaps) seeme much vnsinnowed,\\nAnd yet to me they are strong. The Queen his Mother,\\nLiues almost by his lookes: and for my selfe,\\nMy Vertue or my Plague, be it either which,\\nShe's so coniunctiue to my life, and soule;\\nThat as the Starre moues not but in his Sphere,\\nI could not but by her. The other Motiue,\\nWhy to a publike count I might not go,\\nIs the great loue the generall gender beare him,\\nWho dipping all his Faults in their affection,\\nWould like the Spring that turneth Wood to Stone,\\nConuert his Gyues to Graces. So that my Arrowes\\nToo slightly timbred for so loud a Winde,\\nWould haue reuerted to my Bow againe,\\nAnd not where I had arm'd them\\n\\n   Laer. And so haue I a Noble Father lost,\\nA Sister driuen into desperate tearmes,\\nWho was (if praises may go backe againe)\\nStood Challenger on mount of all the Age\\nFor her perfections. But my reuenge will come\\n\\n   King. Breake not your sleepes for that,\\nYou must not thinke\\nThat we are made of stuffe, so flat, and dull,\\nThat we can let our Beard be shooke with danger,\\nAnd thinke it pastime. You shortly shall heare more,\\nI lou'd your Father, and we loue our Selfe,\\nAnd that I hope will teach you to imagine-\\nEnter a Messenger.\\n\\nHow now? What Newes?\\n  Mes. Letters my Lord from Hamlet, This to your\\nMaiesty: this to the Queene\\n\\n   King. From Hamlet? Who brought them?\\n  Mes. Saylors my Lord they say, I saw them not:\\nThey were giuen me by Claudio, he receiu'd them\\n\\n   King. Laertes you shall heare them:\\nLeaue vs.\\n\\nExit Messenger\\n\\nHigh and Mighty, you shall know I am set naked on your\\nKingdome. To morrow shall I begge leaue to see your Kingly\\nEyes. When I shall (first asking your Pardon thereunto) recount\\nth' Occasions of my sodaine, and more strange returne.\\nHamlet.\\nWhat should this meane? Are all the rest come backe?\\nOr is it some abuse? Or no such thing?\\n  Laer. Know you the hand?\\n  Kin. 'Tis Hamlets Character, naked and in a Postscript\\nhere he sayes alone: Can you aduise me?\\n  Laer. I'm lost in it my Lord; but let him come,\\nIt warmes the very sicknesse in my heart,\\nThat I shall liue and tell him to his teeth;\\nThus diddest thou\\n\\n   Kin. If it be so Laertes, as how should it be so:\\nHow otherwise will you be rul'd by me?\\n  Laer. If so you'l not o'rerule me to a peace\\n\\n   Kin. To thine owne peace: if he be now return'd,\\nAs checking at his Voyage, and that he meanes\\nNo more to vndertake it; I will worke him\\nTo an exployt now ripe in my Deuice,\\nVnder the which he shall not choose but fall;\\nAnd for his death no winde of blame shall breath,\\nBut euen his Mother shall vncharge the practice,\\nAnd call it accident: Some two Monthes hence\\nHere was a Gentleman of Normandy,\\nI'ue seene my selfe, and seru'd against the French,\\nAnd they ran well on Horsebacke; but this Gallant\\nHad witchcraft in't; he grew into his Seat,\\nAnd to such wondrous doing brought his Horse,\\nAs had he beene encorps't and demy-Natur'd\\nWith the braue Beast, so farre he past my thought,\\nThat I in forgery of shapes and trickes,\\nCome short of what he did\\n\\n   Laer. A Norman was't?\\n  Kin. A Norman\\n\\n   Laer. Vpon my life Lamound\\n\\n   Kin. The very same\\n\\n   Laer. I know him well, he is the Brooch indeed,\\nAnd Iemme of all our Nation\\n\\n   Kin. Hee mad confession of you,\\nAnd gaue you such a Masterly report,\\nFor Art and exercise in your defence;\\nAnd for your Rapier most especiall,\\nThat he cryed out, t'would be a sight indeed,\\nIf one could match you Sir. This report of his\\nDid Hamlet so envenom with his Enuy,\\nThat he could nothing doe but wish and begge,\\nYour sodaine comming ore to play with him;\\nNow out of this\\n\\n   Laer. Why out of this, my Lord?\\n  Kin. Laertes was your Father deare to you?\\nOr are you like the painting of a sorrow,\\nA face without a heart?\\n  Laer. Why aske you this?\\n  Kin. Not that I thinke you did not loue your Father,\\nBut that I know Loue is begun by Time:\\nAnd that I see in passages of proofe,\\nTime qualifies the sparke and fire of it:\\nHamlet comes backe: what would you vndertake,\\nTo show your selfe your Fathers sonne indeed,\\nMore then in words?\\n  Laer. To cut his throat i'th' Church\\n\\n   Kin. No place indeed should murder Sancturize;\\nReuenge should haue no bounds: but good Laertes\\nWill you doe this, keepe close within your Chamber,\\nHamlet return'd, shall know you are come home:\\nWee'l put on those shall praise your excellence,\\nAnd set a double varnish on the fame\\nThe Frenchman gaue you, bring you in fine together,\\nAnd wager on your heads, he being remisse,\\nMost generous, and free from all contriuing,\\nWill not peruse the Foiles? So that with ease,\\nOr with a little shuffling, you may choose\\nA Sword vnbaited, and in a passe of practice,\\nRequit him for your Father\\n\\n   Laer. I will doo't.\\nAnd for that purpose Ile annoint my Sword:\\nI bought an Vnction of a Mountebanke\\nSo mortall, I but dipt a knife in it,\\nWhere it drawes blood, no Cataplasme so rare,\\nCollected from all Simples that haue Vertue\\nVnder the Moone, can saue the thing from death,\\nThat is but scratcht withall: Ile touch my point,\\nWith this contagion, that if I gall him slightly,\\nIt may be death\\n\\n   Kin. Let's further thinke of this,\\nWeigh what conuenience both of time and meanes\\nMay fit vs to our shape, if this should faile;\\nAnd that our drift looke through our bad performance,\\n'Twere better not assaid; therefore this Proiect\\nShould haue a backe or second, that might hold,\\nIf this should blast in proofe: Soft, let me see\\nWee'l make a solemne wager on your commings,\\nI ha't: when in your motion you are hot and dry,\\nAs make your bowts more violent to the end,\\nAnd that he cals for drinke; Ile haue prepar'd him\\nA Challice for the nonce; whereon but sipping,\\nIf he by chance escape your venom'd stuck,\\nOur purpose may hold there; how sweet Queene.\\nEnter Queene.\\n\\n  Queen. One woe doth tread vpon anothers heele,\\nSo fast they'l follow: your Sister's drown'd Laertes\\n\\n   Laer. Drown'd! O where?\\n  Queen. There is a Willow growes aslant a Brooke,\\nThat shewes his hore leaues in the glassie streame:\\nThere with fantasticke Garlands did she come,\\nOf Crow-flowers, Nettles, Daysies, and long Purples,\\nThat liberall Shepheards giue a grosser name;\\nBut our cold Maids doe Dead Mens Fingers call them:\\nThere on the pendant boughes, her Coronet weeds\\nClambring to hang; an enuious sliuer broke,\\nWhen downe the weedy Trophies, and her selfe,\\nFell in the weeping Brooke, her cloathes spred wide,\\nAnd Mermaid-like, a while they bore her vp,\\nWhich time she chaunted snatches of old tunes,\\nAs one incapable of her owne distresse,\\nOr like a creature Natiue, and indued\\nVnto that Element: but long it could not be,\\nTill that her garments, heauy with her drinke,\\nPul'd the poore wretch from her melodious buy,\\nTo muddy death\\n\\n   Laer. Alas then, is she drown'd?\\n  Queen. Drown'd, drown'd\\n\\n   Laer. Too much of water hast thou poore Ophelia,\\nAnd therefore I forbid my teares: but yet\\nIt is our tricke, Nature her custome holds,\\nLet shame say what it will; when these are gone\\nThe woman will be out: Adue my Lord,\\nI haue a speech of fire, that faine would blaze,\\nBut that this folly doubts it.\\nEnter.\\n\\n  Kin. Let's follow, Gertrude:\\nHow much I had to doe to calme his rage?\\nNow feare I this will giue it start againe;\\nTherefore let's follow.\\n\\nExeunt.\\n\\nEnter two Clownes.\\n\\n  Clown. Is she to bee buried in Christian buriall, that\\nwilfully seekes her owne saluation?\\n  Other. I tell thee she is, and therefore make her Graue\\nstraight, the Crowner hath sate on her, and finds it Christian\\nburiall\\n\\n   Clo. How can that be, vnlesse she drowned her selfe in\\nher owne defence?\\n  Other. Why 'tis found so\\n\\n   Clo. It must be Se offendendo, it cannot bee else: for\\nheere lies the point; If I drowne my selfe wittingly, it argues\\nan Act: and an Act hath three branches. It is an\\nAct to doe and to performe; argall she drown'd her selfe\\nwittingly\\n\\n   Other. Nay but heare you Goodman Deluer\\n\\n   Clown. Giue me leaue; heere lies the water; good:\\nheere stands the man; good: If the man goe to this water\\nand drowne himselfe; it is will he nill he, he goes;\\nmarke you that? But if the water come to him & drowne\\nhim; hee drownes not himselfe. Argall, hee that is not\\nguilty of his owne death, shortens not his owne life\\n\\n   Other. But is this law?\\n  Clo. I marry is't, Crowners Quest Law\\n\\n   Other. Will you ha the truth on't: if this had not\\nbeene a Gentlewoman, shee should haue beene buried\\nout of Christian Buriall\\n\\n   Clo. Why there thou say'st. And the more pitty that\\ngreat folke should haue countenance in this world to\\ndrowne or hang themselues, more then their euen Christian.\\nCome, my Spade; there is no ancient Gentlemen,\\nbut Gardiners, Ditchers and Graue-makers; they hold vp\\nAdams Profession\\n\\n   Other. Was he a Gentleman?\\n  Clo. He was the first that euer bore Armes\\n\\n   Other. Why he had none\\n\\n   Clo. What, ar't a Heathen? how doth thou vnderstand\\nthe Scripture? the Scripture sayes Adam dig'd;\\ncould hee digge without Armes? Ile put another question\\nto thee; if thou answerest me not to the purpose, confesse\\nthy selfe-\\n  Other. Go too\\n\\n   Clo. What is he that builds stronger then either the\\nMason, the Shipwright, or the Carpenter?\\n  Other. The Gallowes maker; for that Frame outliues a\\nthousand Tenants\\n\\n   Clo. I like thy wit well in good faith, the Gallowes\\ndoes well; but how does it well? it does well to those\\nthat doe ill: now, thou dost ill to say the Gallowes is\\nbuilt stronger then the Church: Argall, the Gallowes\\nmay doe well to thee. Too't againe, Come\\n\\n   Other. Who builds stronger then a Mason, a Shipwright,\\nor a Carpenter?\\n  Clo. I, tell me that, and vnyoake\\n\\n   Other. Marry, now I can tell\\n\\n   Clo. Too't\\n\\n   Other. Masse, I cannot tell.\\nEnter Hamlet and Horatio a farre off.\\n\\n  Clo. Cudgell thy braines no more about it; for your\\ndull Asse will not mend his pace with beating; and when\\nyou are ask't this question next, say a Graue-maker: the\\nHouses that he makes, lasts till Doomesday: go, get thee\\nto Yaughan, fetch me a stoupe of Liquor.\\n\\nSings.\\n\\nIn youth when I did loue, did loue,\\nme thought it was very sweete:\\nTo contract O the time for a my behoue,\\nO me thought there was nothing meete\\n\\n   Ham. Ha's this fellow no feeling of his businesse, that\\nhe sings at Graue-making?\\n  Hor. Custome hath made it in him a property of easinesse\\n\\n   Ham. 'Tis ee'n so; the hand of little Imployment hath\\nthe daintier sense\\n\\n   Clowne sings. But Age with his stealing steps\\nhath caught me in his clutch:\\nAnd hath shipped me intill the Land,\\nas if I had neuer beene such\\n\\n   Ham. That Scull had a tongue in it, and could sing\\nonce: how the knaue iowles it to th' grownd, as if it\\nwere Caines Iaw-bone, that did the first murther: It\\nmight be the Pate of a Polititian which this Asse o're Offices:\\none that could circumuent God, might it not?\\n  Hor. It might, my Lord\\n\\n   Ham. Or of a Courtier, which could say, Good Morrow\\nsweet Lord: how dost thou, good Lord? this\\nmight be my Lord such a one, that prais'd my Lord such\\na ones Horse, when he meant to begge it; might it not?\\n  Hor. I, my Lord\\n\\n   Ham. Why ee'n so: and now my Lady Wormes,\\nChaplesse, and knockt about the Mazard with a Sextons\\nSpade; heere's fine Reuolution, if wee had the tricke to\\nsee't. Did these bones cost no more the breeding, but\\nto play at Loggets with 'em? mine ake to thinke\\non't\\n\\n   Clowne sings. A Pickhaxe and a Spade, a Spade,\\nfor and a shrowding-Sheete:\\nO a Pit of Clay for to be made,\\nfor such a Guest is meete\\n\\n   Ham. There's another: why might not that bee the\\nScull of a Lawyer? where be his Quiddits now? his\\nQuillets? his Cases? his Tenures, and his Tricks? why\\ndoe's he suffer this rude knaue now to knocke him about\\nthe Sconce with a dirty Shouell, and will not tell him of\\nhis Action of Battery? hum. This fellow might be in's\\ntime a great buyer of Land, with his Statutes, his Recognizances,\\nhis Fines, his double Vouchers, his Recoueries:\\nIs this the fine of his Fines, and the recouery of his Recoueries,\\nto haue his fine Pate full of fine Dirt? will his\\nVouchers vouch him no more of his Purchases, and double\\nones too, then the length and breadth of a paire of\\nIndentures? the very Conueyances of his Lands will\\nhardly lye in this Boxe; and must the Inheritor himselfe\\nhaue no more? ha?\\n  Hor. Not a iot more, my Lord\\n\\n   Ham. Is not Parchment made of Sheep-skinnes?\\n  Hor. I my Lord, and of Calue-skinnes too\\n\\n   Ham. They are Sheepe and Calues that seek out assurance\\nin that. I will speake to this fellow: whose Graue's\\nthis Sir?\\n  Clo. Mine Sir:\\nO a Pit of Clay for to be made,\\nfor such a Guest is meete\\n\\n   Ham. I thinke it be thine indeed: for thou liest in't\\n\\n   Clo. You lye out on't Sir, and therefore it is not yours:\\nfor my part, I doe not lye in't; and yet it is mine\\n\\n   Ham. Thou dost lye in't, to be in't and say 'tis thine:\\n'tis for the dead, not for the quicke, therefore thou\\nlyest\\n\\n   Clo. 'Tis a quicke lye Sir, 'twill away againe from me\\nto you\\n\\n   Ham. What man dost thou digge it for?\\n  Clo. For no man Sir\\n\\n   Ham. What woman then?\\n  Clo. For none neither\\n\\n   Ham. Who is to be buried in't?\\n  Clo. One that was a woman Sir; but rest her Soule,\\nshee's dead\\n\\n   Ham. How absolute the knaue is? wee must speake\\nby the Carde, or equiuocation will vndoe vs: by the\\nLord Horatio, these three yeares I haue taken note of it,\\nthe Age is growne so picked, that the toe of the Pesant\\ncomes so neere the heeles of our Courtier, hee galls his\\nKibe. How long hast thou been a Graue-maker?\\n  Clo. Of all the dayes i'th' yeare, I came too't that day\\nthat our last King Hamlet o'recame Fortinbras\\n\\n   Ham. How long is that since?\\n  Clo. Cannot you tell that? euery foole can tell that:\\nIt was the very day, that young Hamlet was borne, hee\\nthat was mad, and sent into England\\n\\n   Ham. I marry, why was he sent into England?\\n  Clo. Why, because he was mad; hee shall recouer his\\nwits there; or if he do not, it's no great matter there\\n\\n   Ham. Why?\\n  Clo. 'Twill not be seene in him, there the men are as\\nmad as he\\n\\n   Ham. How came he mad?\\n  Clo. Very strangely they say\\n\\n   Ham. How strangely?\\n  Clo. Faith e'ene with loosing his wits\\n\\n   Ham. Vpon what ground?\\n  Clo. Why heere in Denmarke: I haue bin sixeteene\\nheere, man and Boy thirty yeares\\n\\n   Ham. How long will a man lie i'th' earth ere he rot?\\n  Clo. Ifaith, if he be not rotten before he die (as we haue\\nmany pocky Coarses now adaies, that will scarce hold\\nthe laying in) he will last you some eight yeare, or nine\\nyeare. A Tanner will last you nine yeare\\n\\n   Ham. Why he, more then another?\\n  Clo. Why sir, his hide is so tan'd with his Trade, that\\nhe will keepe out water a great while. And your water,\\nis a sore Decayer of your horson dead body. Heres a Scull\\nnow: this Scul, has laine in the earth three & twenty years\\n\\n   Ham. Whose was it?\\n  Clo. A whoreson mad Fellowes it was;\\nWhose doe you thinke it was?\\n  Ham. Nay, I know not\\n\\n   Clo. A pestilence on him for a mad Rogue, a pour'd a\\nFlaggon of Renish on my head once. This same Scull\\nSir, this same Scull sir, was Yoricks Scull, the Kings Iester\\n\\n   Ham. This?\\n  Clo. E'ene that\\n\\n   Ham. Let me see. Alas poore Yorick, I knew him Horatio,\\na fellow of infinite Iest; of most excellent fancy, he\\nhath borne me on his backe a thousand times: And how\\nabhorred my Imagination is, my gorge rises at it. Heere\\nhung those lipps, that I haue kist I know not how oft.\\nWhere be your Iibes now? Your Gambals? Your\\nSongs? Your flashes of Merriment that were wont to\\nset the Table on a Rore? No one now to mock your own\\nIeering? Quite chopfalne? Now get you to my Ladies\\nChamber, and tell her, let her paint an inch thicke, to this\\nfauour she must come. Make her laugh at that: prythee\\nHoratio tell me one thing\\n\\n   Hor. What's that my Lord?\\n  Ham. Dost thou thinke Alexander lookt o'this fashion\\ni'th' earth?\\n  Hor. E'ene so\\n\\n   Ham. And smelt so? Puh\\n\\n   Hor. E'ene so, my Lord\\n\\n   Ham. To what base vses we may returne Horatio.\\nWhy may not Imagination trace the Noble dust of Alexander,\\ntill he find it stopping a bunghole\\n\\n   Hor. 'Twere to consider: to curiously to consider so\\n\\n   Ham. No faith, not a iot. But to follow him thether\\nwith modestie enough, & likeliehood to lead it; as thus.\\nAlexander died: Alexander was buried: Alexander returneth\\ninto dust; the dust is earth; of earth we make\\nLome, and why of that Lome (whereto he was conuerted)\\nmight they not stopp a Beere-barrell?\\nImperiall Caesar, dead and turn'd to clay,\\nMight stop a hole to keepe the winde away.\\nOh, that that earth, which kept the world in awe,\\nShould patch a Wall, t' expell the winters flaw.\\nBut soft, but soft, aside; heere comes the King.\\nEnter King, Queene, Laertes, and a Coffin, with Lords attendant.\\n\\nThe Queene, the Courtiers. Who is that they follow,\\nAnd with such maimed rites? This doth betoken,\\nThe Coarse they follow, did with disperate hand,\\nFore do it owne life; 'twas some Estate.\\nCouch we a while, and mark\\n\\n   Laer. What Cerimony else?\\n  Ham. That is Laertes, a very Noble youth: Marke\\n\\n   Laer. What Cerimony else?\\n  Priest. Her Obsequies haue bin as farre inlarg'd.\\nAs we haue warrantie, her death was doubtfull,\\nAnd but that great Command, o're-swaies the order,\\nShe should in ground vnsanctified haue lodg'd,\\nTill the last Trumpet. For charitable praier,\\nShardes, Flints, and Peebles, should be throwne on her:\\nYet heere she is allowed her Virgin Rites,\\nHer Maiden strewments, and the bringing home\\nOf Bell and Buriall\\n\\n   Laer. Must there no more be done ?\\n  Priest. No more be done:\\nWe should prophane the seruice of the dead,\\nTo sing sage Requiem, and such rest to her\\nAs to peace-parted Soules\\n\\n   Laer. Lay her i'th' earth,\\nAnd from her faire and vnpolluted flesh,\\nMay Violets spring. I tell thee (churlish Priest)\\nA Ministring Angell shall my Sister be,\\nWhen thou liest howling?\\n  Ham. What, the faire Ophelia?\\n  Queene. Sweets, to the sweet farewell.\\nI hop'd thou should'st haue bin my Hamlets wife:\\nI thought thy Bride-bed to haue deckt (sweet Maid)\\nAnd not t'haue strew'd thy Graue\\n\\n   Laer. Oh terrible woer,\\nFall ten times trebble, on that cursed head\\nWhose wicked deed, thy most Ingenious sence\\nDepriu'd thee of. Hold off the earth a while,\\nTill I haue caught her once more in mine armes:\\n\\nLeaps in the graue.\\n\\nNow pile your dust, vpon the quicke, and dead,\\nTill of this flat a Mountaine you haue made,\\nTo o're top old Pelion, or the skyish head\\nOf blew Olympus\\n\\n   Ham. What is he, whose griefes\\nBeares such an Emphasis? whose phrase of Sorrow\\nConiure the wandring Starres, and makes them stand\\nLike wonder-wounded hearers? This is I,\\nHamlet the Dane\\n\\n   Laer. The deuill take thy soule\\n\\n   Ham. Thou prai'st not well,\\nI prythee take thy fingers from my throat;\\nSir though I am not Spleenatiue, and rash,\\nYet haue I something in me dangerous,\\nWhich let thy wisenesse feare. Away thy hand\\n\\n   King. Pluck them asunder\\n\\n   Qu. Hamlet, Hamlet\\n\\n   Gen. Good my Lord be quiet\\n\\n   Ham. Why I will fight with him vppon this Theme.\\nVntill my eielids will no longer wag\\n\\n   Qu. Oh my Sonne, what Theame?\\n  Ham. I lou'd Ophelia; fortie thousand Brothers\\nCould not (with all there quantitie of Loue)\\nMake vp my summe. What wilt thou do for her?\\n  King. Oh he is mad Laertes,\\n  Qu. For loue of God forbeare him\\n\\n   Ham. Come show me what thou'lt doe.\\nWoo't weepe? Woo't fight? Woo't teare thy selfe?\\nWoo't drinke vp Esile, eate a Crocodile?\\nIle doo't. Dost thou come heere to whine;\\nTo outface me with leaping in her Graue?\\nBe buried quicke with her, and so will I.\\nAnd if thou prate of Mountaines; let them throw\\nMillions of Akers on vs; till our ground\\nSindging his pate against the burning Zone,\\nMake Ossa like a wart. Nay, and thou'lt mouth,\\nIle rant as well as thou\\n\\n   Kin. This is meere Madnesse:\\nAnd thus awhile the fit will worke on him:\\nAnon as patient as the female Doue,\\nWhen that her Golden Cuplet are disclos'd;\\nHis silence will sit drooping\\n\\n   Ham. Heare you Sir:\\nWhat is the reason that you vse me thus?\\nI lou'd you euer; but it is no matter:\\nLet Hercules himselfe doe what he may,\\nThe Cat will Mew, and Dogge will haue his day.\\nEnter.\\n\\n  Kin. I pray you good Horatio wait vpon him,\\nStrengthen your patience in our last nights speech,\\nWee'l put the matter to the present push:\\nGood Gertrude set some watch ouer your Sonne,\\nThis Graue shall haue a liuing Monument:\\nAn houre of quiet shortly shall we see;\\nTill then, in patience our proceeding be.\\n\\nExeunt.\\n\\nEnter Hamlet and Horatio\\n\\n   Ham. So much for this Sir; now let me see the other,\\nYou doe remember all the Circumstance\\n\\n   Hor. Remember it my Lord?\\n  Ham. Sir, in my heart there was a kinde of fighting,\\nThat would not let me sleepe; me thought I lay\\nWorse then the mutines in the Bilboes, rashly,\\n(And praise be rashnesse for it) let vs know,\\nOur indiscretion sometimes serues vs well,\\nWhen our deare plots do paule, and that should teach vs,\\nThere's a Diuinity that shapes our ends,\\nRough-hew them how we will\\n\\n   Hor. That is most certaine\\n\\n   Ham. Vp from my Cabin\\nMy sea-gowne scarft about me in the darke,\\nGrop'd I to finde out them; had my desire,\\nFinger'd their Packet, and in fine, withdrew\\nTo mine owne roome againe, making so bold,\\n(My feares forgetting manners) to vnseale\\nTheir grand Commission, where I found Horatio,\\nOh royall knauery: An exact command,\\nLarded with many seuerall sorts of reason;\\nImporting Denmarks health, and Englands too,\\nWith hoo, such Bugges and Goblins in my life,\\nThat on the superuize no leasure bated,\\nNo not to stay the grinding of the Axe,\\nMy head should be struck off\\n\\n   Hor. Ist possible?\\n  Ham. Here's the Commission, read it at more leysure:\\nBut wilt thou heare me how I did proceed?\\n  Hor. I beseech you\\n\\n   Ham. Being thus benetted round with Villaines,\\nEre I could make a Prologue to my braines,\\nThey had begun the Play. I sate me downe,\\nDeuis'd a new Commission, wrote it faire,\\nI once did hold it as our Statists doe,\\nA basenesse to write faire; and laboured much\\nHow to forget that learning: but Sir now,\\nIt did me Yeomans seriuce: wilt thou know\\nThe effects of what I wrote?\\n  Hor. I, good my Lord\\n\\n   Ham. An earnest Coniuration from the King,\\nAs England was his faithfull Tributary,\\nAs loue betweene them, as the Palme should flourish,\\nAs Peace should still her wheaten Garland weare,\\nAnd stand a Comma 'tweene their amities,\\nAnd many such like Assis of great charge,\\nThat on the view and know of these Contents,\\nWithout debatement further, more or lesse,\\nHe should the bearers put to sodaine death,\\nNot shriuing time allowed\\n\\n   Hor. How was this seal'd?\\n  Ham. Why, euen in that was Heauen ordinate;\\nI had my fathers Signet in my Purse,\\nWhich was the Modell of that Danish Seale:\\nFolded the Writ vp in forme of the other,\\nSubscrib'd it, gau't th' impression, plac't it safely,\\nThe changeling neuer knowne: Now, the next day\\nWas our Sea Fight, and what to this was sement,\\nThou know'st already\\n\\n   Hor. So Guildensterne and Rosincrance, go too't\\n\\n   Ham. Why man, they did make loue to this imployment\\nThey are not neere my Conscience; their debate\\nDoth by their owne insinuation grow:\\n'Tis dangerous, when the baser nature comes\\nBetweene the passe, and fell incensed points\\nOf mighty opposites\\n\\n   Hor. Why, what a King is this?\\n  Ham. Does it not, thinkst thee, stand me now vpon\\nHe that hath kil'd my King, and whor'd my Mother,\\nPopt in betweene th' election and my hopes,\\nThrowne out his Angle for my proper life,\\nAnd with such coozenage; is't not perfect conscience,\\nTo quit him with this arme? And is't not to be damn'd\\nTo let this Canker of our nature come\\nIn further euill\\n\\n   Hor. It must be shortly knowne to him from England\\nWhat is the issue of the businesse there\\n\\n   Ham. It will be short,\\nThe interim's mine, and a mans life's no more\\nThen to say one: but I am very sorry good Horatio,\\nThat to Laertes I forgot my selfe;\\nFor by the image of my Cause, I see\\nThe Portraiture of his; Ile count his fauours:\\nBut sure the brauery of his griefe did put me\\nInto a Towring passion\\n\\n   Hor. Peace, who comes heere?\\nEnter young Osricke.\\n\\n  Osr. Your Lordship is right welcome back to Denmarke\\n\\n   Ham. I humbly thank you Sir, dost know this waterflie?\\n  Hor. No my good Lord\\n\\n   Ham. Thy state is the more gracious; for 'tis a vice to\\nknow him: he hath much Land, and fertile; let a Beast\\nbe Lord of Beasts, and his Crib shall stand at the Kings\\nMesse; 'tis a Chowgh; but as I saw spacious in the possession\\nof dirt\\n\\n   Osr. Sweet Lord, if your friendship were at leysure,\\nI should impart a thing to you from his Maiesty\\n\\n   Ham. I will receiue it with all diligence of spirit; put\\nyour Bonet to his right vse, 'tis for the head\\n\\n   Osr. I thanke your Lordship, 'tis very hot\\n\\n   Ham. No, beleeue mee 'tis very cold, the winde is\\nNortherly\\n\\n   Osr. It is indifferent cold my Lord indeed\\n\\n   Ham. Mee thinkes it is very soultry, and hot for my\\nComplexion\\n\\n   Osr. Exceedingly, my Lord, it is very soultry, as 'twere\\nI cannot tell how: but my Lord, his Maiesty bad me signifie\\nto you, that he ha's laid a great wager on your head:\\nSir, this is the matter\\n\\n   Ham. I beseech you remember\\n\\n   Osr. Nay, in good faith, for mine ease in good faith:\\nSir, you are not ignorant of what excellence Laertes is at\\nhis weapon\\n\\n   Ham. What's his weapon?\\n  Osr. Rapier and dagger\\n\\n   Ham. That's two of his weapons; but well\\n\\n   Osr. The sir King ha's wag'd with him six Barbary horses,\\nagainst the which he impon'd as I take it, sixe French\\nRapiers and Poniards, with their assignes, as Girdle,\\nHangers or so: three of the Carriages infaith are very\\ndeare to fancy, very responsiue to the hilts, most delicate\\ncarriages, and of very liberall conceit\\n\\n   Ham. What call you the Carriages?\\n  Osr. The Carriages Sir, are the hangers\\n\\n   Ham. The phrase would bee more Germaine to the\\nmatter: If we could carry Cannon by our sides; I would\\nit might be Hangers till then; but on sixe Barbary Horses\\nagainst sixe French Swords: their Assignes, and three\\nliberall conceited Carriages, that's the French but against\\nthe Danish; why is this impon'd as you call it?\\n  Osr. The King Sir, hath laid that in a dozen passes betweene\\nyou and him, hee shall not exceed you three hits;\\nHe hath one twelue for mine, and that would come to\\nimediate tryall, if your Lordship would vouchsafe the\\nAnswere\\n\\n   Ham. How if I answere no?\\n  Osr. I meane my Lord, the opposition of your person\\nin tryall\\n\\n   Ham. Sir, I will walke heere in the Hall; if it please\\nhis Maiestie, 'tis the breathing time of day with me; let\\nthe Foyles bee brought, the Gentleman willing, and the\\nKing hold his purpose; I will win for him if I can: if\\nnot, Ile gaine nothing but my shame, and the odde hits\\n\\n   Osr. Shall I redeliuer you ee'n so?\\n  Ham. To this effect Sir, after what flourish your nature\\nwill\\n\\n   Osr. I commend my duty to your Lordship\\n\\n   Ham. Yours, yours; hee does well to commend it\\nhimselfe, there are no tongues else for's tongue\\n\\n   Hor. This Lapwing runs away with the shell on his\\nhead\\n\\n   Ham. He did Complie with his Dugge before hee\\nsuck't it: thus had he and mine more of the same Beauty\\nthat I know the drossie age dotes on; only got the tune of\\nthe time, and outward habite of encounter, a kinde of\\nyesty collection, which carries them through & through\\nthe most fond and winnowed opinions; and doe but blow\\nthem to their tryalls: the Bubbles are out\\n\\n   Hor. You will lose this wager, my Lord\\n\\n   Ham. I doe not thinke so, since he went into France,\\nI haue beene in continuall practice; I shall winne at the\\noddes: but thou wouldest not thinke how all heere about\\nmy heart: but it is no matter\\n\\n   Hor. Nay, good my Lord\\n\\n   Ham. It is but foolery; but it is such a kinde of\\ngain-giuing as would perhaps trouble a woman\\n\\n   Hor. If your minde dislike any thing, obey. I will forestall\\ntheir repaire hither, and say you are not fit\\n\\n   Ham. Not a whit, we defie Augury; there's a speciall\\nProuidence in the fall of a sparrow. If it be now, 'tis not\\nto come: if it bee not to come, it will bee now: if it\\nbe not now; yet it will come; the readinesse is all, since no\\nman ha's ought of what he leaues. What is't to leaue betimes?\\nEnter King, Queene, Laertes and Lords, with other Attendants with\\nFoyles,\\nand Gauntlets, a Table and Flagons of Wine on it.\\n\\n  Kin. Come Hamlet, come, and take this hand from me\\n\\n   Ham. Giue me your pardon Sir, I'ue done you wrong,\\nBut pardon't as you are a Gentleman.\\nThis presence knowes,\\nAnd you must needs haue heard how I am punisht\\nWith sore distraction? What I haue done\\nThat might your nature honour, and exception\\nRoughly awake, I heere proclaime was madnesse:\\nWas't Hamlet wrong'd Laertes? Neuer Hamlet.\\nIf Hamlet from himselfe be tane away:\\nAnd when he's not himselfe, do's wrong Laertes,\\nThen Hamlet does it not, Hamlet denies it:\\nWho does it then? His Madnesse? If't be so,\\nHamlet is of the Faction that is wrong'd,\\nHis madnesse is poore Hamlets Enemy.\\nSir, in this Audience,\\nLet my disclaiming from a purpos'd euill,\\nFree me so farre in your most generous thoughts,\\nThat I haue shot mine Arrow o're the house,\\nAnd hurt my Mother\\n\\n   Laer. I am satisfied in Nature,\\nWhose motiue in this case should stirre me most\\nTo my Reuenge. But in my termes of Honor\\nI stand aloofe, and will no reconcilement,\\nTill by some elder Masters of knowne Honor,\\nI haue a voyce, and president of peace\\nTo keepe my name vngorg'd. But till that time,\\nI do receiue your offer'd loue like loue,\\nAnd wil not wrong it\\n\\n   Ham. I do embrace it freely,\\nAnd will this Brothers wager frankely play.\\nGiue vs the Foyles: Come on\\n\\n   Laer. Come one for me\\n\\n   Ham. Ile be your foile Laertes, in mine ignorance,\\nYour Skill shall like a Starre i'th' darkest night,\\nSticke fiery off indeede\\n\\n   Laer. You mocke me Sir\\n\\n   Ham. No by this hand\\n\\n   King. Giue them the Foyles yong Osricke,\\nCousen Hamlet, you know the wager\\n\\n   Ham. Verie well my Lord,\\nYour Grace hath laide the oddes a'th' weaker side\\n\\n   King. I do not feare it,\\nI haue seene you both:\\nBut since he is better'd, we haue therefore oddes\\n\\n   Laer. This is too heauy,\\nLet me see another\\n\\n   Ham. This likes me well,\\nThese Foyles haue all a length.\\n\\nPrepare to play.\\n\\n  Osricke. I my good Lord\\n\\n   King. Set me the Stopes of wine vpon that Table:\\nIf Hamlet giue the first, or second hit,\\nOr quit in answer of the third exchange,\\nLet all the Battlements their Ordinance fire,\\nThe King shal drinke to Hamlets better breath,\\nAnd in the Cup an vnion shal he throw\\nRicher then that, which foure successiue Kings\\nIn Denmarkes Crowne haue worne.\\nGiue me the Cups,\\nAnd let the Kettle to the Trumpets speake,\\nThe Trumpet to the Cannoneer without,\\nThe Cannons to the Heauens, the Heauen to Earth,\\nNow the King drinkes to Hamlet. Come, begin,\\nAnd you the Iudges beare a wary eye\\n\\n   Ham. Come on sir\\n\\n   Laer. Come on sir.\\n\\nThey play.\\n\\n  Ham. One\\n\\n   Laer. No\\n\\n   Ham. Iudgement\\n\\n   Osr. A hit, a very palpable hit\\n\\n   Laer. Well: againe\\n\\n   King. Stay, giue me drinke.\\nHamlet, this Pearle is thine,\\nHere's to thy health. Giue him the cup,\\n\\nTrumpets sound, and shot goes off.\\n\\n  Ham. Ile play this bout first, set by a-while.\\nCome: Another hit; what say you?\\n  Laer. A touch, a touch, I do confesse\\n\\n   King. Our Sonne shall win\\n\\n   Qu. He's fat, and scant of breath.\\nHeere's a Napkin, rub thy browes,\\nThe Queene Carowses to thy fortune, Hamlet\\n\\n   Ham. Good Madam\\n\\n   King. Gertrude, do not drinke\\n\\n   Qu. I will my Lord;\\nI pray you pardon me\\n\\n   King. It is the poyson'd Cup, it is too late\\n\\n   Ham. I dare not drinke yet Madam,\\nBy and by\\n\\n   Qu. Come, let me wipe thy face\\n\\n   Laer. My Lord, Ile hit him now\\n\\n   King. I do not thinke't\\n\\n   Laer. And yet 'tis almost 'gainst my conscience\\n\\n   Ham. Come for the third.\\nLaertes, you but dally,\\nI pray you passe with your best violence,\\nI am affear'd you make a wanton of me\\n\\n   Laer. Say you so? Come on.\\n\\nPlay.\\n\\n  Osr. Nothing neither way\\n\\n   Laer. Haue at you now.\\n\\nIn scuffling they change Rapiers.\\n\\n  King. Part them, they are incens'd\\n\\n   Ham. Nay come, againe\\n\\n   Osr. Looke to the Queene there hoa\\n\\n   Hor. They bleed on both sides. How is't my Lord?\\n  Osr. How is't Laertes?\\n  Laer. Why as a Woodcocke\\nTo mine Sprindge, Osricke,\\nI am iustly kill'd with mine owne Treacherie\\n\\n   Ham. How does the Queene?\\n  King. She sounds to see them bleede\\n\\n   Qu. No, no, the drinke, the drinke.\\nOh my deere Hamlet, the drinke, the drinke,\\nI am poyson'd\\n\\n   Ham. Oh Villany! How? Let the doore be lock'd.\\nTreacherie, seeke it out\\n\\n   Laer. It is heere Hamlet.\\nHamlet, thou art slaine,\\nNo Medicine in the world can do thee good.\\nIn thee, there is not halfe an houre of life;\\nThe Treacherous Instrument is in thy hand,\\nVnbated and envenom'd: the foule practise\\nHath turn'd it selfe on me. Loe, heere I lye,\\nNeuer to rise againe: Thy Mothers poyson'd:\\nI can no more, the King, the King's too blame\\n\\n   Ham. The point envenom'd too,\\nThen venome to thy worke.\\n\\nHurts the King.\\n\\n  All. Treason, Treason\\n\\n   King. O yet defend me Friends, I am but hurt\\n\\n   Ham. Heere thou incestuous, murdrous,\\nDamned Dane,\\nDrinke off this Potion: Is thy Vnion heere?\\nFollow my Mother.\\n\\nKing Dyes.\\n\\n  Laer. He is iustly seru'd.\\nIt is a poyson temp'red by himselfe:\\nExchange forgiuenesse with me, Noble Hamlet;\\nMine and my Fathers death come not vpon thee,\\nNor thine on me.\\n\\nDyes.\\n\\n  Ham. Heauen make thee free of it, I follow thee.\\nI am dead Horatio, wretched Queene adiew,\\nYou that looke pale, and tremble at this chance,\\nThat are but Mutes or audience to this acte:\\nHad I but time (as this fell Sergeant death\\nIs strick'd in his Arrest) oh I could tell you.\\nBut let it be: Horatio, I am dead,\\nThou liu'st, report me and my causes right\\nTo the vnsatisfied\\n\\n   Hor. Neuer beleeue it.\\nI am more an Antike Roman then a Dane:\\nHeere's yet some Liquor left\\n\\n   Ham. As th'art a man, giue me the Cup.\\nLet go, by Heauen Ile haue't.\\nOh good Horatio, what a wounded name,\\n(Things standing thus vnknowne) shall liue behind me.\\nIf thou did'st euer hold me in thy heart,\\nAbsent thee from felicitie awhile,\\nAnd in this harsh world draw thy breath in paine,\\nTo tell my Storie.\\n\\nMarch afarre off, and shout within.\\n\\nWhat warlike noyse is this?\\nEnter Osricke.\\n\\n  Osr. Yong Fortinbras, with conquest come fro[m] Poland\\nTo th' Ambassadors of England giues this warlike volly\\n\\n   Ham. O I dye Horatio:\\nThe potent poyson quite ore-crowes my spirit,\\nI cannot liue to heare the Newes from England,\\nBut I do prophesie th' election lights\\nOn Fortinbras, he ha's my dying voyce,\\nSo tell him with the occurrents more and lesse,\\nWhich haue solicited. The rest is silence. O, o, o, o.\\n\\nDyes\\n\\n  Hora. Now cracke a Noble heart:\\nGoodnight sweet Prince,\\nAnd flights of Angels sing thee to thy rest,\\nWhy do's the Drumme come hither?\\nEnter Fortinbras and English Ambassador, with Drumme, Colours,\\nand\\nAttendants.\\n\\n  Fortin. Where is this sight?\\n  Hor. What is it ye would see;\\nIf ought of woe, or wonder, cease your search\\n\\n   For. His quarry cries on hauocke. Oh proud death,\\nWhat feast is toward in thine eternall Cell.\\nThat thou so many Princes, at a shoote,\\nSo bloodily hast strooke\\n\\n   Amb. The sight is dismall,\\nAnd our affaires from England come too late,\\nThe eares are senselesse that should giue vs hearing,\\nTo tell him his command'ment is fulfill'd,\\nThat Rosincrance and Guildensterne are dead:\\nWhere should we haue our thankes?\\n  Hor. Not from his mouth,\\nHad it th' abilitie of life to thanke you:\\nHe neuer gaue command'ment for their death.\\nBut since so iumpe vpon this bloodie question,\\nYou from the Polake warres, and you from England\\nAre heere arriued. Giue order that these bodies\\nHigh on a stage be placed to the view,\\nAnd let me speake to th' yet vnknowing world,\\nHow these things came about. So shall you heare\\nOf carnall, bloudie, and vnnaturall acts,\\nOf accidentall iudgements, casuall slaughters\\nOf death's put on by cunning, and forc'd cause,\\nAnd in this vpshot, purposes mistooke,\\nFalne on the Inuentors head. All this can I\\nTruly deliuer\\n\\n   For. Let vs hast to heare it,\\nAnd call the Noblest to the Audience.\\nFor me, with sorrow, I embrace my Fortune,\\nI haue some Rites of memory in this Kingdome,\\nWhich are to claime, my vantage doth\\nInuite me,\\n  Hor. Of that I shall haue alwayes cause to speake,\\nAnd from his mouth\\nWhose voyce will draw on more:\\nBut let this same be presently perform'd,\\nEuen whiles mens mindes are wilde,\\nLest more mischance\\nOn plots, and errors happen\\n\\n   For. Let foure Captaines\\nBeare Hamlet like a Soldier to the Stage,\\nFor he was likely, had he beene put on\\nTo haue prou'd most royally:\\nAnd for his passage,\\nThe Souldiours Musicke, and the rites of Warre\\nSpeake lowdly for him.\\nTake vp the body; Such a sight as this\\nBecomes the Field, but heere shewes much amis.\\nGo, bid the Souldiers shoote.\\n\\nExeunt. Marching: after the which, a Peale of Ordenance are shot\\noff.\\n\\n\\nFINIS. The tragedie of HAMLET, Prince of Denmarke.\\n\")]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "doc=TextLoader(r'D:\\btch 16\\NPl\\hamlet.txt')\n",
    "fin_t= doc.load()\n",
    "fin_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5e1fb5ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 296, which is longer than the specified 200\n",
      "Created a chunk of size 216, which is longer than the specified 200\n",
      "Created a chunk of size 233, which is longer than the specified 200\n",
      "Created a chunk of size 1698, which is longer than the specified 200\n",
      "Created a chunk of size 519, which is longer than the specified 200\n",
      "Created a chunk of size 381, which is longer than the specified 200\n",
      "Created a chunk of size 367, which is longer than the specified 200\n",
      "Created a chunk of size 488, which is longer than the specified 200\n",
      "Created a chunk of size 1155, which is longer than the specified 200\n",
      "Created a chunk of size 600, which is longer than the specified 200\n",
      "Created a chunk of size 682, which is longer than the specified 200\n",
      "Created a chunk of size 265, which is longer than the specified 200\n",
      "Created a chunk of size 489, which is longer than the specified 200\n",
      "Created a chunk of size 1345, which is longer than the specified 200\n",
      "Created a chunk of size 360, which is longer than the specified 200\n",
      "Created a chunk of size 1432, which is longer than the specified 200\n",
      "Created a chunk of size 257, which is longer than the specified 200\n",
      "Created a chunk of size 219, which is longer than the specified 200\n",
      "Created a chunk of size 741, which is longer than the specified 200\n",
      "Created a chunk of size 299, which is longer than the specified 200\n",
      "Created a chunk of size 402, which is longer than the specified 200\n",
      "Created a chunk of size 250, which is longer than the specified 200\n",
      "Created a chunk of size 238, which is longer than the specified 200\n",
      "Created a chunk of size 1518, which is longer than the specified 200\n",
      "Created a chunk of size 305, which is longer than the specified 200\n",
      "Created a chunk of size 1212, which is longer than the specified 200\n",
      "Created a chunk of size 456, which is longer than the specified 200\n",
      "Created a chunk of size 244, which is longer than the specified 200\n",
      "Created a chunk of size 940, which is longer than the specified 200\n",
      "Created a chunk of size 360, which is longer than the specified 200\n",
      "Created a chunk of size 848, which is longer than the specified 200\n",
      "Created a chunk of size 321, which is longer than the specified 200\n",
      "Created a chunk of size 229, which is longer than the specified 200\n",
      "Created a chunk of size 680, which is longer than the specified 200\n",
      "Created a chunk of size 415, which is longer than the specified 200\n",
      "Created a chunk of size 2123, which is longer than the specified 200\n",
      "Created a chunk of size 876, which is longer than the specified 200\n",
      "Created a chunk of size 286, which is longer than the specified 200\n",
      "Created a chunk of size 310, which is longer than the specified 200\n",
      "Created a chunk of size 699, which is longer than the specified 200\n",
      "Created a chunk of size 393, which is longer than the specified 200\n",
      "Created a chunk of size 510, which is longer than the specified 200\n",
      "Created a chunk of size 332, which is longer than the specified 200\n",
      "Created a chunk of size 329, which is longer than the specified 200\n",
      "Created a chunk of size 460, which is longer than the specified 200\n",
      "Created a chunk of size 642, which is longer than the specified 200\n",
      "Created a chunk of size 389, which is longer than the specified 200\n",
      "Created a chunk of size 635, which is longer than the specified 200\n",
      "Created a chunk of size 404, which is longer than the specified 200\n",
      "Created a chunk of size 439, which is longer than the specified 200\n",
      "Created a chunk of size 753, which is longer than the specified 200\n",
      "Created a chunk of size 335, which is longer than the specified 200\n",
      "Created a chunk of size 289, which is longer than the specified 200\n",
      "Created a chunk of size 994, which is longer than the specified 200\n",
      "Created a chunk of size 231, which is longer than the specified 200\n",
      "Created a chunk of size 459, which is longer than the specified 200\n",
      "Created a chunk of size 528, which is longer than the specified 200\n",
      "Created a chunk of size 533, which is longer than the specified 200\n",
      "Created a chunk of size 910, which is longer than the specified 200\n",
      "Created a chunk of size 237, which is longer than the specified 200\n",
      "Created a chunk of size 300, which is longer than the specified 200\n",
      "Created a chunk of size 449, which is longer than the specified 200\n",
      "Created a chunk of size 465, which is longer than the specified 200\n",
      "Created a chunk of size 216, which is longer than the specified 200\n",
      "Created a chunk of size 249, which is longer than the specified 200\n",
      "Created a chunk of size 259, which is longer than the specified 200\n",
      "Created a chunk of size 257, which is longer than the specified 200\n",
      "Created a chunk of size 246, which is longer than the specified 200\n",
      "Created a chunk of size 324, which is longer than the specified 200\n",
      "Created a chunk of size 941, which is longer than the specified 200\n",
      "Created a chunk of size 255, which is longer than the specified 200\n",
      "Created a chunk of size 473, which is longer than the specified 200\n",
      "Created a chunk of size 208, which is longer than the specified 200\n",
      "Created a chunk of size 397, which is longer than the specified 200\n",
      "Created a chunk of size 342, which is longer than the specified 200\n",
      "Created a chunk of size 234, which is longer than the specified 200\n",
      "Created a chunk of size 282, which is longer than the specified 200\n",
      "Created a chunk of size 350, which is longer than the specified 200\n",
      "Created a chunk of size 390, which is longer than the specified 200\n",
      "Created a chunk of size 249, which is longer than the specified 200\n",
      "Created a chunk of size 584, which is longer than the specified 200\n",
      "Created a chunk of size 1456, which is longer than the specified 200\n",
      "Created a chunk of size 1267, which is longer than the specified 200\n",
      "Created a chunk of size 639, which is longer than the specified 200\n",
      "Created a chunk of size 303, which is longer than the specified 200\n",
      "Created a chunk of size 213, which is longer than the specified 200\n",
      "Created a chunk of size 2498, which is longer than the specified 200\n",
      "Created a chunk of size 299, which is longer than the specified 200\n",
      "Created a chunk of size 375, which is longer than the specified 200\n",
      "Created a chunk of size 217, which is longer than the specified 200\n",
      "Created a chunk of size 297, which is longer than the specified 200\n",
      "Created a chunk of size 285, which is longer than the specified 200\n",
      "Created a chunk of size 1533, which is longer than the specified 200\n",
      "Created a chunk of size 255, which is longer than the specified 200\n",
      "Created a chunk of size 323, which is longer than the specified 200\n",
      "Created a chunk of size 556, which is longer than the specified 200\n",
      "Created a chunk of size 328, which is longer than the specified 200\n",
      "Created a chunk of size 417, which is longer than the specified 200\n",
      "Created a chunk of size 593, which is longer than the specified 200\n",
      "Created a chunk of size 1139, which is longer than the specified 200\n",
      "Created a chunk of size 749, which is longer than the specified 200\n",
      "Created a chunk of size 1049, which is longer than the specified 200\n",
      "Created a chunk of size 411, which is longer than the specified 200\n",
      "Created a chunk of size 1418, which is longer than the specified 200\n",
      "Created a chunk of size 276, which is longer than the specified 200\n",
      "Created a chunk of size 382, which is longer than the specified 200\n",
      "Created a chunk of size 663, which is longer than the specified 200\n",
      "Created a chunk of size 282, which is longer than the specified 200\n",
      "Created a chunk of size 437, which is longer than the specified 200\n",
      "Created a chunk of size 375, which is longer than the specified 200\n",
      "Created a chunk of size 1319, which is longer than the specified 200\n",
      "Created a chunk of size 269, which is longer than the specified 200\n",
      "Created a chunk of size 378, which is longer than the specified 200\n",
      "Created a chunk of size 295, which is longer than the specified 200\n",
      "Created a chunk of size 325, which is longer than the specified 200\n",
      "Created a chunk of size 231, which is longer than the specified 200\n",
      "Created a chunk of size 217, which is longer than the specified 200\n",
      "Created a chunk of size 204, which is longer than the specified 200\n",
      "Created a chunk of size 208, which is longer than the specified 200\n",
      "Created a chunk of size 512, which is longer than the specified 200\n",
      "Created a chunk of size 628, which is longer than the specified 200\n",
      "Created a chunk of size 286, which is longer than the specified 200\n",
      "Created a chunk of size 590, which is longer than the specified 200\n",
      "Created a chunk of size 396, which is longer than the specified 200\n",
      "Created a chunk of size 1655, which is longer than the specified 200\n",
      "Created a chunk of size 1010, which is longer than the specified 200\n",
      "Created a chunk of size 243, which is longer than the specified 200\n",
      "Created a chunk of size 206, which is longer than the specified 200\n",
      "Created a chunk of size 210, which is longer than the specified 200\n",
      "Created a chunk of size 213, which is longer than the specified 200\n",
      "Created a chunk of size 411, which is longer than the specified 200\n",
      "Created a chunk of size 564, which is longer than the specified 200\n",
      "Created a chunk of size 1172, which is longer than the specified 200\n",
      "Created a chunk of size 233, which is longer than the specified 200\n",
      "Created a chunk of size 235, which is longer than the specified 200\n",
      "Created a chunk of size 708, which is longer than the specified 200\n",
      "Created a chunk of size 748, which is longer than the specified 200\n",
      "Created a chunk of size 649, which is longer than the specified 200\n",
      "Created a chunk of size 718, which is longer than the specified 200\n",
      "Created a chunk of size 308, which is longer than the specified 200\n",
      "Created a chunk of size 476, which is longer than the specified 200\n",
      "Created a chunk of size 668, which is longer than the specified 200\n",
      "Created a chunk of size 216, which is longer than the specified 200\n",
      "Created a chunk of size 262, which is longer than the specified 200\n",
      "Created a chunk of size 369, which is longer than the specified 200\n",
      "Created a chunk of size 499, which is longer than the specified 200\n",
      "Created a chunk of size 341, which is longer than the specified 200\n",
      "Created a chunk of size 222, which is longer than the specified 200\n",
      "Created a chunk of size 307, which is longer than the specified 200\n",
      "Created a chunk of size 680, which is longer than the specified 200\n",
      "Created a chunk of size 278, which is longer than the specified 200\n",
      "Created a chunk of size 530, which is longer than the specified 200\n",
      "Created a chunk of size 324, which is longer than the specified 200\n",
      "Created a chunk of size 201, which is longer than the specified 200\n",
      "Created a chunk of size 332, which is longer than the specified 200\n",
      "Created a chunk of size 312, which is longer than the specified 200\n",
      "Created a chunk of size 331, which is longer than the specified 200\n",
      "Created a chunk of size 1022, which is longer than the specified 200\n",
      "Created a chunk of size 566, which is longer than the specified 200\n",
      "Created a chunk of size 309, which is longer than the specified 200\n",
      "Created a chunk of size 305, which is longer than the specified 200\n",
      "Created a chunk of size 221, which is longer than the specified 200\n",
      "Created a chunk of size 476, which is longer than the specified 200\n",
      "Created a chunk of size 338, which is longer than the specified 200\n",
      "Created a chunk of size 309, which is longer than the specified 200\n",
      "Created a chunk of size 499, which is longer than the specified 200\n",
      "Created a chunk of size 233, which is longer than the specified 200\n",
      "Created a chunk of size 1054, which is longer than the specified 200\n",
      "Created a chunk of size 206, which is longer than the specified 200\n",
      "Created a chunk of size 905, which is longer than the specified 200\n",
      "Created a chunk of size 211, which is longer than the specified 200\n",
      "Created a chunk of size 317, which is longer than the specified 200\n",
      "Created a chunk of size 611, which is longer than the specified 200\n",
      "Created a chunk of size 753, which is longer than the specified 200\n",
      "Created a chunk of size 380, which is longer than the specified 200\n",
      "Created a chunk of size 497, which is longer than the specified 200\n",
      "Created a chunk of size 585, which is longer than the specified 200\n",
      "Created a chunk of size 394, which is longer than the specified 200\n",
      "Created a chunk of size 670, which is longer than the specified 200\n",
      "Created a chunk of size 840, which is longer than the specified 200\n",
      "Created a chunk of size 309, which is longer than the specified 200\n",
      "Created a chunk of size 221, which is longer than the specified 200\n",
      "Created a chunk of size 237, which is longer than the specified 200\n",
      "Created a chunk of size 324, which is longer than the specified 200\n",
      "Created a chunk of size 285, which is longer than the specified 200\n",
      "Created a chunk of size 243, which is longer than the specified 200\n",
      "Created a chunk of size 260, which is longer than the specified 200\n",
      "Created a chunk of size 264, which is longer than the specified 200\n",
      "Created a chunk of size 294, which is longer than the specified 200\n",
      "Created a chunk of size 225, which is longer than the specified 200\n",
      "Created a chunk of size 262, which is longer than the specified 200\n",
      "Created a chunk of size 819, which is longer than the specified 200\n",
      "Created a chunk of size 423, which is longer than the specified 200\n",
      "Created a chunk of size 268, which is longer than the specified 200\n",
      "Created a chunk of size 265, which is longer than the specified 200\n",
      "Created a chunk of size 617, which is longer than the specified 200\n",
      "Created a chunk of size 607, which is longer than the specified 200\n",
      "Created a chunk of size 215, which is longer than the specified 200\n",
      "Created a chunk of size 435, which is longer than the specified 200\n",
      "Created a chunk of size 391, which is longer than the specified 200\n",
      "Created a chunk of size 208, which is longer than the specified 200\n",
      "Created a chunk of size 236, which is longer than the specified 200\n",
      "Created a chunk of size 458, which is longer than the specified 200\n",
      "Created a chunk of size 308, which is longer than the specified 200\n",
      "Created a chunk of size 391, which is longer than the specified 200\n",
      "Created a chunk of size 567, which is longer than the specified 200\n",
      "Created a chunk of size 398, which is longer than the specified 200\n",
      "Created a chunk of size 428, which is longer than the specified 200\n",
      "Created a chunk of size 367, which is longer than the specified 200\n",
      "Created a chunk of size 247, which is longer than the specified 200\n",
      "Created a chunk of size 384, which is longer than the specified 200\n",
      "Created a chunk of size 309, which is longer than the specified 200\n",
      "Created a chunk of size 239, which is longer than the specified 200\n",
      "Created a chunk of size 324, which is longer than the specified 200\n",
      "Created a chunk of size 551, which is longer than the specified 200\n",
      "Created a chunk of size 280, which is longer than the specified 200\n",
      "Created a chunk of size 369, which is longer than the specified 200\n",
      "Created a chunk of size 418, which is longer than the specified 200\n",
      "Created a chunk of size 764, which is longer than the specified 200\n",
      "Created a chunk of size 359, which is longer than the specified 200\n",
      "Created a chunk of size 572, which is longer than the specified 200\n",
      "Created a chunk of size 363, which is longer than the specified 200\n",
      "Created a chunk of size 375, which is longer than the specified 200\n",
      "Created a chunk of size 309, which is longer than the specified 200\n",
      "Created a chunk of size 298, which is longer than the specified 200\n",
      "Created a chunk of size 214, which is longer than the specified 200\n",
      "Created a chunk of size 874, which is longer than the specified 200\n",
      "Created a chunk of size 435, which is longer than the specified 200\n",
      "Created a chunk of size 344, which is longer than the specified 200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='[The Tragedie of Hamlet by William Shakespeare 1599]\\n\\n\\nActus Primus. Scoena Prima.\\n\\nEnter Barnardo and Francisco two Centinels.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Barnardo. Who's there?\\n  Fran. Nay answer me: Stand & vnfold\\nyour selfe\\n\\n   Bar. Long liue the King\\n\\n   Fran. Barnardo?\\n  Bar. He\\n\\n   Fran. You come most carefully vpon your houre\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Bar. 'Tis now strook twelue, get thee to bed Francisco\\n\\n   Fran. For this releefe much thankes: 'Tis bitter cold,\\nAnd I am sicke at heart\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Barn. Haue you had quiet Guard?\\n  Fran. Not a Mouse stirring'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Barn. Well, goodnight. If you do meet Horatio and\\nMarcellus, the Riuals of my Watch, bid them make hast.\\nEnter Horatio and Marcellus.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Fran. I thinke I heare them. Stand: who's there?\\n  Hor. Friends to this ground\\n\\n   Mar. And Leige-men to the Dane\\n\\n   Fran. Giue you good night\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Mar. O farwel honest Soldier, who hath relieu'd you?\\n  Fra. Barnardo ha's my place: giue you goodnight.\\n\\nExit Fran.\\n\\n  Mar. Holla Barnardo\\n\\n   Bar. Say, what is Horatio there?\\n  Hor. A peece of him\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Bar. Welcome Horatio, welcome good Marcellus\\n\\n   Mar. What, ha's this thing appear'd againe to night\\n\\n   Bar. I haue seene nothing\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Mar. Horatio saies, 'tis but our Fantasie,\\nAnd will not let beleefe take hold of him\\nTouching this dreaded sight, twice seene of vs,\\nTherefore I haue intreated him along\\nWith vs, to watch the minutes of this Night,\\nThat if againe this Apparition come,\\nHe may approue our eyes, and speake to it\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Hor. Tush, tush, 'twill not appeare\\n\\n   Bar. Sit downe a-while,\\nAnd let vs once againe assaile your eares,\\nThat are so fortified against our Story,\\nWhat we two Nights haue seene\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Hor. Well, sit we downe,\\nAnd let vs heare Barnardo speake of this'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Barn. Last night of all,\\nWhen yond same Starre that's Westward from the Pole\\nHad made his course t' illume that part of Heauen\\nWhere now it burnes, Marcellus and my selfe,\\nThe Bell then beating one\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Mar. Peace, breake thee of:\\nEnter the Ghost.\\n\\nLooke where it comes againe\\n\\n   Barn. In the same figure, like the King that's dead\\n\\n   Mar. Thou art a Scholler; speake to it Horatio\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Barn. Lookes it not like the King? Marke it Horatio\\n\\n   Hora. Most like: It harrowes me with fear & wonder\\n  Barn. It would be spoke too\\n\\n   Mar. Question it Horatio'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Hor. What art thou that vsurp'st this time of night,\\nTogether with that Faire and Warlike forme\\nIn which the Maiesty of buried Denmarke\\nDid sometimes march: By Heauen I charge thee speake\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Mar. It is offended\\n\\n   Barn. See, it stalkes away\\n\\n   Hor. Stay: speake; speake: I Charge thee, speake.\\n\\nExit the Ghost.\\n\\n  Mar. 'Tis gone, and will not answer\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Barn. How now Horatio? You tremble & look pale:\\nIs not this something more then Fantasie?\\nWhat thinke you on't?\\n  Hor. Before my God, I might not this beleeue\\nWithout the sensible and true auouch\\nOf mine owne eyes\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Mar. Is it not like the King?\\n  Hor. As thou art to thy selfe,\\nSuch was the very Armour he had on,\\nWhen th' Ambitious Norwey combatted:\\nSo frown'd he once, when in an angry parle\\nHe smot the sledded Pollax on the Ice.\\n'Tis strange\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Mar. Thus twice before, and iust at this dead houre,\\nWith Martiall stalke, hath he gone by our Watch'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Hor. In what particular thought to work, I know not:\\nBut in the grosse and scope of my Opinion,\\nThis boades some strange erruption to our State'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Mar. Good now sit downe, & tell me he that knowes\\nWhy this same strict and most obseruant Watch,\\nSo nightly toyles the subiect of the Land,\\nAnd why such dayly Cast of Brazon Cannon\\nAnd Forraigne Mart for Implements of warre:\\nWhy such impresse of Ship-wrights, whose sore Taske\\nDo's not diuide the Sunday from the weeke,\\nWhat might be toward, that this sweaty hast\\nDoth make the Night ioynt-Labourer with the day:\\nWho is't that can informe me?\\n  Hor. That can I,\\nAt least the whisper goes so: Our last King,\\nWhose Image euen but now appear'd to vs,\\nWas (as you know) by Fortinbras of Norway,\\n(Thereto prick'd on by a most emulate Pride)\\nDar'd to the Combate. In which, our Valiant Hamlet,\\n(For so this side of our knowne world esteem'd him)\\nDid slay this Fortinbras: who by a Seal'd Compact,\\nWell ratified by Law, and Heraldrie,\\nDid forfeite (with his life) all those his Lands\\nWhich he stood seiz'd on, to the Conqueror:\\nAgainst the which, a Moity competent\\nWas gaged by our King: which had return'd\\nTo the Inheritance of Fortinbras,\\nHad he bin Vanquisher, as by the same Cou'nant\\nAnd carriage of the Article designe,\\nHis fell to Hamlet. Now sir, young Fortinbras,\\nOf vnimproued Mettle, hot and full,\\nHath in the skirts of Norway, heere and there,\\nShark'd vp a List of Landlesse Resolutes,\\nFor Foode and Diet, to some Enterprize\\nThat hath a stomacke in't: which is no other\\n(And it doth well appeare vnto our State)\\nBut to recouer of vs by strong hand\\nAnd termes Compulsatiue, those foresaid Lands\\nSo by his Father lost: and this (I take it)\\nIs the maine Motiue of our Preparations,\\nThe Sourse of this our Watch, and the cheefe head\\nOf this post-hast, and Romage in the Land.\\nEnter Ghost againe.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='But soft, behold: Loe, where it comes againe:\\nIle crosse it, though it blast me. Stay Illusion:\\nIf thou hast any sound, or vse of Voyce,\\nSpeake to me. If there be any good thing to be done,\\nThat may to thee do ease, and grace to me; speak to me.\\nIf thou art priuy to thy Countries Fate\\n(Which happily foreknowing may auoyd) Oh speake.\\nOr, if thou hast vp-hoorded in thy life\\nExtorted Treasure in the wombe of Earth,\\n(For which, they say, you Spirits oft walke in death)\\nSpeake of it. Stay, and speake. Stop it Marcellus'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Mar. Shall I strike at it with my Partizan?\\n  Hor. Do, if it will not stand\\n\\n   Barn. 'Tis heere\\n\\n   Hor. 'Tis heere\\n\\n   Mar. 'Tis gone.\\n\\nExit Ghost.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Exit Ghost.\\n\\nWe do it wrong, being so Maiesticall\\nTo offer it the shew of Violence,\\nFor it is as the Ayre, invulnerable,\\nAnd our vaine blowes, malicious Mockery'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Barn. It was about to speake, when the Cocke crew'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Hor. And then it started, like a guilty thing\\nVpon a fearfull Summons. I haue heard,\\nThe Cocke that is the Trumpet to the day,\\nDoth with his lofty and shrill-sounding Throate\\nAwake the God of Day: and at his warning,\\nWhether in Sea, or Fire, in Earth, or Ayre,\\nTh' extrauagant, and erring Spirit, hyes\\nTo his Confine. And of the truth heerein,\\nThis present Obiect made probation\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Mar. It faded on the crowing of the Cocke.\\nSome sayes, that euer 'gainst that Season comes\\nWherein our Sauiours Birch is celebrated,\\nThe Bird of Dawning singeth all night long:\\nAnd then (they say) no Spirit can walke abroad,\\nThe nights are wholsome, then no Planets strike,\\nNo Faiery talkes, nor Witch hath power to Charme:\\nSo hallow'd, and so gracious is the time\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Hor. So haue I heard, and do in part beleeue it.\\nBut looke, the Morne in Russet mantle clad,\\nWalkes o're the dew of yon high Easterne Hill,\\nBreake we our Watch vp, and by my aduice\\nLet vs impart what we haue seene to night\\nVnto yong Hamlet. For vpon my life,\\nThis Spirit dumbe to vs, will speake to him:\\nDo you consent we shall acquaint him with it,\\nAs needfull in our Loues, fitting our Duty?\\n  Mar. Let do't I pray, and I this morning know\\nWhere we shall finde him most conueniently.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Exeunt.\\n\\nScena Secunda.\\n\\nEnter Claudius King of Denmarke, Gertrude the Queene, Hamlet,\\nPolonius,\\nLaertes, and his Sister Ophelia, Lords Attendant.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"King. Though yet of Hamlet our deere Brothers death\\nThe memory be greene: and that it vs befitted\\nTo beare our hearts in greefe, and our whole Kingdome\\nTo be contracted in one brow of woe:\\nYet so farre hath Discretion fought with Nature,\\nThat we with wisest sorrow thinke on him,\\nTogether with remembrance of our selues.\\nTherefore our sometimes Sister, now our Queene,\\nTh' imperiall Ioyntresse of this warlike State,\\nHaue we, as 'twere, with a defeated ioy,\\nWith one Auspicious, and one Dropping eye,\\nWith mirth in Funerall, and with Dirge in Marriage,\\nIn equall Scale weighing Delight and Dole\\nTaken to Wife; nor haue we heerein barr'd\\nYour better Wisedomes, which haue freely gone\\nWith this affaire along, for all our Thankes.\\nNow followes, that you know young Fortinbras,\\nHolding a weake supposall of our worth;\\nOr thinking by our late deere Brothers death,\\nOur State to be disioynt, and out of Frame,\\nColleagued with the dreame of his Aduantage;\\nHe hath not fayl'd to pester vs with Message,\\nImporting the surrender of those Lands\\nLost by his Father: with all Bonds of Law\\nTo our most valiant Brother. So much for him.\\nEnter Voltemand and Cornelius.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Now for our selfe, and for this time of meeting\\nThus much the businesse is. We haue heere writ\\nTo Norway, Vncle of young Fortinbras,\\nWho Impotent and Bedrid, scarsely heares\\nOf this his Nephewes purpose, to suppresse\\nHis further gate heerein. In that the Leuies,\\nThe Lists, and full proportions are all made\\nOut of his subiect: and we heere dispatch\\nYou good Cornelius, and you Voltemand,\\nFor bearing of this greeting to old Norway,\\nGiuing to you no further personall power\\nTo businesse with the King, more then the scope\\nOf these dilated Articles allow:\\nFarewell, and let your hast commend your duty'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Volt. In that, and all things, will we shew our duty\\n\\n   King. We doubt it nothing, heartily farewell.\\n\\nExit Voltemand and Cornelius.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"And now Laertes, what's the newes with you?\\nYou told vs of some suite. What is't Laertes?\\nYou cannot speake of Reason to the Dane,\\nAnd loose your voyce. What would'st thou beg Laertes,\\nThat shall not be my Offer, not thy Asking?\\nThe Head is not more Natiue to the Heart,\\nThe Hand more instrumentall to the Mouth,\\nThen is the Throne of Denmarke to thy Father.\\nWhat would'st thou haue Laertes?\\n  Laer. Dread my Lord,\\nYour leaue and fauour to returne to France,\\nFrom whence, though willingly I came to Denmarke\\nTo shew my duty in your Coronation,\\nYet now I must confesse, that duty done,\\nMy thoughts and wishes bend againe towards France,\\nAnd bow them to your gracious leaue and pardon\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='King. Haue you your Fathers leaue?\\nWhat sayes Pollonius?\\n  Pol. He hath my Lord:\\nI do beseech you giue him leaue to go'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='King. Take thy faire houre Laertes, time be thine,\\nAnd thy best graces spend it at thy will:\\nBut now my Cosin Hamlet, and my Sonne?\\n  Ham. A little more then kin, and lesse then kinde'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"King. How is it that the Clouds still hang on you?\\n  Ham. Not so my Lord, I am too much i'th' Sun\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Queen. Good Hamlet cast thy nightly colour off,\\nAnd let thine eye looke like a Friend on Denmarke.\\nDo not for euer with thy veyled lids\\nSeeke for thy Noble Father in the dust;\\nThou know'st 'tis common, all that liues must dye,\\nPassing through Nature, to Eternity\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. I Madam, it is common\\n\\n   Queen. If it be;\\nWhy seemes it so particular with thee'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Seemes Madam? Nay, it is: I know not Seemes:\\n'Tis not alone my Inky Cloake (good Mother)\\nNor Customary suites of solemne Blacke,\\nNor windy suspiration of forc'd breath,\\nNo, nor the fruitfull Riuer in the Eye,\\nNor the deiected hauiour of the Visage,\\nTogether with all Formes, Moods, shewes of Griefe,\\nThat can denote me truly. These indeed Seeme,\\nFor they are actions that a man might play:\\nBut I haue that Within, which passeth show;\\nThese, but the Trappings, and the Suites of woe\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"King. 'Tis sweet and commendable\\nIn your Nature Hamlet,\\nTo giue these mourning duties to your Father:\\nBut you must know, your Father lost a Father,\\nThat Father lost, lost his, and the Suruiuer bound\\nIn filiall Obligation, for some terme\\nTo do obsequious Sorrow. But to perseuer\\nIn obstinate Condolement, is a course\\nOf impious stubbornnesse. 'Tis vnmanly greefe,\\nIt shewes a will most incorrect to Heauen,\\nA Heart vnfortified, a Minde impatient,\\nAn Vnderstanding simple, and vnschool'd:\\nFor, what we know must be, and is as common\\nAs any the most vulgar thing to sence,\\nWhy should we in our peeuish Opposition\\nTake it to heart? Fye, 'tis a fault to Heauen,\\nA fault against the Dead, a fault to Nature,\\nTo Reason most absurd, whose common Theame\\nIs death of Fathers, and who still hath cried,\\nFrom the first Coarse, till he that dyed to day,\\nThis must be so. We pray you throw to earth\\nThis vnpreuayling woe, and thinke of vs\\nAs of a Father; For let the world take note,\\nYou are the most immediate to our Throne,\\nAnd with no lesse Nobility of Loue,\\nThen that which deerest Father beares his Sonne,\\nDo I impart towards you. For your intent\\nIn going backe to Schoole in Wittenberg,\\nIt is most retrograde to our desire:\\nAnd we beseech you, bend you to remaine\\nHeere in the cheere and comfort of our eye,\\nOur cheefest Courtier Cosin, and our Sonne\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Qu. Let not thy Mother lose her Prayers Hamlet:\\nI prythee stay with vs, go not to Wittenberg\\n\\n   Ham. I shall in all my best\\nObey you Madam'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"King. Why 'tis a louing, and a faire Reply,\\nBe as our selfe in Denmarke. Madam come,\\nThis gentle and vnforc'd accord of Hamlet\\nSits smiling to my heart; in grace whereof,\\nNo iocond health that Denmarke drinkes to day,\\nBut the great Cannon to the Clowds shall tell,\\nAnd the Kings Rouce, the Heauens shall bruite againe,\\nRespeaking earthly Thunder. Come away.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Exeunt.\\n\\nManet Hamlet.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Oh that this too too solid Flesh, would melt,\\nThaw, and resolue it selfe into a Dew:\\nOr that the Euerlasting had not fixt\\nHis Cannon 'gainst Selfe-slaughter. O God, O God!\\nHow weary, stale, flat, and vnprofitable\\nSeemes to me all the vses of this world?\\nFie on't? Oh fie, fie, 'tis an vnweeded Garden\\nThat growes to Seed: Things rank, and grosse in Nature\\nPossesse it meerely. That it should come to this:\\nBut two months dead: Nay, not so much; not two,\\nSo excellent a King, that was to this\\nHiperion to a Satyre: so louing to my Mother,\\nThat he might not beteene the windes of heauen\\nVisit her face too roughly. Heauen and Earth\\nMust I remember: why she would hang on him,\\nAs if encrease of Appetite had growne\\nBy what is fed on; and yet within a month?\\nLet me not thinke on't: Frailty, thy name is woman.\\nA little Month, or ere those shooes were old,\\nWith which she followed my poore Fathers body\\nLike Niobe, all teares. Why she, euen she.\\n(O Heauen! A beast that wants discourse of Reason\\nWould haue mourn'd longer) married with mine Vnkle,\\nMy Fathers Brother: but no more like my Father,\\nThen I to Hercules. Within a Moneth?\\nEre yet the salt of most vnrighteous Teares\\nHad left the flushing of her gauled eyes,\\nShe married. O most wicked speed, to post\\nWith such dexterity to Incestuous sheets:\\nIt is not, nor it cannot come to good.\\nBut breake my heart, for I must hold my tongue.\\nEnter Horatio, Barnardo, and Marcellus.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Hor. Haile to your Lordship\\n\\n   Ham. I am glad to see you well:\\nHoratio, or I do forget my selfe\\n\\n   Hor. The same my Lord,\\nAnd your poore Seruant euer'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. Sir my good friend,\\nIle change that name with you:\\nAnd what make you from Wittenberg Horatio?\\nMarcellus\\n\\n   Mar. My good Lord'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Mar. My good Lord\\n\\n   Ham. I am very glad to see you: good euen Sir.\\nBut what in faith make you from Wittemberge?\\n  Hor. A truant disposition, good my Lord'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. I would not haue your Enemy say so;\\nNor shall you doe mine eare that violence,\\nTo make it truster of your owne report\\nAgainst your selfe. I know you are no Truant:\\nBut what is your affaire in Elsenour?\\nWee'l teach you to drinke deepe, ere you depart\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Hor. My Lord, I came to see your Fathers Funerall\\n\\n   Ham. I pray thee doe not mock me (fellow Student)\\nI thinke it was to see my Mothers Wedding\\n\\n   Hor. Indeed my Lord, it followed hard vpon'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. Thrift thrift Horatio: the Funerall Bakt-meats\\nDid coldly furnish forth the Marriage Tables;\\nWould I had met my dearest foe in heauen,\\nEre I had euer seene that day Horatio.\\nMy father, me thinkes I see my father'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Hor. Oh where my Lord?\\n  Ham. In my minds eye (Horatio)\\n  Hor. I saw him once; he was a goodly King\\n\\n   Ham. He was a man, take him for all in all:\\nI shall not look vpon his like againe'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Hor. My Lord, I thinke I saw him yesternight\\n\\n   Ham. Saw? Who?\\n  Hor. My Lord, the King your Father'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. The King my Father?\\n  Hor. Season your admiration for a while\\nWith an attent eare; till I may deliuer\\nVpon the witnesse of these Gentlemen,\\nThis maruell to you'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. For Heauens loue let me heare'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Hor. Two nights together, had these Gentlemen\\n(Marcellus and Barnardo) on their Watch\\nIn the dead wast and middle of the night\\nBeene thus encountred. A figure like your Father,\\nArm'd at all points exactly, Cap a Pe,\\nAppeares before them, and with sollemne march\\nGoes slow and stately: By them thrice he walkt,\\nBy their opprest and feare-surprized eyes,\\nWithin his Truncheons length; whilst they bestil'd\\nAlmost to Ielly with the Act of feare,\\nStand dumbe and speake not to him. This to me\\nIn dreadfull secrecie impart they did,\\nAnd I with them the third Night kept the Watch,\\nWhereas they had deliuer'd both in time,\\nForme of the thing; each word made true and good,\\nThe Apparition comes. I knew your Father:\\nThese hands are not more like\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. But where was this?\\n  Mar. My Lord vpon the platforme where we watcht'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. Did you not speake to it?\\n  Hor. My Lord, I did;\\nBut answere made it none: yet once me thought\\nIt lifted vp it head, and did addresse\\nIt selfe to motion, like as it would speake:\\nBut euen then, the Morning Cocke crew lowd;\\nAnd at the sound it shrunke in hast away,\\nAnd vanisht from our sight'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Tis very strange\\n\\n   Hor. As I doe liue my honourd Lord 'tis true;\\nAnd we did thinke it writ downe in our duty\\nTo let you know of it\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Indeed, indeed Sirs; but this troubles me.\\nHold you the watch to Night?\\n  Both. We doe my Lord\\n\\n   Ham. Arm'd, say you?\\n  Both. Arm'd, my Lord\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. From top to toe?\\n  Both. My Lord, from head to foote\\n\\n   Ham. Then saw you not his face?\\n  Hor. O yes, my Lord, he wore his Beauer vp'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. What, lookt he frowningly?\\n  Hor. A countenance more in sorrow then in anger\\n\\n   Ham. Pale, or red?\\n  Hor. Nay very pale\\n\\n   Ham. And fixt his eyes vpon you?\\n  Hor. Most constantly'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. I would I had beene there\\n\\n   Hor. It would haue much amaz'd you\\n\\n   Ham. Very like, very like: staid it long?\\n  Hor. While one with moderate hast might tell a hundred\\n\\n   All. Longer, longer\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Hor. Not when I saw't\\n\\n   Ham. His Beard was grisly? no\\n\\n   Hor. It was, as I haue seene it in his life,\\nA Sable Siluer'd\\n\\n   Ham. Ile watch to Night; perchance 'twill wake againe\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Hor. I warrant you it will'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. If it assume my noble Fathers person,\\nIle speake to it, though Hell it selfe should gape\\nAnd bid me hold my peace. I pray you all,\\nIf you haue hitherto conceald this sight;\\nLet it bee treble in your silence still:\\nAnd whatsoeuer els shall hap to night,\\nGiue it an vnderstanding but no tongue;\\nI will requite your loues; so fare ye well:\\nVpon the Platforme twixt eleuen and twelue,\\nIle visit you'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='All. Our duty to your Honour.\\n\\nExeunt'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. Your loue, as mine to you: farewell.\\nMy Fathers Spirit in Armes? All is not well:\\nI doubt some foule play: would the Night were come;\\nTill then sit still my soule; foule deeds will rise,\\nThough all the earth orewhelm them to mens eies.\\nEnter.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Scena Tertia\\n\\n\\nEnter Laertes and Ophelia.\\n\\n  Laer. My necessaries are imbark't; Farewell:\\nAnd Sister, as the Winds giue Benefit,\\nAnd Conuoy is assistant; doe not sleepe,\\nBut let me heare from you\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ophel. Doe you doubt that?\\n  Laer. For Hamlet, and the trifling of his fauours,\\nHold it a fashion and a toy in Bloude;\\nA Violet in the youth of Primy Nature;\\nFroward, not permanent; sweet not lasting\\nThe suppliance of a minute? No more'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ophel. No more but so'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Laer. Thinke it no more:\\nFor nature cressant does not grow alone,\\nIn thewes and Bulke: but as his Temple waxes,\\nThe inward seruice of the Minde and Soule\\nGrowes wide withall. Perhaps he loues you now,\\nAnd now no soyle nor cautell doth besmerch\\nThe vertue of his feare: but you must feare\\nHis greatnesse weigh'd, his will is not his owne;\\nFor hee himselfe is subiect to his Birth:\\nHee may not, as vnuallued persons doe,\\nCarue for himselfe; for, on his choyce depends\\nThe sanctity and health of the whole State.\\nAnd therefore must his choyce be circumscrib'd\\nVnto the voyce and yeelding of that Body,\\nWhereof he is the Head. Then if he sayes he loues you,\\nIt fits your wisedome so farre to beleeue it;\\nAs he in his peculiar Sect and force\\nMay giue his saying deed: which is no further,\\nThen the maine voyce of Denmarke goes withall.\\nThen weight what losse your Honour may sustaine,\\nIf with too credent eare you list his Songs;\\nOr lose your Heart; or your chast Treasure open\\nTo his vnmastred importunity.\\nFeare it Ophelia, feare it my deare Sister,\\nAnd keepe within the reare of your Affection;\\nOut of the shot and danger of Desire.\\nThe chariest Maid is Prodigall enough,\\nIf she vnmaske her beauty to the Moone:\\nVertue it selfe scapes not calumnious stroakes,\\nThe Canker Galls, the Infants of the Spring\\nToo oft before the buttons be disclos'd,\\nAnd in the Morne and liquid dew of Youth,\\nContagious blastments are most imminent.\\nBe wary then, best safety lies in feare;\\nYouth to it selfe rebels, though none else neere\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ophe. I shall th' effect of this good Lesson keepe,\\nAs watchmen to my heart: but good my Brother\\nDoe not as some vngracious Pastors doe,\\nShew me the steepe and thorny way to Heauen;\\nWhilst like a puft and recklesse Libertine\\nHimselfe, the Primrose path of dalliance treads,\\nAnd reaks not his owne reade\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Laer. Oh, feare me not.\\nEnter Polonius.\\n\\nI stay too long; but here my Father comes:\\nA double blessing is a double grace;\\nOccasion smiles vpon a second leaue'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Polon. Yet heere Laertes? Aboord, aboord for shame,\\nThe winde sits in the shoulder of your saile,\\nAnd you are staid for there: my blessing with you;\\nAnd these few Precepts in thy memory,\\nSee thou Character. Giue thy thoughts no tongue,\\nNor any vnproportion'd thoughts his Act:\\nBe thou familiar; but by no meanes vulgar:\\nThe friends thou hast, and their adoption tride,\\nGrapple them to thy Soule, with hoopes of Steele:\\nBut doe not dull thy palme, with entertainment\\nOf each vnhatch't, vnfledg'd Comrade. Beware\\nOf entrance to a quarrell: but being in\\nBear't that th' opposed may beware of thee.\\nGiue euery man thine eare; but few thy voyce:\\nTake each mans censure; but reserue thy iudgement:\\nCostly thy habit as thy purse can buy;\\nBut not exprest in fancie; rich, not gawdie:\\nFor the Apparell oft proclaimes the man.\\nAnd they in France of the best ranck and station,\\nAre of a most select and generous cheff in that.\\nNeither a borrower, nor a lender be;\\nFor lone oft loses both it selfe and friend:\\nAnd borrowing duls the edge of Husbandry.\\nThis aboue all; to thine owne selfe be true:\\nAnd it must follow, as the Night the Day,\\nThou canst not then be false to any man.\\nFarewell: my Blessing season this in thee\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Laer. Most humbly doe I take my leaue, my Lord\\n\\n   Polon. The time inuites you, goe, your seruants tend\\n\\n   Laer. Farewell Ophelia, and remember well\\nWhat I haue said to you'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ophe. Tis in my memory lockt,\\nAnd you your selfe shall keepe the key of it\\n\\n   Laer. Farewell.\\n\\nExit Laer.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Exit Laer.\\n\\n  Polon. What ist Ophelia he hath said to you?\\n  Ophe. So please you, somthing touching the L[ord]. Hamlet'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Polon. Marry, well bethought:\\nTis told me he hath very oft of late\\nGiuen priuate time to you; and you your selfe\\nHaue of your audience beene most free and bounteous.\\nIf it be so, as so tis put on me;\\nAnd that in way of caution: I must tell you,\\nYou doe not vnderstand your selfe so cleerely,\\nAs it behoues my Daughter, and your Honour.\\nWhat is betweene you, giue me vp the truth?\\n  Ophe. He hath my Lord of late, made many tenders\\nOf his affection to me'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Polon. Affection, puh. You speake like a greene Girle,\\nVnsifted in such perillous Circumstance.\\nDoe you beleeue his tenders, as you call them?\\n  Ophe. I do not know, my Lord, what I should thinke'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Polon. Marry Ile teach you; thinke your selfe a Baby,\\nThat you haue tane his tenders for true pay,\\nWhich are not starling. Tender your selfe more dearly;\\nOr not to crack the winde of the poore Phrase,\\nRoaming it thus, you'l tender me a foole\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ophe. My Lord, he hath importun'd me with loue,\\nIn honourable fashion\\n\\n   Polon. I, fashion you may call it, go too, go too\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ophe. And hath giuen countenance to his speech,\\nMy Lord, with all the vowes of Heauen'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Polon. I, Springes to catch Woodcocks. I doe know\\nWhen the Bloud burnes, how Prodigall the Soule\\nGiues the tongue vowes: these blazes, Daughter,\\nGiuing more light then heate; extinct in both,\\nEuen in their promise, as it is a making;\\nYou must not take for fire. For this time Daughter,\\nBe somewhat scanter of your Maiden presence;\\nSet your entreatments at a higher rate,\\nThen a command to parley. For Lord Hamlet,\\nBeleeue so much in him, that he is young,\\nAnd with a larger tether may he walke,\\nThen may be giuen you. In few, Ophelia,\\nDoe not beleeue his vowes; for they are Broakers,\\nNot of the eye, which their Inuestments show:\\nBut meere implorators of vnholy Sutes,\\nBreathing like sanctified and pious bonds,\\nThe better to beguile. This is for all:\\nI would not, in plaine tearmes, from this time forth,\\nHaue you so slander any moment leisure,\\nAs to giue words or talke with the Lord Hamlet:\\nLooke too't, I charge you; come your wayes\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ophe. I shall obey my Lord.\\n\\nExeunt.\\n\\nEnter Hamlet, Horatio, Marcellus.\\n\\n  Ham. The Ayre bites shrewdly: is it very cold?\\n  Hor. It is a nipping and an eager ayre'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. What hower now?\\n  Hor. I thinke it lacks of twelue\\n\\n   Mar. No, it is strooke'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Hor. Indeed I heard it not: then it drawes neere the season,\\nWherein the Spirit held his wont to walke.\\nWhat does this meane my Lord?\\n  Ham. The King doth wake to night, and takes his rouse,\\nKeepes wassels and the swaggering vpspring reeles,\\nAnd as he dreines his draughts of Renish downe,\\nThe kettle Drum and Trumpet thus bray out\\nThe triumph of his Pledge'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Horat. Is it a custome?\\n  Ham. I marry ist;\\nAnd to my mind, though I am natiue heere,\\nAnd to the manner borne: It is a Custome\\nMore honour'd in the breach, then the obseruance.\\nEnter Ghost.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Hor. Looke my Lord, it comes'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Angels and Ministers of Grace defend vs:\\nBe thou a Spirit of health, or Goblin damn'd,\\nBring with thee ayres from Heauen, or blasts from Hell,\\nBe thy euents wicked or charitable,\\nThou com'st in such a questionable shape\\nThat I will speake to thee. Ile call thee Hamlet,\\nKing, Father, Royall Dane: Oh, oh, answer me,\\nLet me not burst in Ignorance; but tell\\nWhy thy Canoniz'd bones Hearsed in death,\\nHaue burst their cerments, why the Sepulcher\\nWherein we saw thee quietly enurn'd,\\nHath op'd his ponderous and Marble iawes,\\nTo cast thee vp againe? What may this meane?\\nThat thou dead Coarse againe in compleat steele,\\nReuisits thus the glimpses of the Moone,\\nMaking Night hidious? And we fooles of Nature,\\nSo horridly to shake our disposition,\\nWith thoughts beyond thee; reaches of our Soules,\\nSay, why is this? wherefore? what should we doe?\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ghost beckens Hamlet.\\n\\n  Hor. It beckons you to goe away with it,\\nAs if it some impartment did desire\\nTo you alone'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Mar. Looke with what courteous action\\nIt wafts you to a more remoued ground:\\nBut doe not goe with it\\n\\n   Hor. No, by no meanes\\n\\n   Ham. It will not speake: then will I follow it'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Hor. Doe not my Lord'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. Why, what should be the feare?\\nI doe not set my life at a pins fee;\\nAnd for my Soule, what can it doe to that?\\nBeing a thing immortall as it selfe:\\nIt waues me forth againe; Ile follow it'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Hor. What if it tempt you toward the Floud my Lord?\\nOr to the dreadfull Sonnet of the Cliffe,\\nThat beetles o're his base into the Sea,\\nAnd there assumes some other horrible forme,\\nWhich might depriue your Soueraignty of Reason,\\nAnd draw you into madnesse thinke of it?\\n  Ham. It wafts me still: goe on, Ile follow thee\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Mar. You shall not goe my Lord\\n\\n   Ham. Hold off your hand\\n\\n   Hor. Be rul'd, you shall not goe\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. My fate cries out,\\nAnd makes each petty Artire in this body,\\nAs hardy as the Nemian Lions nerue:\\nStill am I cal'd? Vnhand me Gentlemen:\\nBy Heau'n, Ile make a Ghost of him that lets me:\\nI say away, goe on, Ile follow thee.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Exeunt. Ghost & Hamlet.\\n\\n  Hor. He waxes desperate with imagination\\n\\n   Mar. Let's follow; 'tis not fit thus to obey him\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Hor. Haue after, to what issue will this come?\\n  Mar. Something is rotten in the State of Denmarke\\n\\n   Hor. Heauen will direct it\\n\\n   Mar. Nay, let's follow him.\\n\\nExeunt.\\n\\nEnter Ghost and Hamlet.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. Where wilt thou lead me? speak; Ile go no further\\n\\n   Gho. Marke me\\n\\n   Ham. I will\\n\\n   Gho. My hower is almost come,\\nWhen I to sulphurous and tormenting Flames\\nMust render vp my selfe'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. Alas poore Ghost\\n\\n   Gho. Pitty me not, but lend thy serious hearing\\nTo what I shall vnfold\\n\\n   Ham. Speake, I am bound to heare\\n\\n   Gho. So art thou to reuenge, when thou shalt heare'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. What?\\n  Gho. I am thy Fathers Spirit,\\nDoom'd for a certaine terme to walke the night;\\nAnd for the day confin'd to fast in Fiers,\\nTill the foule crimes done in my dayes of Nature\\nAre burnt and purg'd away? But that I am forbid\\nTo tell the secrets of my Prison-House;\\nI could a Tale vnfold, whose lightest word\\nWould harrow vp thy soule, freeze thy young blood,\\nMake thy two eyes like Starres, start from their Spheres,\\nThy knotty and combined lockes to part,\\nAnd each particular haire to stand an end,\\nLike Quilles vpon the fretfull Porpentine:\\nBut this eternall blason must not be\\nTo eares of flesh and bloud; list Hamlet, oh list,\\nIf thou didst euer thy deare Father loue\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. Oh Heauen!\\n  Gho. Reuenge his foule and most vnnaturall Murther\\n\\n   Ham. Murther?\\n  Ghost. Murther most foule, as in the best it is;\\nBut this most foule, strange, and vnnaturall'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. Hast, hast me to know it,\\nThat with wings as swift\\nAs meditation, or the thoughts of Loue,\\nMay sweepe to my Reuenge'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ghost. I finde thee apt,\\nAnd duller should'st thou be then the fat weede\\nThat rots it selfe in ease, on Lethe Wharfe,\\nWould'st thou not stirre in this. Now Hamlet heare:\\nIt's giuen out, that sleeping in mine Orchard,\\nA Serpent stung me: so the whole eare of Denmarke,\\nIs by a forged processe of my death\\nRankly abus'd: But know thou Noble youth,\\nThe Serpent that did sting thy Fathers life,\\nNow weares his Crowne\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. O my Propheticke soule: mine Vncle?\\n  Ghost. I that incestuous, that adulterate Beast\\nWith witchcraft of his wits, hath Traitorous guifts.\\nOh wicked Wit, and Gifts, that haue the power\\nSo to seduce? Won to this shamefull Lust\\nThe will of my most seeming vertuous Queene:\\nOh Hamlet, what a falling off was there,\\nFrom me, whose loue was of that dignity,\\nThat it went hand in hand, euen with the Vow\\nI made to her in Marriage; and to decline\\nVpon a wretch, whose Naturall gifts were poore\\nTo those of mine. But Vertue, as it neuer wil be moued,\\nThough Lewdnesse court it in a shape of Heauen:\\nSo Lust, though to a radiant Angell link'd,\\nWill sate it selfe in a Celestiall bed, & prey on Garbage.\\nBut soft, me thinkes I sent the Mornings Ayre;\\nBriefe let me be: Sleeping within mine Orchard,\\nMy custome alwayes in the afternoone;\\nVpon my secure hower thy Vncle stole\\nWith iuyce of cursed Hebenon in a Violl,\\nAnd in the Porches of mine eares did poure\\nThe leaperous Distilment; whose effect\\nHolds such an enmity with bloud of Man,\\nThat swift as Quick-siluer, it courses through\\nThe naturall Gates and Allies of the body;\\nAnd with a sodaine vigour it doth posset\\nAnd curd, like Aygre droppings into Milke,\\nThe thin and wholsome blood: so did it mine;\\nAnd a most instant Tetter bak'd about,\\nMost Lazar-like, with vile and loathsome crust,\\nAll my smooth Body.\\nThus was I, sleeping, by a Brothers hand,\\nOf Life, of Crowne, and Queene at once dispatcht;\\nCut off euen in the Blossomes of my Sinne,\\nVnhouzzled, disappointed, vnnaneld,\\nNo reckoning made, but sent to my account\\nWith all my imperfections on my head;\\nOh horrible Oh horrible, most horrible:\\nIf thou hast nature in thee beare it not;\\nLet not the Royall Bed of Denmarke be\\nA Couch for Luxury and damned Incest.\\nBut howsoeuer thou pursuest this Act,\\nTaint not thy mind; nor let thy Soule contriue\\nAgainst thy Mother ought; leaue her to heauen,\\nAnd to those Thornes that in her bosome lodge,\\nTo pricke and sting her. Fare thee well at once;\\nThe Glow-worme showes the Matine to be neere,\\nAnd gins to pale his vneffectuall Fire:\\nAdue, adue, Hamlet: remember me.\\nEnter.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Oh all you host of Heauen! Oh Earth; what els?\\nAnd shall I couple Hell? Oh fie: hold my heart;\\nAnd you my sinnewes, grow not instant Old;\\nBut beare me stiffely vp: Remember thee?\\nI, thou poore Ghost, while memory holds a seate\\nIn this distracted Globe: Remember thee?\\nYea, from the Table of my Memory,\\nIle wipe away all triuiall fond Records,\\nAll sawes of Bookes, all formes, all presures past,\\nThat youth and obseruation coppied there;\\nAnd thy Commandment all alone shall liue\\nWithin the Booke and Volume of my Braine,\\nVnmixt with baser matter; yes yes, by Heauen:\\nOh most pernicious woman!\\nOh Villaine, Villaine, smiling damned Villaine!\\nMy Tables, my Tables; meet it is I set it downe,\\nThat one may smile, and smile and be a Villaine;\\nAt least I'm sure it may be so in Denmarke;\\nSo Vnckle there you are: now to my word;\\nIt is; Adue, Adue, Remember me: I haue sworn't\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Hor. & Mar. within. My Lord, my Lord.\\nEnter Horatio and Marcellus.\\n\\n  Mar. Lord Hamlet\\n\\n   Hor. Heauen secure him\\n\\n   Mar. So be it\\n\\n   Hor. Illo, ho, ho, my Lord'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Hillo, ho, ho, boy; come bird, come\\n\\n   Mar. How ist my Noble Lord?\\n  Hor. What newes, my Lord?\\n  Ham. Oh wonderfull!\\n  Hor. Good my Lord tell it\\n\\n   Ham. No you'l reueale it\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Hor. Not I, my Lord, by Heauen\\n\\n   Mar. Nor I, my Lord\\n\\n   Ham. How say you then, would heart of man once think it?\\nBut you'l be secret?\\n  Both. I, by Heau'n, my Lord\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. There's nere a villaine dwelling in all Denmarke\\nBut hee's an arrant knaue\\n\\n   Hor. There needs no Ghost my Lord, come from the\\nGraue, to tell vs this\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Why right, you are i'th' right;\\nAnd so, without more circumstance at all,\\nI hold it fit that we shake hands, and part:\\nYou, as your busines and desires shall point you:\\nFor euery man ha's businesse and desire,\\nSuch as it is: and for mine owne poore part,\\nLooke you, Ile goe pray\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Hor. These are but wild and hurling words, my Lord\\n\\n   Ham. I'm sorry they offend you heartily:\\nYes faith, heartily\\n\\n   Hor. There's no offence my Lord\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Yes, by Saint Patricke, but there is my Lord,\\nAnd much offence too, touching this Vision heere:\\nIt is an honest Ghost, that let me tell you:\\nFor your desire to know what is betweene vs,\\nO'remaster't as you may. And now good friends,\\nAs you are Friends, Schollers and Soldiers,\\nGiue me one poore request\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Hor. What is't my Lord? we will\\n\\n   Ham. Neuer make known what you haue seen to night\\n\\n   Both. My Lord, we will not\\n\\n   Ham. Nay, but swear't\\n\\n   Hor. Infaith my Lord, not I\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Mar. Nor I my Lord: in faith\\n\\n   Ham. Vpon my sword\\n\\n   Marcell. We haue sworne my Lord already\\n\\n   Ham. Indeed, vpon my sword, Indeed\\n\\n   Gho. Sweare.\\n\\nGhost cries vnder the Stage.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. Ah ha boy, sayest thou so. Art thou there truepenny?\\nCome one you here this fellow in the selleredge\\nConsent to sweare\\n\\n   Hor. Propose the Oath my Lord'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. Neuer to speake of this that you haue seene.\\nSweare by my sword\\n\\n   Gho. Sweare'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Gho. Sweare\\n\\n   Ham. Hic & vbique? Then wee'l shift for grownd,\\nCome hither Gentlemen,\\nAnd lay your hands againe vpon my sword,\\nNeuer to speake of this that you haue heard:\\nSweare by my Sword\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Gho. Sweare\\n\\n   Ham. Well said old Mole, can'st worke i'th' ground so fast?\\nA worthy Pioner, once more remoue good friends\\n\\n   Hor. Oh day and night: but this is wondrous strange\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. And therefore as a stranger giue it welcome.\\nThere are more things in Heauen and Earth, Horatio,\\nThen are dream't of in our Philosophy. But come,\\nHere as before, neuer so helpe you mercy,\\nHow strange or odde so ere I beare my selfe;\\n(As I perchance heereafter shall thinke meet\\nTo put an Anticke disposition on:)\\nThat you at such time seeing me, neuer shall\\nWith Armes encombred thus, or thus, head shake;\\nOr by pronouncing of some doubtfull Phrase;\\nAs well, we know, or we could and if we would,\\nOr if we list to speake; or there be and if there might,\\nOr such ambiguous giuing out to note,\\nThat you know ought of me; this not to doe:\\nSo grace and mercy at your most neede helpe you:\\nSweare\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ghost. Sweare'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Rest, rest perturbed Spirit: so Gentlemen,\\nWith all my loue I doe commend me to you;\\nAnd what so poore a man as Hamlet is,\\nMay doe t' expresse his loue and friending to you,\\nGod willing shall not lacke: let vs goe in together,\\nAnd still your fingers on your lippes I pray,\\nThe time is out of ioynt: Oh cursed spight,\\nThat euer I was borne to set it right.\\nNay, come let's goe together.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Exeunt.\\n\\n\\nActus Secundus.\\n\\nEnter Polonius, and Reynoldo.\\n\\n  Polon. Giue him his money, and these notes Reynoldo\\n\\n   Reynol. I will my Lord'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Polon. You shall doe maruels wisely: good Reynoldo,\\nBefore you visite him you make inquiry\\nOf his behauiour\\n\\n   Reynol. My Lord, I did intend it'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Polon. Marry, well said;\\nVery well said. Looke you Sir,\\nEnquire me first what Danskers are in Paris;\\nAnd how, and who; what meanes; and where they keepe:\\nWhat company, at what expence: and finding\\nBy this encompassement and drift of question,\\nThat they doe know my sonne: Come you more neerer\\nThen your particular demands will touch it,\\nTake you as 'twere some distant knowledge of him,\\nAnd thus I know his father and his friends,\\nAnd in part him. Doe you marke this Reynoldo?\\n  Reynol. I, very well my Lord\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Polon. And in part him, but you may say not well;\\nBut if't be hee I meane, hees very wilde;\\nAddicted so and so; and there put on him\\nWhat forgeries you please; marry, none so ranke,\\nAs may dishonour him; take heed of that:\\nBut Sir, such wanton, wild, and vsuall slips,\\nAs are Companions noted and most knowne\\nTo youth and liberty\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Reynol. As gaming my Lord\\n\\n   Polon. I, or drinking, fencing, swearing,\\nQuarelling, drabbing. You may goe so farre\\n\\n   Reynol. My Lord that would dishonour him'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Polon. Faith no, as you may season it in the charge;\\nYou must not put another scandall on him,\\nThat hee is open to Incontinencie;\\nThat's not my meaning: but breath his faults so quaintly,\\nThat they may seeme the taints of liberty;\\nThe flash and out-breake of a fiery minde,\\nA sauagenes in vnreclaim'd bloud of generall assault\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Reynol. But my good Lord\\n\\n   Polon. Wherefore should you doe this?\\n  Reynol. I my Lord, I would know that'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Polon. Marry Sir, heere's my drift,\\nAnd I belieue it is a fetch of warrant:\\nYou laying these slight sulleyes on my Sonne,\\nAs 'twere a thing a little soil'd i'th' working:\\nMarke you your party in conuerse; him you would sound,\\nHauing euer seene. In the prenominate crimes,\\nThe youth you breath of guilty, be assur'd\\nHe closes with you in this consequence:\\nGood sir, or so, or friend, or Gentleman.\\nAccording to the Phrase and the Addition,\\nOf man and Country\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Reynol. Very good my Lord'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Polon. And then Sir does he this?\\nHe does: what was I about to say?\\nI was about say somthing: where did I leaue?\\n  Reynol. At closes in the consequence:\\nAt friend, or so, and Gentleman'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Polon. At closes in the consequence, I marry,\\nHe closes with you thus. I know the Gentleman,\\nI saw him yesterday, or tother day;\\nOr then or then, with such and such; and as you say,\\nThere was he gaming, there o'retooke in's Rouse,\\nThere falling out at Tennis; or perchance,\\nI saw him enter such a house of saile;\\nVidelicet, a Brothell, or so forth. See you now;\\nYour bait of falshood, takes this Cape of truth;\\nAnd thus doe we of wisedome and of reach\\nWith windlesses, and with assaies of Bias,\\nBy indirections finde directions out:\\nSo by my former Lecture and aduice\\nShall you my Sonne; you haue me, haue you not?\\n  Reynol. My Lord I haue\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Polon. God buy you; fare you well\\n\\n   Reynol. Good my Lord\\n\\n   Polon. Obserue his inclination in your selfe\\n\\n   Reynol. I shall my Lord\\n\\n   Polon. And let him plye his Musicke'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Reynol. Well, my Lord.\\nEnter.\\n\\nEnter Ophelia.\\n\\n  Polon. Farewell:\\nHow now Ophelia, what's the matter?\\n  Ophe. Alas my Lord, I haue beene so affrighted\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Polon. With what, in the name of Heauen?\\n  Ophe. My Lord, as I was sowing in my Chamber,\\nLord Hamlet with his doublet all vnbrac'd,\\nNo hat vpon his head, his stockings foul'd,\\nVngartred, and downe giued to his Anckle,\\nPale as his shirt, his knees knocking each other,\\nAnd with a looke so pitious in purport,\\nAs if he had been loosed out of hell,\\nTo speake of horrors: he comes before me\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Polon. Mad for thy Loue?\\n  Ophe. My Lord, I doe not know: but truly I do feare it'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Polon. What said he?\\n  Ophe. He tooke me by the wrist, and held me hard;\\nThen goes he to the length of all his arme;\\nAnd with his other hand thus o're his brow,\\nHe fals to such perusall of my face,\\nAs he would draw it. Long staid he so,\\nAt last, a little shaking of mine Arme:\\nAnd thrice his head thus wauing vp and downe;\\nHe rais'd a sigh, so pittious and profound,\\nThat it did seeme to shatter all his bulke,\\nAnd end his being. That done, he lets me goe,\\nAnd with his head ouer his shoulders turn'd,\\nHe seem'd to finde his way without his eyes,\\nFor out adores he went without their helpe;\\nAnd to the last, bended their light on me\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Polon. Goe with me, I will goe seeke the King,\\nThis is the very extasie of Loue,\\nWhose violent property foredoes it selfe,\\nAnd leads the will to desperate Vndertakings,\\nAs oft as any passion vnder Heauen,\\nThat does afflict our Natures. I am sorrie,\\nWhat haue you giuen him any hard words of late?\\n  Ophe. No my good Lord: but as you did command,\\nI did repell his Letters, and deny'de\\nHis accesse to me\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Pol. That hath made him mad.\\nI am sorrie that with better speed and iudgement\\nI had not quoted him. I feare he did but trifle,\\nAnd meant to wracke thee: but beshrew my iealousie:\\nIt seemes it is as proper to our Age,\\nTo cast beyond our selues in our Opinions,\\nAs it is common for the yonger sort\\nTo lacke discretion. Come, go we to the King,\\nThis must be knowne, being kept close might moue\\nMore greefe to hide, then hate to vtter loue.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Exeunt.\\n\\n\\nScena Secunda.\\n\\nEnter King, Queene, Rosincrane, and Guildensterne Cum alijs.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"King. Welcome deere Rosincrance and Guildensterne.\\nMoreouer, that we much did long to see you,\\nThe neede we haue to vse you, did prouoke\\nOur hastie sending. Something haue you heard\\nOf Hamlets transformation: so I call it,\\nSince not th' exterior, nor the inward man\\nResembles that it was. What it should bee\\nMore then his Fathers death, that thus hath put him\\nSo much from th' vnderstanding of himselfe,\\nI cannot deeme of. I intreat you both,\\nThat being of so young dayes brought vp with him:\\nAnd since so Neighbour'd to his youth, and humour,\\nThat you vouchsafe your rest heere in our Court\\nSome little time: so by your Companies\\nTo draw him on to pleasures, and to gather\\nSo much as from Occasions you may gleane,\\nThat open'd lies within our remedie\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Qu. Good Gentlemen, he hath much talk'd of you,\\nAnd sure I am, two men there are not liuing,\\nTo whom he more adheres. If it will please you\\nTo shew vs so much Gentrie, and good will,\\nAs to expend your time with vs a-while,\\nFor the supply and profit of our Hope,\\nYour Visitation shall receiue such thankes\\nAs fits a Kings remembrance\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Rosin. Both your Maiesties\\nMight by the Soueraigne power you haue of vs,\\nPut your dread pleasures, more into Command\\nThen to Entreatie'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Guil. We both obey,\\nAnd here giue vp our selues, in the full bent,\\nTo lay our Seruices freely at your feete,\\nTo be commanded\\n\\n   King. Thankes Rosincrance, and gentle Guildensterne'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Qu. Thankes Guildensterne and gentle Rosincrance.\\nAnd I beseech you instantly to visit\\nMy too much changed Sonne.\\nGo some of ye,\\nAnd bring the Gentlemen where Hamlet is'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Guil. Heauens make our presence and our practises\\nPleasant and helpfull to him.\\nEnter.\\n\\n  Queene. Amen.\\nEnter Polonius.\\n\\n  Pol. Th' Ambassadors from Norwey, my good Lord,\\nAre ioyfully return'd\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='King. Thou still hast bin the father of good Newes'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Pol. Haue I, my Lord? Assure you, my good Liege,\\nI hold my dutie, as I hold my Soule,\\nBoth to my God, one to my gracious King:\\nAnd I do thinke, or else this braine of mine\\nHunts not the traile of Policie, so sure\\nAs I haue vs'd to do: that I haue found\\nThe very cause of Hamlets Lunacie\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"King. Oh speake of that, that I do long to heare\\n\\n   Pol. Giue first admittance to th' Ambassadors,\\nMy Newes shall be the Newes to that great Feast\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='King. Thy selfe do grace to them, and bring them in.\\nHe tels me my sweet Queene, that he hath found\\nThe head and sourse of all your Sonnes distemper'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Qu. I doubt it is no other, but the maine,\\nHis Fathers death, and our o're-hasty Marriage.\\nEnter Polonius, Voltumand, and Cornelius.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"King. Well, we shall sift him. Welcome good Frends:\\nSay Voltumand, what from our Brother Norwey?\\n  Volt. Most faire returne of Greetings, and Desires.\\nVpon our first, he sent out to suppresse\\nHis Nephewes Leuies, which to him appear'd\\nTo be a preparation 'gainst the Poleak:\\nBut better look'd into, he truly found\\nIt was against your Highnesse, whereat greeued,\\nThat so his Sicknesse, Age, and Impotence\\nWas falsely borne in hand, sends out Arrests\\nOn Fortinbras, which he (in breefe) obeyes,\\nReceiues rebuke from Norwey: and in fine,\\nMakes Vow before his Vnkle, neuer more\\nTo giue th' assay of Armes against your Maiestie.\\nWhereon old Norwey, ouercome with ioy,\\nGiues him three thousand Crownes in Annuall Fee,\\nAnd his Commission to imploy those Soldiers\\nSo leuied as before, against the Poleak:\\nWith an intreaty heerein further shewne,\\nThat it might please you to giue quiet passe\\nThrough your Dominions, for his Enterprize,\\nOn such regards of safety and allowance,\\nAs therein are set downe\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"King. It likes vs well:\\nAnd at our more consider'd time wee'l read,\\nAnswer, and thinke vpon this Businesse.\\nMeane time we thanke you, for your well-tooke Labour.\\nGo to your rest, at night wee'l Feast together.\\nMost welcome home.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Exit Ambass.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Pol. This businesse is very well ended.\\nMy Liege, and Madam, to expostulate\\nWhat Maiestie should be, what Dutie is,\\nWhy day is day; night, night; and time is time,\\nWere nothing but to waste Night, Day, and Time.\\nTherefore, since Breuitie is the Soule of Wit,\\nAnd tediousnesse, the limbes and outward flourishes,\\nI will be breefe. Your Noble Sonne is mad:\\nMad call I it; for to define true Madnesse,\\nWhat is't, but to be nothing else but mad.\\nBut let that go\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Qu. More matter, with lesse Art'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Pol. Madam, I sweare I vse no Art at all:\\nThat he is mad, 'tis true: 'Tis true 'tis pittie,\\nAnd pittie it is true: A foolish figure,\\nBut farewell it: for I will vse no Art.\\nMad let vs grant him then: and now remaines\\nThat we finde out the cause of this effect,\\nOr rather say, the cause of this defect;\\nFor this effect defectiue, comes by cause,\\nThus it remaines, and the remainder thus. Perpend,\\nI haue a daughter: haue, whil'st she is mine,\\nWho in her Dutie and Obedience, marke,\\nHath giuen me this: now gather, and surmise.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='The Letter.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"To the Celestiall, and my Soules Idoll, the most beautifed Ophelia.\\nThat's an ill Phrase, a vilde Phrase, beautified is a vilde\\nPhrase: but you shall heare these in her excellent white\\nbosome, these\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Qu. Came this from Hamlet to her'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Pol. Good Madam stay awhile, I will be faithfull.\\nDoubt thou, the Starres are fire,\\nDoubt, that the Sunne doth moue:\\nDoubt Truth to be a Lier,\\nBut neuer Doubt, I loue.\\nO deere Ophelia, I am ill at these Numbers: I haue not Art to\\nreckon my grones; but that I loue thee best, oh most Best beleeue\\nit. Adieu.\\nThine euermore most deere Lady, whilst this\\nMachine is to him, Hamlet.\\nThis in Obedience hath my daughter shew'd me:\\nAnd more aboue hath his soliciting,\\nAs they fell out by Time, by Meanes, and Place,\\nAll giuen to mine eare\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"King. But how hath she receiu'd his Loue?\\n  Pol. What do you thinke of me?\\n  King. As of a man, faithfull and Honourable\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Pol. I wold faine proue so. But what might you think?\\nWhen I had seene this hot loue on the wing,\\nAs I perceiued it, I must tell you that\\nBefore my Daughter told me what might you\\nOr my deere Maiestie your Queene heere, think,\\nIf I had playd the Deske or Table-booke,\\nOr giuen my heart a winking, mute and dumbe,\\nOr look'd vpon this Loue, with idle sight,\\nWhat might you thinke? No, I went round to worke,\\nAnd (my yong Mistris) thus I did bespeake\\nLord Hamlet is a Prince out of thy Starre,\\nThis must not be: and then, I Precepts gaue her,\\nThat she should locke her selfe from his Resort,\\nAdmit no Messengers, receiue no Tokens:\\nWhich done, she tooke the Fruites of my Aduice,\\nAnd he repulsed. A short Tale to make,\\nFell into a Sadnesse, then into a Fast,\\nThence to a Watch, thence into a Weaknesse,\\nThence to a Lightnesse, and by this declension\\nInto the Madnesse whereon now he raues,\\nAnd all we waile for\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"King. Do you thinke 'tis this?\\n  Qu. It may be very likely\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Pol. Hath there bene such a time, I'de fain know that,\\nThat I haue possitiuely said, 'tis so,\\nWhen it prou'd otherwise?\\n  King. Not that I know\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Pol. Take this from this; if this be otherwise,\\nIf Circumstances leade me, I will finde\\nWhere truth is hid, though it were hid indeede\\nWithin the Center'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"King. How may we try it further?\\n  Pol. You know sometimes\\nHe walkes foure houres together, heere\\nIn the Lobby\\n\\n   Qu. So he ha's indeed\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Pol. At such a time Ile loose my Daughter to him,\\nBe you and I behinde an Arras then,\\nMarke the encounter: If he loue her not,\\nAnd be not from his reason falne thereon;\\nLet me be no Assistant for a State,\\nAnd keepe a Farme and Carters'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='King. We will try it.\\nEnter Hamlet reading on a Booke.\\n\\n  Qu. But looke where sadly the poore wretch\\nComes reading\\n\\n   Pol. Away I do beseech you, both away,\\nIle boord him presently.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Exit King & Queen.\\n\\nOh giue me leaue. How does my good Lord Hamlet?\\n  Ham. Well, God-a-mercy\\n\\n   Pol. Do you know me, my Lord?\\n  Ham. Excellent, excellent well: y'are a Fishmonger\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Pol. Not I my Lord\\n\\n   Ham. Then I would you were so honest a man\\n\\n   Pol. Honest, my Lord?\\n  Ham. I sir, to be honest as this world goes, is to bee\\none man pick'd out of two thousand\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Pol. That's very true, my Lord\\n\\n   Ham. For if the Sun breed Magots in a dead dogge,\\nbeing a good kissing Carrion-\\nHaue you a daughter?\\n  Pol. I haue my Lord\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Let her not walke i'thSunne: Conception is a\\nblessing, but not as your daughter may conceiue. Friend\\nlooke too't\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Pol. How say you by that? Still harping on my daughter:\\nyet he knew me not at first; he said I was a Fishmonger:\\nhe is farre gone, farre gone: and truly in my youth,\\nI suffred much extreamity for loue: very neere this. Ile\\nspeake to him againe. What do you read my Lord?\\n  Ham. Words, words, words'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Pol. What is the matter, my Lord?\\n  Ham. Betweene who?\\n  Pol. I meane the matter you meane, my Lord'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. Slanders Sir: for the Satyricall slaue saies here,\\nthat old men haue gray Beards; that their faces are wrinkled;\\ntheir eyes purging thicke Amber, or Plum-Tree\\nGumme: and that they haue a plentifull locke of Wit,\\ntogether with weake Hammes. All which Sir, though I\\nmost powerfully, and potently beleeue; yet I holde it\\nnot Honestie to haue it thus set downe: For you your\\nselfe Sir, should be old as I am, if like a Crab you could\\ngo backward'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Pol. Though this be madnesse,\\nYet there is Method in't: will you walke\\nOut of the ayre my Lord?\\n  Ham. Into my Graue?\\n  Pol. Indeed that is out o'th' Ayre:\\nHow pregnant (sometimes) his Replies are?\\nA happinesse,\\nThat often Madnesse hits on,\\nWhich Reason and Sanitie could not\\nSo prosperously be deliuer'd of.\\nI will leaue him,\\nAnd sodainely contriue the meanes of meeting\\nBetweene him, and my daughter.\\nMy Honourable Lord, I will most humbly\\nTake my leaue of you\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. You cannot Sir take from me any thing, that I\\nwill more willingly part withall, except my life, my\\nlife\\n\\n   Polon. Fare you well my Lord\\n\\n   Ham. These tedious old fooles'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Polon. You goe to seeke my Lord Hamlet; there\\nhee is.\\nEnter Rosincran and Guildensterne.\\n\\n  Rosin. God saue you Sir'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Guild. Mine honour'd Lord?\\n  Rosin. My most deare Lord?\\n  Ham. My excellent good friends? How do'st thou\\nGuildensterne? Oh, Rosincrane; good Lads: How doe ye\\nboth?\\n  Rosin. As the indifferent Children of the earth\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Guild. Happy, in that we are not ouer-happy: on Fortunes\\nCap, we are not the very Button\\n\\n   Ham. Nor the Soales of her Shoo?\\n  Rosin. Neither my Lord'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. Then you liue about her waste, or in the middle\\nof her fauour?\\n  Guil. Faith, her priuates, we'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. In the secret parts of Fortune? Oh, most true:\\nshe is a Strumpet. What's the newes?\\n  Rosin. None my Lord; but that the World's growne\\nhonest\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Then is Doomesday neere: But your newes is\\nnot true. Let me question more in particular: what haue\\nyou my good friends, deserued at the hands of Fortune,\\nthat she sends you to Prison hither?\\n  Guil. Prison, my Lord?\\n  Ham. Denmark's a Prison\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Rosin. Then is the World one\\n\\n   Ham. A goodly one, in which there are many Confines,\\nWards, and Dungeons; Denmarke being one o'th'\\nworst\\n\\n   Rosin. We thinke not so my Lord\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Why then 'tis none to you; for there is nothing\\neither good or bad, but thinking makes it so: to me it is\\na prison\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Rosin. Why then your Ambition makes it one: 'tis\\ntoo narrow for your minde\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. O God, I could be bounded in a nutshell, and\\ncount my selfe a King of infinite space; were it not that\\nI haue bad dreames'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Guil. Which dreames indeed are Ambition: for the\\nvery substance of the Ambitious, is meerely the shadow\\nof a Dreame\\n\\n   Ham. A dreame it selfe is but a shadow'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Rosin. Truely, and I hold Ambition of so ayry and\\nlight a quality, that it is but a shadowes shadow'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Then are our Beggers bodies; and our Monarchs\\nand out-stretcht Heroes the Beggers Shadowes:\\nshall wee to th' Court: for, by my fey I cannot reason?\\n  Both. Wee'l wait vpon you\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. No such matter. I will not sort you with the\\nrest of my seruants: for to speake to you like an honest\\nman: I am most dreadfully attended; but in the beaten\\nway of friendship, What make you at Elsonower?\\n  Rosin. To visit you my Lord, no other occasion'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. Begger that I am, I am euen poore in thankes;\\nbut I thanke you: and sure deare friends my thanks\\nare too deare a halfepeny; were you not sent for? Is it\\nyour owne inclining? Is it a free visitation? Come,\\ndeale iustly with me: come, come; nay speake'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Guil. What should we say my Lord?\\n  Ham. Why any thing. But to the purpose; you were\\nsent for; and there is a kinde confession in your lookes;\\nwhich your modesties haue not craft enough to color,\\nI know the good King & Queene haue sent for you'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Rosin. To what end my Lord?\\n  Ham. That you must teach me: but let mee coniure\\nyou by the rights of our fellowship, by the consonancy of\\nour youth, by the Obligation of our euer-preserued loue,\\nand by what more deare, a better proposer could charge\\nyou withall; be euen and direct with me, whether you\\nwere sent for or no'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Rosin. What say you?\\n  Ham. Nay then I haue an eye of you: if you loue me\\nhold not off\\n\\n   Guil. My Lord, we were sent for'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. I will tell you why; so shall my anticipation\\npreuent your discouery of your secricie to the King and\\nQueene: moult no feather, I haue of late, but wherefore\\nI know not, lost all my mirth, forgone all custome of exercise;\\nand indeed, it goes so heauenly with my disposition;\\nthat this goodly frame the Earth, seemes to me a sterrill\\nPromontory; this most excellent Canopy the Ayre,\\nlook you, this braue ore-hanging, this Maiesticall Roofe,\\nfretted with golden fire: why, it appeares no other thing\\nto mee, then a foule and pestilent congregation of vapours.\\nWhat a piece of worke is a man! how Noble in\\nReason? how infinite in faculty? in forme and mouing\\nhow expresse and admirable? in Action, how like an Angel?\\nin apprehension, how like a God? the beauty of the\\nworld, the Parragon of Animals; and yet to me, what is\\nthis Quintessence of Dust? Man delights not me; no,\\nnor Woman neither; though by your smiling you seeme\\nto say so'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Rosin. My Lord, there was no such stuffe in my\\nthoughts'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. Why did you laugh, when I said, Man delights\\nnot me?\\n  Rosin. To thinke, my Lord, if you delight not in Man,\\nwhat Lenton entertainment the Players shall receiue\\nfrom you: wee coated them on the way, and hither are\\nthey comming to offer you Seruice'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. He that playes the King shall be welcome; his\\nMaiesty shall haue Tribute of mee: the aduenturous\\nKnight shal vse his Foyle and Target: the Louer shall\\nnot sigh gratis, the humorous man shall end his part in\\npeace: the Clowne shall make those laugh whose lungs\\nare tickled a'th' sere: and the Lady shall say her minde\\nfreely; or the blanke Verse shall halt for't: what Players\\nare they?\\n  Rosin. Euen those you were wont to take delight in\\nthe Tragedians of the City\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. How chances it they trauaile? their residence\\nboth in reputation and profit was better both\\nwayes'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Rosin. I thinke their Inhibition comes by the meanes\\nof the late Innouation?\\n  Ham. Doe they hold the same estimation they did\\nwhen I was in the City? Are they so follow'd?\\n  Rosin. No indeed, they are not\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. How comes it? doe they grow rusty?\\n  Rosin. Nay, their indeauour keepes in the wonted\\npace; But there is Sir an ayrie of Children, little\\nYases, that crye out on the top of question; and\\nare most tyrannically clap't for't: these are now the\\nfashion, and so be-ratled the common Stages (so they\\ncall them) that many wearing Rapiers, are affraide of\\nGoose-quils, and dare scarse come thither\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. What are they Children? Who maintains 'em?\\nHow are they escorted? Will they pursue the Quality no\\nlonger then they can sing? Will they not say afterwards\\nif they should grow themselues to common Players (as\\nit is most like if their meanes are not better) their Writers\\ndo them wrong, to make them exclaim against their\\nowne Succession\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Rosin. Faith there ha's bene much to do on both sides:\\nand the Nation holds it no sinne, to tarre them to Controuersie.\\nThere was for a while, no mony bid for argument,\\nvnlesse the Poet and the Player went to Cuffes in\\nthe Question\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Is't possible?\\n  Guild. Oh there ha's beene much throwing about of\\nBraines\\n\\n   Ham. Do the Boyes carry it away?\\n  Rosin. I that they do my Lord. Hercules & his load too\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. It is not strange: for mine Vnckle is King of\\nDenmarke, and those that would make mowes at him\\nwhile my Father liued; giue twenty, forty, an hundred\\nDucates a peece, for his picture in Little. There is something\\nin this more then Naturall, if Philosophie could\\nfinde it out.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Flourish for the Players.\\n\\n  Guil. There are the Players'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Gentlemen, you are welcom to Elsonower: your\\nhands, come: The appurtenance of Welcome, is Fashion\\nand Ceremony. Let me comply with you in the Garbe,\\nlest my extent to the Players (which I tell you must shew\\nfairely outward) should more appeare like entertainment\\nthen yours. You are welcome: but my Vnckle Father,\\nand Aunt Mother are deceiu'd\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Guil. In what my deere Lord?\\n  Ham. I am but mad North, North-West: when the\\nWinde is Southerly, I know a Hawke from a Handsaw.\\nEnter Polonius.\\n\\n  Pol. Well be with you Gentlemen'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. Hearke you Guildensterne, and you too: at each\\neare a hearer: that great Baby you see there, is not yet\\nout of his swathing clouts'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Rosin. Happily he's the second time come to them: for\\nthey say, an old man is twice a childe\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. I will Prophesie. Hee comes to tell me of the\\nPlayers. Mark it, you say right Sir: for a Monday morning\\n'twas so indeed\\n\\n   Pol. My Lord, I haue Newes to tell you\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. My Lord, I haue Newes to tell you.\\nWhen Rossius an Actor in Rome-\\n  Pol. The Actors are come hither my Lord\\n\\n   Ham. Buzze, buzze\\n\\n   Pol. Vpon mine Honor'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. Then can each Actor on his Asse-\\n  Polon. The best Actors in the world, either for Tragedie,\\nComedie, Historie, Pastorall:\\nPastoricall-Comicall-Historicall-Pastorall:\\nTragicall-Historicall: Tragicall-Comicall-Historicall-Pastorall:\\nScene indiuidible: or Poem\\nvnlimited. Seneca cannot be too heauy, nor Plautus\\ntoo light, for the law of Writ, and the Liberty. These are\\nthe onely men'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. O Iephta Iudge of Israel, what a Treasure had'st\\nthou?\\n  Pol. What a Treasure had he, my Lord?\\n  Ham. Why one faire Daughter, and no more,\\nThe which he loued passing well\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Pol. Still on my Daughter\\n\\n   Ham. Am I not i'th' right old Iephta?\\n  Polon. If you call me Iephta my Lord, I haue a daughter\\nthat I loue passing well\\n\\n   Ham. Nay that followes not\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Polon. What followes then, my Lord?\\n  Ha. Why, As by lot, God wot: and then you know, It\\ncame to passe, as most like it was: The first rowe of the\\nPons Chanson will shew you more. For looke where my\\nAbridgements come.\\nEnter foure or fiue Players.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Y'are welcome Masters, welcome all. I am glad to see\\nthee well: Welcome good Friends. Oh my olde Friend?\\nThy face is valiant since I saw thee last: Com'st thou to\\nbeard me in Denmarke? What, my yong Lady and Mistris?\\nByrlady your Ladiship is neerer Heauen then when\\nI saw you last, by the altitude of a Choppine. Pray God\\nyour voice like a peece of vncurrant Gold be not crack'd\\nwithin the ring. Masters, you are all welcome: wee'l e'ne\\nto't like French Faulconers, flie at any thing we see: wee'l\\nhaue a Speech straight. Come giue vs a tast of your quality:\\ncome, a passionate speech\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"1.Play. What speech, my Lord?\\n  Ham. I heard thee speak me a speech once, but it was\\nneuer Acted: or if it was, not aboue once, for the Play I\\nremember pleas'd not the Million, 'twas Cauiarie to the\\nGenerall: but it was (as I receiu'd it, and others, whose\\niudgement in such matters, cried in the top of mine) an\\nexcellent Play; well digested in the Scoenes, set downe\\nwith as much modestie, as cunning. I remember one said,\\nthere was no Sallets in the lines, to make the matter sauory;\\nnor no matter in the phrase, that might indite the\\nAuthor of affectation, but cal'd it an honest method. One\\ncheefe Speech in it, I cheefely lou'd, 'twas Aeneas Tale\\nto Dido, and thereabout of it especially, where he speaks\\nof Priams slaughter. If it liue in your memory, begin at\\nthis Line, let me see, let me see: The rugged Pyrrhus like\\nth'Hyrcanian Beast. It is not so: it begins with Pyrrhus\\nThe rugged Pyrrhus, he whose Sable Armes\\nBlacke as his purpose, did the night resemble\\nWhen he lay couched in the Ominous Horse,\\nHath now this dread and blacke Complexion smear'd\\nWith Heraldry more dismall: Head to foote\\nNow is he to take Geulles, horridly Trick'd\\nWith blood of Fathers, Mothers, Daughters, Sonnes,\\nBak'd and impasted with the parching streets,\\nThat lend a tyrannous, and damned light\\nTo their vilde Murthers, roasted in wrath and fire,\\nAnd thus o're-sized with coagulate gore,\\nWith eyes like Carbuncles, the hellish Pyrrhus\\nOlde Grandsire Priam seekes\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Pol. Fore God, my Lord, well spoken, with good accent,\\nand good discretion'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"1.Player. Anon he findes him,\\nStriking too short at Greekes. His anticke Sword,\\nRebellious to his Arme, lyes where it falles\\nRepugnant to command: vnequall match,\\nPyrrhus at Priam driues, in Rage strikes wide:\\nBut with the whiffe and winde of his fell Sword,\\nTh' vnnerued Father fals. Then senselesse Illium,\\nSeeming to feele his blow, with flaming top\\nStoopes to his Bace, and with a hideous crash\\nTakes Prisoner Pyrrhus eare. For loe, his Sword\\nWhich was declining on the Milkie head\\nOf Reuerend Priam, seem'd i'th' Ayre to sticke:\\nSo as a painted Tyrant Pyrrhus stood,\\nAnd like a Newtrall to his will and matter, did nothing.\\nBut as we often see against some storme,\\nA silence in the Heauens, the Racke stand still,\\nThe bold windes speechlesse, and the Orbe below\\nAs hush as death: Anon the dreadfull Thunder\\nDoth rend the Region. So after Pyrrhus pause,\\nA rowsed Vengeance sets him new a-worke,\\nAnd neuer did the Cyclops hammers fall\\nOn Mars his Armours, forg'd for proofe Eterne,\\nWith lesse remorse then Pyrrhus bleeding sword\\nNow falles on Priam.\\nOut, out, thou Strumpet-Fortune, all you Gods,\\nIn generall Synod take away her power:\\nBreake all the Spokes and Fallies from her wheele,\\nAnd boule the round Naue downe the hill of Heauen,\\nAs low as to the Fiends\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Pol. This is too long\\n\\n   Ham. It shall to'th Barbars, with your beard. Prythee\\nsay on: He's for a Iigge, or a tale of Baudry, or hee\\nsleepes. Say on; come to Hecuba\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"1.Play. But who, O who, had seen the inobled Queen\\n\\n   Ham. The inobled Queene?\\n  Pol. That's good: Inobled Queene is good\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"1.Play. Run bare-foot vp and downe,\\nThreatning the flame\\nWith Bisson Rheume: A clout about that head,\\nWhere late the Diadem stood, and for a Robe\\nAbout her lanke and all ore-teamed Loines,\\nA blanket in th' Alarum of feare caught vp.\\nWho this had seene, with tongue in Venome steep'd,\\n'Gainst Fortunes State, would Treason haue pronounc'd?\\nBut if the Gods themselues did see her then,\\nWhen she saw Pyrrhus make malicious sport\\nIn mincing with his Sword her Husbands limbes,\\nThe instant Burst of Clamour that she made\\n(Vnlesse things mortall moue them not at all)\\nWould haue made milche the Burning eyes of Heauen,\\nAnd passion in the Gods\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Pol. Looke where he ha's not turn'd his colour, and\\nha's teares in's eyes. Pray you no more\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. 'Tis well, Ile haue thee speake out the rest,\\nsoone. Good my Lord, will you see the Players wel bestow'd.\\nDo ye heare, let them be well vs'd: for they are\\nthe Abstracts and breefe Chronicles of the time. After\\nyour death, you were better haue a bad Epitaph, then\\ntheir ill report while you liued\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Pol. My Lord, I will vse them according to their desart'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. Gods bodykins man, better. Vse euerie man\\nafter his desart, and who should scape whipping: vse\\nthem after your own Honor and Dignity. The lesse they\\ndeserue, the more merit is in your bountie. Take them\\nin'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Pol. Come sirs.\\n\\nExit Polon.\\n\\n  Ham. Follow him Friends: wee'l heare a play to morrow.\\nDost thou heare me old Friend, can you play the\\nmurther of Gonzago?\\n  Play. I my Lord\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Wee'l ha't to morrow night. You could for a\\nneed study a speech of some dosen or sixteene lines, which\\nI would set downe, and insert in't? Could ye not?\\n  Play. I my Lord\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. Very well. Follow that Lord, and looke you\\nmock him not. My good Friends, Ile leaue you til night\\nyou are welcome to Elsonower?\\n  Rosin. Good my Lord.\\n\\nExeunt.\\n\\nManet Hamlet.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. I so, God buy'ye: Now I am alone.\\nOh what a Rogue and Pesant slaue am I?\\nIs it not monstrous that this Player heere,\\nBut in a Fixion, in a dreame of Passion,\\nCould force his soule so to his whole conceit,\\nThat from her working, all his visage warm'd;\\nTeares in his eyes, distraction in's Aspect,\\nA broken voyce, and his whole Function suiting\\nWith Formes, to his Conceit? And all for nothing?\\nFor Hecuba?\\nWhat's Hecuba to him, or he to Hecuba,\\nThat he should weepe for her? What would he doe,\\nHad he the Motiue and the Cue for passion\\nThat I haue? He would drowne the Stage with teares,\\nAnd cleaue the generall eare with horrid speech:\\nMake mad the guilty, and apale the free,\\nConfound the ignorant, and amaze indeed,\\nThe very faculty of Eyes and Eares. Yet I,\\nA dull and muddy-metled Rascall, peake\\nLike Iohn a-dreames, vnpregnant of my cause,\\nAnd can say nothing: No, not for a King,\\nVpon whose property, and most deere life,\\nA damn'd defeate was made. Am I a Coward?\\nWho calles me Villaine? breakes my pate a-crosse?\\nPluckes off my Beard, and blowes it in my face?\\nTweakes me by'th' Nose? giues me the Lye i'th' Throate,\\nAs deepe as to the Lungs? Who does me this?\\nHa? Why I should take it: for it cannot be,\\nBut I am Pigeon-Liuer'd, and lacke Gall\\nTo make Oppression bitter, or ere this,\\nI should haue fatted all the Region Kites\\nWith this Slaues Offall, bloudy: a Bawdy villaine,\\nRemorselesse, Treacherous, Letcherous, kindles villaine!\\nOh Vengeance!\\nWho? What an Asse am I? I sure, this is most braue,\\nThat I, the Sonne of the Deere murthered,\\nPrompted to my Reuenge by Heauen, and Hell,\\nMust (like a Whore) vnpacke my heart with words,\\nAnd fall a Cursing like a very Drab.\\nA Scullion? Fye vpon't: Foh. About my Braine.\\nI haue heard, that guilty Creatures sitting at a Play,\\nHaue by the very cunning of the Scoene,\\nBene strooke so to the soule, that presently\\nThey haue proclaim'd their Malefactions.\\nFor Murther, though it haue no tongue, will speake\\nWith most myraculous Organ. Ile haue these Players,\\nPlay something like the murder of my Father,\\nBefore mine Vnkle. Ile obserue his lookes,\\nIle rent him to the quicke: If he but blench\\nI know my course. The Spirit that I haue seene\\nMay be the Diuell, and the Diuel hath power\\nT' assume a pleasing shape, yea and perhaps\\nOut of my Weaknesse, and my Melancholly,\\nAs he is very potent with such Spirits,\\nAbuses me to damne me. Ile haue grounds\\nMore Relatiue then this: The Play's the thing,\\nWherein Ile catch the Conscience of the King.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Exit\\n\\nEnter King, Queene, Polonius, Ophelia, Rosincrance,\\nGuildenstern, and\\nLords.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='King. And can you by no drift of circumstance\\nGet from him why he puts on this Confusion:\\nGrating so harshly all his dayes of quiet\\nWith turbulent and dangerous Lunacy'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Rosin. He does confesse he feeles himselfe distracted,\\nBut from what cause he will by no meanes speake'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Guil. Nor do we finde him forward to be sounded,\\nBut with a crafty Madnesse keepes aloofe:\\nWhen we would bring him on to some Confession\\nOf his true state'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Qu. Did he receiue you well?\\n  Rosin. Most like a Gentleman\\n\\n   Guild. But with much forcing of his disposition\\n\\n   Rosin. Niggard of question, but of our demands\\nMost free in his reply'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Qu. Did you assay him to any pastime?\\n  Rosin. Madam, it so fell out, that certaine Players\\nWe ore-wrought on the way: of these we told him,\\nAnd there did seeme in him a kinde of ioy\\nTo heare of it: They are about the Court,\\nAnd (as I thinke) they haue already order\\nThis night to play before him'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Pol. 'Tis most true:\\nAnd he beseech'd me to intreate your Maiesties\\nTo heare, and see the matter\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"King. With all my heart, and it doth much content me\\nTo heare him so inclin'd. Good Gentlemen,\\nGiue him a further edge, and driue his purpose on\\nTo these delights\\n\\n   Rosin. We shall my Lord.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Exeunt.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"King. Sweet Gertrude leaue vs too,\\nFor we haue closely sent for Hamlet hither,\\nThat he, as 'twere by accident, may there\\nAffront Ophelia. Her Father, and my selfe (lawful espials)\\nWill so bestow our selues, that seeing vnseene\\nWe may of their encounter frankely iudge,\\nAnd gather by him, as he is behaued,\\nIf't be th' affliction of his loue, or no.\\nThat thus he suffers for\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Qu. I shall obey you,\\nAnd for your part Ophelia, I do wish\\nThat your good Beauties be the happy cause\\nOf Hamlets wildenesse: so shall I hope your Vertues\\nWill bring him to his wonted way againe,\\nTo both your Honors'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ophe. Madam, I wish it may'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Pol. Ophelia, walke you heere. Gracious so please ye\\nWe will bestow our selues: Reade on this booke,\\nThat shew of such an exercise may colour\\nYour lonelinesse. We are oft too blame in this,\\n'Tis too much prou'd, that with Deuotions visage,\\nAnd pious Action, we do surge o're\\nThe diuell himselfe\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"King. Oh 'tis true:\\nHow smart a lash that speech doth giue my Conscience?\\nThe Harlots Cheeke beautied with plaist'ring Art\\nIs not more vgly to the thing that helpes it,\\nThen is my deede, to my most painted word.\\nOh heauie burthen!\\n  Pol. I heare him comming, let's withdraw my Lord.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Exeunt.\\n\\nEnter Hamlet.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. To be, or not to be, that is the Question:\\nWhether 'tis Nobler in the minde to suffer\\nThe Slings and Arrowes of outragious Fortune,\\nOr to take Armes against a Sea of troubles,\\nAnd by opposing end them: to dye, to sleepe\\nNo more; and by a sleepe, to say we end\\nThe Heart-ake, and the thousand Naturall shockes\\nThat Flesh is heyre too? 'Tis a consummation\\nDeuoutly to be wish'd. To dye to sleepe,\\nTo sleepe, perchance to Dreame; I, there's the rub,\\nFor in that sleepe of death, what dreames may come,\\nWhen we haue shuffel'd off this mortall coile,\\nMust giue vs pawse. There's the respect\\nThat makes Calamity of so long life:\\nFor who would beare the Whips and Scornes of time,\\nThe Oppressors wrong, the poore mans Contumely,\\nThe pangs of dispriz'd Loue, the Lawes delay,\\nThe insolence of Office, and the Spurnes\\nThat patient merit of the vnworthy takes,\\nWhen he himselfe might his Quietus make\\nWith a bare Bodkin? Who would these Fardles beare\\nTo grunt and sweat vnder a weary life,\\nBut that the dread of something after death,\\nThe vndiscouered Countrey, from whose Borne\\nNo Traueller returnes, Puzels the will,\\nAnd makes vs rather beare those illes we haue,\\nThen flye to others that we know not of.\\nThus Conscience does make Cowards of vs all,\\nAnd thus the Natiue hew of Resolution\\nIs sicklied o're, with the pale cast of Thought,\\nAnd enterprizes of great pith and moment,\\nWith this regard their Currants turne away,\\nAnd loose the name of Action. Soft you now,\\nThe faire Ophelia? Nimph, in thy Orizons\\nBe all my sinnes remembred\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ophe. Good my Lord,\\nHow does your Honor for this many a day?\\n  Ham. I humbly thanke you: well, well, well'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ophe. My Lord, I haue Remembrances of yours,\\nThat I haue longed long to re-deliuer.\\nI pray you now, receiue them\\n\\n   Ham. No, no, I neuer gaue you ought'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ophe. My honor'd Lord, I know right well you did,\\nAnd with them words of so sweet breath compos'd,\\nAs made the things more rich, then perfume left:\\nTake these againe, for to the Noble minde\\nRich gifts wax poore, when giuers proue vnkinde.\\nThere my Lord\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. Ha, ha: Are you honest?\\n  Ophe. My Lord'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. Are you faire?\\n  Ophe. What meanes your Lordship?\\n  Ham. That if you be honest and faire, your Honesty\\nshould admit no discourse to your Beautie'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ophe. Could Beautie my Lord, haue better Comerce\\nthen your Honestie?\\n  Ham. I trulie: for the power of Beautie, will sooner\\ntransforme Honestie from what is, to a Bawd, then the\\nforce of Honestie can translate Beautie into his likenesse.\\nThis was sometime a Paradox, but now the time giues it\\nproofe. I did loue you once'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ophe. Indeed my Lord, you made me beleeue so\\n\\n   Ham. You should not haue beleeued me. For vertue\\ncannot so innocculate our old stocke, but we shall rellish\\nof it. I loued you not'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ophe. I was the more deceiued'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Get thee to a Nunnerie. Why would'st thou\\nbe a breeder of Sinners? I am my selfe indifferent honest,\\nbut yet I could accuse me of such things, that it were better\\nmy Mother had not borne me. I am very prowd, reuengefull,\\nAmbitious, with more offences at my becke,\\nthen I haue thoughts to put them in imagination, to giue\\nthem shape, or time to acte them in. What should such\\nFellowes as I do, crawling betweene Heauen and Earth.\\nWe are arrant Knaues all, beleeue none of vs. Goe thy\\nwayes to a Nunnery. Where's your Father?\\n  Ophe. At home, my Lord\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Let the doores be shut vpon him, that he may\\nplay the Foole no way, but in's owne house. Farewell\\n\\n   Ophe. O helpe him, you sweet Heauens\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. If thou doest Marry, Ile giue thee this Plague\\nfor thy Dowrie. Be thou as chast as Ice, as pure as Snow,\\nthou shalt not escape Calumny. Get thee to a Nunnery.\\nGo, Farewell. Or if thou wilt needs Marry, marry a fool:\\nfor Wise men know well enough, what monsters you\\nmake of them. To a Nunnery go, and quickly too. Farwell'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ophe. O heauenly Powers, restore him'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. I haue heard of your pratlings too wel enough.\\nGod has giuen you one pace, and you make your selfe another:\\nyou gidge, you amble, and you lispe, and nickname\\nGods creatures, and make your Wantonnesse, your Ignorance.\\nGo too, Ile no more on't, it hath made me mad.\\nI say, we will haue no more Marriages. Those that are\\nmarried already, all but one shall liue, the rest shall keep\\nas they are. To a Nunnery, go.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Exit Hamlet.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ophe. O what a Noble minde is heere o're-throwne?\\nThe Courtiers, Soldiers, Schollers: Eye, tongue, sword,\\nTh' expectansie and Rose of the faire State,\\nThe glasse of Fashion, and the mould of Forme,\\nTh' obseru'd of all Obseruers, quite, quite downe.\\nHaue I of Ladies most deiect and wretched,\\nThat suck'd the Honie of his Musicke Vowes:\\nNow see that Noble, and most Soueraigne Reason,\\nLike sweet Bels iangled out of tune, and harsh,\\nThat vnmatch'd Forme and Feature of blowne youth,\\nBlasted with extasie. Oh woe is me,\\nT'haue seene what I haue seene: see what I see.\\nEnter King, and Polonius.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"King. Loue? His affections do not that way tend,\\nNor what he spake, though it lack'd Forme a little,\\nWas not like Madnesse. There's something in his soule?\\nO're which his Melancholly sits on brood,\\nAnd I do doubt the hatch, and the disclose\\nWill be some danger, which to preuent\\nI haue in quicke determination\\nThus set it downe. He shall with speed to England\\nFor the demand of our neglected Tribute:\\nHaply the Seas and Countries different\\nWith variable Obiects, shall expell\\nThis something setled matter in his heart:\\nWhereon his Braines still beating, puts him thus\\nFrom fashion of himselfe. What thinke you on't?\\n  Pol. It shall do well. But yet do I beleeue\\nThe Origin and Commencement of this greefe\\nSprung from neglected loue. How now Ophelia?\\nYou neede not tell vs, what Lord Hamlet saide,\\nWe heard it all. My Lord, do as you please,\\nBut if you hold it fit after the Play,\\nLet his Queene Mother all alone intreat him\\nTo shew his Greefes: let her be round with him,\\nAnd Ile be plac'd so, please you in the eare\\nOf all their Conference. If she finde him not,\\nTo England send him: Or confine him where\\nYour wisedome best shall thinke\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"King. It shall be so:\\nMadnesse in great Ones, must not vnwatch'd go.\\n\\nExeunt.\\n\\nEnter Hamlet, and two or three of the Players.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Speake the Speech I pray you, as I pronounc'd\\nit to you trippingly on the Tongue: But if you mouth it,\\nas many of your Players do, I had as liue the Town-Cryer\\nhad spoke my Lines: Nor do not saw the Ayre too much\\nyour hand thus, but vse all gently; for in the verie Torrent,\\nTempest, and (as I say) the Whirle-winde of\\nPassion, you must acquire and beget a Temperance that\\nmay giue it Smoothnesse. O it offends mee to the Soule,\\nto see a robustious Pery-wig-pated Fellow, teare a Passion\\nto tatters, to verie ragges, to split the eares of the\\nGroundlings: who (for the most part) are capeable of\\nnothing, but inexplicable dumbe shewes, & noise: I could\\nhaue such a Fellow whipt for o're-doing Termagant: it\\noutHerod's Herod. Pray you auoid it\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Player. I warrant your Honor'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Be not too tame neyther: but let your owne\\nDiscretion be your Tutor. Sute the Action to the Word,\\nthe Word to the Action, with this speciall obseruance:\\nThat you ore-stop not the modestie of Nature; for any\\nthing so ouer-done, is fro[m] the purpose of Playing, whose\\nend both at the first and now, was and is, to hold as 'twer\\nthe Mirrour vp to Nature; to shew Vertue her owne\\nFeature, Scorne her owne Image, and the verie Age and\\nBodie of the Time, his forme and pressure. Now, this\\nouer-done, or come tardie off, though it make the vnskilfull\\nlaugh, cannot but make the Iudicious greeue; The\\ncensure of the which One, must in your allowance o'reway\\na whole Theater of Others. Oh, there bee Players\\nthat I haue seene Play, and heard others praise, and that\\nhighly (not to speake it prophanely) that neyther hauing\\nthe accent of Christians, nor the gate of Christian, Pagan,\\nor Norman, haue so strutted and bellowed, that I haue\\nthought some of Natures Iouerney-men had made men,\\nand not made them well, they imitated Humanity so abhominably\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Play. I hope we haue reform'd that indifferently with\\nvs, Sir\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. O reforme it altogether. And let those that\\nplay your Clownes, speake no more then is set downe for\\nthem. For there be of them, that will themselues laugh,\\nto set on some quantitie of barren Spectators to laugh\\ntoo, though in the meane time, some necessary Question\\nof the Play be then to be considered: that's Villanous, &\\nshewes a most pittifull Ambition in the Foole that vses\\nit. Go make you readie.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Exit Players.\\n\\nEnter Polonius, Rosincrance, and Guildensterne.\\n\\nHow now my Lord,\\nWill the King heare this peece of Worke?\\n  Pol. And the Queene too, and that presently'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. Bid the Players make hast.\\n\\nExit Polonius.\\n\\nWill you two helpe to hasten them?\\n  Both. We will my Lord.\\n\\nExeunt.\\n\\nEnter Horatio.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Enter Horatio.\\n\\n  Ham. What hoa, Horatio?\\n  Hora. Heere sweet Lord, at your Seruice\\n\\n   Ham. Horatio, thou art eene as iust a man\\nAs ere my Conuersation coap'd withall\\n\\n   Hora. O my deere Lord\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Nay, do not thinke I flatter:\\nFor what aduancement may I hope from thee,\\nThat no Reuennew hast, but thy good spirits\\nTo feed & cloath thee. Why shold the poor be flatter'd?\\nNo, let the Candied tongue, like absurd pompe,\\nAnd crooke the pregnant Hindges of the knee,\\nWhere thrift may follow faining? Dost thou heare,\\nSince my deere Soule was Mistris of my choyse,\\nAnd could of men distinguish, her election\\nHath seal'd thee for her selfe. For thou hast bene\\nAs one in suffering all, that suffers nothing.\\nA man that Fortunes buffets, and Rewards\\nHath 'tane with equall Thankes. And blest are those,\\nWhose Blood and Iudgement are so well co-mingled,\\nThat they are not a Pipe for Fortunes finger.\\nTo sound what stop she please. Giue me that man,\\nThat is not Passions Slaue, and I will weare him\\nIn my hearts Core. I, in my Heart of heart,\\nAs I do thee. Something too much of this.\\nThere is a Play to night to before the King.\\nOne Scoene of it comes neere the Circumstance\\nWhich I haue told thee, of my Fathers death.\\nI prythee, when thou see'st that Acte a-foot,\\nEuen with the verie Comment of my Soule\\nObserue mine Vnkle: If his occulted guilt,\\nDo not it selfe vnkennell in one speech,\\nIt is a damned Ghost that we haue seene:\\nAnd my Imaginations are as foule\\nAs Vulcans Stythe. Giue him needfull note,\\nFor I mine eyes will riuet to his Face:\\nAnd after we will both our iudgements ioyne,\\nTo censure of his seeming\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Hora. Well my Lord.\\nIf he steale ought the whil'st this Play is Playing,\\nAnd scape detecting, I will pay the Theft.\\nEnter King, Queene, Polonius, Ophelia, Rosincrance,\\nGuildensterne, and\\nother Lords attendant with his Guard carrying Torches. Danish\\nMarch. Sound\\na Flourish.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. They are comming to the Play: I must be idle.\\nGet you a place'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"King. How fares our Cosin Hamlet?\\n  Ham. Excellent Ifaith, of the Camelions dish: I eate\\nthe Ayre promise-cramm'd, you cannot feed Capons so\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='King. I haue nothing with this answer Hamlet, these\\nwords are not mine'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. No, nor mine. Now my Lord, you plaid once\\ni'th' Vniuersity, you say?\\n  Polon. That I did my Lord, and was accounted a good\\nActor\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. And what did you enact?\\n  Pol. I did enact Iulius Caesar, I was kill'd i'th' Capitol:\\nBrutus kill'd me\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. It was a bruite part of him, to kill so Capitall a\\nCalfe there. Be the Players ready?\\n  Rosin. I my Lord, they stay vpon your patience\\n\\n   Qu. Come hither my good Hamlet, sit by me'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ha. No good Mother, here's Mettle more attractiue\\n\\n   Pol. Oh ho, do you marke that?\\n  Ham. Ladie, shall I lye in your Lap?\\n  Ophe. No my Lord\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. I meane, my Head vpon your Lap?\\n  Ophe. I my Lord\\n\\n   Ham. Do you thinke I meant Country matters?\\n  Ophe. I thinke nothing, my Lord'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. That's a faire thought to ly betweene Maids legs\\n  Ophe. What is my Lord?\\n  Ham. Nothing\\n\\n   Ophe. You are merrie, my Lord?\\n  Ham. Who I?\\n  Ophe. I my Lord\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Oh God, your onely Iigge-maker: what should\\na man do, but be merrie. For looke you how cheerefully\\nmy Mother lookes, and my Father dyed within's two\\nHoures\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ophe. Nay, 'tis twice two moneths, my Lord\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. So long? Nay then let the Diuel weare blacke,\\nfor Ile haue a suite of Sables. Oh Heauens! dye two moneths\\nago, and not forgotten yet? Then there's hope, a\\ngreat mans Memorie, may out-liue his life halfe a yeare:\\nBut byrlady he must builde Churches then: or else shall\\nhe suffer not thinking on, with the Hoby-horsse, whose\\nEpitaph is, For o, For o, the Hoby-horse is forgot.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Hoboyes play. The dumbe shew enters.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Enter a King and Queene, very louingly; the Queene embracing\\nhim. She\\nkneeles, and makes shew of Protestation vnto him. He takes her\\nvp, and\\ndeclines his head vpon her neck. Layes him downe vpon a Banke\\nof Flowers.\\nShe seeing him a-sleepe, leaues him. Anon comes in a Fellow,\\ntakes off his\\nCrowne, kisses it, and powres poyson in the Kings eares, and\\nExits. The\\nQueene returnes, findes the King dead, and makes passionate\\nAction. The\\nPoysoner, with some two or three Mutes comes in againe, seeming\\nto lament\\nwith her. The dead body is carried away: The Poysoner Wooes the\\nQueene with\\nGifts, she seemes loath and vnwilling awhile, but in the end,\\naccepts his\\nloue.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Exeunt.\\n\\n  Ophe. What meanes this, my Lord?\\n  Ham. Marry this is Miching Malicho, that meanes\\nMischeefe'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ophe. Belike this shew imports the Argument of the\\nPlay?\\n  Ham. We shall know by these Fellowes: the Players\\ncannot keepe counsell, they'l tell all\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ophe. Will they tell vs what this shew meant?\\n  Ham. I, or any shew that you'l shew him. Bee not\\nyou asham'd to shew, hee'l not shame to tell you what it\\nmeanes\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ophe. You are naught, you are naught, Ile marke the\\nPlay.\\nEnter Prologue.\\n\\nFor vs, and for our Tragedie,\\nHeere stooping to your Clemencie:\\nWe begge your hearing Patientlie'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Is this a Prologue, or the Poesie of a Ring?\\n  Ophe. 'Tis briefe my Lord\\n\\n   Ham. As Womans loue.\\nEnter King and his Queene.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='King. Full thirtie times hath Phoebus Cart gon round,\\nNeptunes salt Wash, and Tellus Orbed ground:\\nAnd thirtie dozen Moones with borrowed sheene,\\nAbout the World haue times twelue thirties beene,\\nSince loue our hearts, and Hymen did our hands\\nVnite comutuall, in most sacred Bands'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Bap. So many iournies may the Sunne and Moone\\nMake vs againe count o're, ere loue be done.\\nBut woe is me, you are so sicke of late,\\nSo farre from cheere, and from your former state,\\nThat I distrust you: yet though I distrust,\\nDiscomfort you (my Lord) it nothing must:\\nFor womens Feare and Loue, holds quantitie,\\nIn neither ought, or in extremity:\\nNow what my loue is, proofe hath made you know,\\nAnd as my Loue is siz'd, my Feare is so\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"King. Faith I must leaue thee Loue, and shortly too:\\nMy operant Powers my Functions leaue to do:\\nAnd thou shalt liue in this faire world behinde,\\nHonour'd, belou'd, and haply, one as kinde.\\nFor Husband shalt thou-\\n  Bap. Oh confound the rest:\\nSuch Loue, must needs be Treason in my brest:\\nIn second Husband, let me be accurst,\\nNone wed the second, but who kill'd the first\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. Wormwood, Wormwood\\n\\n   Bapt. The instances that second Marriage moue,\\nAre base respects of Thrift, but none of Loue.\\nA second time, I kill my Husband dead,\\nWhen second Husband kisses me in Bed'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"King. I do beleeue you. Think what now you speak:\\nBut what we do determine, oft we breake:\\nPurpose is but the slaue to Memorie,\\nOf violent Birth, but poore validitie:\\nWhich now like Fruite vnripe stickes on the Tree,\\nBut fall vnshaken, when they mellow bee.\\nMost necessary 'tis, that we forget\\nTo pay our selues, what to our selues is debt:\\nWhat to our selues in passion we propose,\\nThe passion ending, doth the purpose lose.\\nThe violence of other Greefe or Ioy,\\nTheir owne ennactors with themselues destroy:\\nWhere Ioy most Reuels, Greefe doth most lament;\\nGreefe ioyes, Ioy greeues on slender accident.\\nThis world is not for aye, nor 'tis not strange\\nThat euen our Loues should with our Fortunes change.\\nFor 'tis a question left vs yet to proue,\\nWhether Loue lead Fortune, or else Fortune Loue.\\nThe great man downe, you marke his fauourites flies,\\nThe poore aduanc'd, makes Friends of Enemies:\\nAnd hitherto doth Loue on Fortune tend,\\nFor who not needs, shall neuer lacke a Frend:\\nAnd who in want a hollow Friend doth try,\\nDirectly seasons him his Enemie.\\nBut orderly to end, where I begun,\\nOur Willes and Fates do so contrary run,\\nThat our Deuices still are ouerthrowne,\\nOur thoughts are ours, their ends none of our owne.\\nSo thinke thou wilt no second Husband wed.\\nBut die thy thoughts, when thy first Lord is dead\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Bap. Nor Earth to giue me food, nor Heauen light,\\nSport and repose locke from me day and night:\\nEach opposite that blankes the face of ioy,\\nMeet what I would haue well, and it destroy:\\nBoth heere, and hence, pursue me lasting strife,\\nIf once a Widdow, euer I be Wife'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. If she should breake it now\\n\\n   King. 'Tis deepely sworne:\\nSweet, leaue me heere a while,\\nMy spirits grow dull, and faine I would beguile\\nThe tedious day with sleepe\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Qu. Sleepe rocke thy Braine,\\n\\nSleepes\\n\\nAnd neuer come mischance betweene vs twaine.\\n\\nExit\\n\\n  Ham. Madam, how like you this Play?\\n  Qu. The Lady protests to much me thinkes'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Oh but shee'l keepe her word\\n\\n   King. Haue you heard the Argument, is there no Offence\\nin't?\\n  Ham. No, no, they do but iest, poyson in iest, no Offence\\ni'th' world\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"King. What do you call the Play?\\n  Ham. The Mouse-trap: Marry how? Tropically:\\nThis Play is the Image of a murder done in Vienna: Gonzago\\nis the Dukes name, his wife Baptista: you shall see\\nanon: 'tis a knauish peece of worke: But what o'that?\\nYour Maiestie, and wee that haue free soules, it touches\\nvs not: let the gall'd iade winch: our withers are vnrung.\\nEnter Lucianus.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='This is one Lucianus nephew to the King\\n\\n   Ophe. You are a good Chorus, my Lord\\n\\n   Ham. I could interpret betweene you and your loue:\\nif I could see the Puppets dallying'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ophe. You are keene my Lord, you are keene\\n\\n   Ham. It would cost you a groaning, to take off my\\nedge\\n\\n   Ophe. Still better and worse'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. So you mistake Husbands.\\nBegin Murderer. Pox, leaue thy damnable Faces, and\\nbegin. Come, the croaking Rauen doth bellow for Reuenge'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Lucian. Thoughts blacke, hands apt,\\nDrugges fit, and Time agreeing:\\nConfederate season, else, no Creature seeing:\\nThou mixture ranke, of Midnight Weeds collected,\\nWith Hecats Ban, thrice blasted, thrice infected,\\nThy naturall Magicke, and dire propertie,\\nOn wholsome life, vsurpe immediately.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Powres the poyson in his eares.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. He poysons him i'th' Garden for's estate: His\\nname's Gonzago: the Story is extant and writ in choyce\\nItalian. You shall see anon how the Murtherer gets the\\nloue of Gonzago's wife\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ophe. The King rises\\n\\n   Ham. What, frighted with false fire\\n\\n   Qu. How fares my Lord?\\n  Pol. Giue o're the Play\\n\\n   King. Giue me some Light. Away\\n\\n   All. Lights, Lights, Lights.\\n\\nExeunt.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Exeunt.\\n\\nManet Hamlet & Horatio.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Why let the strucken Deere go weepe,\\nThe Hart vngalled play:\\nFor some must watch, while some must sleepe;\\nSo runnes the world away.\\nWould not this Sir, and a Forrest of Feathers, if the rest of\\nmy Fortunes turne Turke with me; with two Prouinciall\\nRoses on my rac'd Shooes, get me a Fellowship in a crie\\nof Players sir\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Hor. Halfe a share\\n\\n   Ham. A whole one I,\\nFor thou dost know: Oh Damon deere,\\nThis Realme dismantled was of Ioue himselfe,\\nAnd now reignes heere.\\nA verie verie Paiocke'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Hora. You might haue Rim'd\\n\\n   Ham. Oh good Horatio, Ile take the Ghosts word for\\na thousand pound. Did'st perceiue?\\n  Hora. Verie well my Lord\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. Vpon the talke of the poysoning?\\n  Hora. I did verie well note him.\\nEnter Rosincrance and Guildensterne.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. Oh, ha? Come some Musick. Come y Recorders:\\nFor if the King like not the Comedie,\\nWhy then belike he likes it not perdie.\\nCome some Musicke\\n\\n   Guild. Good my Lord, vouchsafe me a word with you'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Sir, a whole History\\n\\n   Guild. The King, sir\\n\\n   Ham. I sir, what of him?\\n  Guild. Is in his retyrement, maruellous distemper'd\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. With drinke Sir?\\n  Guild. No my Lord, rather with choller'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. Your wisedome should shew it selfe more richer,\\nto signifie this to his Doctor: for for me to put him\\nto his Purgation, would perhaps plundge him into farre\\nmore Choller'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Guild. Good my Lord put your discourse into some\\nframe, and start not so wildely from my affayre\\n\\n   Ham. I am tame Sir, pronounce'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Guild. The Queene your Mother, in most great affliction\\nof spirit, hath sent me to you\\n\\n   Ham. You are welcome'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Guild. Nay, good my Lord, this courtesie is not of\\nthe right breed. If it shall please you to make me a wholsome\\nanswer, I will doe your Mothers command'ment:\\nif not, your pardon, and my returne shall bee the end of\\nmy Businesse\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. Sir, I cannot'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Guild. What, my Lord?\\n  Ham. Make you a wholsome answere: my wits diseas'd.\\nBut sir, such answers as I can make, you shal command:\\nor rather you say, my Mother: therfore no more\\nbut to the matter. My Mother you say\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Rosin. Then thus she sayes: your behauior hath stroke\\nher into amazement, and admiration'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. Oh wonderfull Sonne, that can so astonish a\\nMother. But is there no sequell at the heeles of this Mothers\\nadmiration?\\n  Rosin. She desires to speake with you in her Closset,\\nere you go to bed'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. We shall obey, were she ten times our Mother.\\nHaue you any further Trade with vs?\\n  Rosin. My Lord, you once did loue me\\n\\n   Ham. So I do still, by these pickers and stealers'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Rosin. Good my Lord, what is your cause of distemper?\\nYou do freely barre the doore of your owne Libertie,\\nif you deny your greefes to your Friend\\n\\n   Ham. Sir I lacke Aduancement'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Rosin. How can that be, when you haue the voyce of\\nthe King himselfe, for your Succession in Denmarke?\\n  Ham. I, but while the grasse growes, the Prouerbe is\\nsomething musty.\\nEnter one with a Recorder.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='O the Recorder. Let me see, to withdraw with you, why\\ndo you go about to recouer the winde of mee, as if you\\nwould driue me into a toyle?\\n  Guild. O my Lord, if my Dutie be too bold, my loue\\nis too vnmannerly'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. I do not well vnderstand that. Will you play\\nvpon this Pipe?\\n  Guild. My Lord, I cannot\\n\\n   Ham. I pray you\\n\\n   Guild. Beleeue me, I cannot\\n\\n   Ham. I do beseech you'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Guild. I know no touch of it, my Lord'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. 'Tis as easie as lying: gouerne these Ventiges\\nwith your finger and thumbe, giue it breath with your\\nmouth, and it will discourse most excellent Musicke.\\nLooke you, these are the stoppes\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Guild. But these cannot I command to any vtterance\\nof hermony, I haue not the skill'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. Why looke you now, how vnworthy a thing\\nyou make of me: you would play vpon mee; you would\\nseeme to know my stops: you would pluck out the heart\\nof my Mysterie; you would sound mee from my lowest\\nNote, to the top of my Compasse: and there is much Musicke,\\nexcellent Voice, in this little Organe, yet cannot\\nyou make it. Why do you thinke, that I am easier to bee\\nplaid on, then a Pipe? Call me what Instrument you will,\\nthough you can fret me, you cannot play vpon me. God\\nblesse you Sir.\\nEnter Polonius.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Polon. My Lord; the Queene would speak with you,\\nand presently\\n\\n   Ham. Do you see that Clowd? that's almost in shape\\nlike a Camell\\n\\n   Polon. By'th' Masse, and it's like a Camell indeed\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Me thinkes it is like a Weazell\\n\\n   Polon. It is back'd like a Weazell\\n\\n   Ham. Or like a Whale?\\n  Polon. Verie like a Whale\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. Then will I come to my Mother, by and by:\\nThey foole me to the top of my bent.\\nI will come by and by\\n\\n   Polon. I will say so.\\nEnter.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. By and by, is easily said. Leaue me Friends:\\n'Tis now the verie witching time of night,\\nWhen Churchyards yawne, and Hell it selfe breaths out\\nContagion to this world. Now could I drink hot blood,\\nAnd do such bitter businesse as the day\\nWould quake to looke on. Soft now, to my Mother:\\nOh Heart, loose not thy Nature; let not euer\\nThe Soule of Nero, enter this firme bosome:\\nLet me be cruell, not vnnaturall,\\nI will speake Daggers to her, but vse none:\\nMy Tongue and Soule in this be Hypocrites.\\nHow in my words someuer she be shent,\\nTo giue them Seales, neuer my Soule consent.\\nEnter King, Rosincrance, and Guildensterne.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='King. I like him not, nor stands it safe with vs,\\nTo let his madnesse range. Therefore prepare you,\\nI your Commission will forthwith dispatch,\\nAnd he to England shall along with you:\\nThe termes of our estate, may not endure\\nHazard so dangerous as doth hourely grow\\nOut of his Lunacies'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Guild. We will our selues prouide:\\nMost holie and Religious feare it is\\nTo keepe those many many bodies safe\\nThat liue and feede vpon your Maiestie'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Rosin. The single\\nAnd peculiar life is bound\\nWith all the strength and Armour of the minde,\\nTo keepe it selfe from noyance: but much more,\\nThat Spirit, vpon whose spirit depends and rests\\nThe liues of many, the cease of Maiestie\\nDies not alone; but like a Gulfe doth draw\\nWhat's neere it, with it. It is a massie wheele\\nFixt on the Somnet of the highest Mount.\\nTo whose huge Spoakes, ten thousand lesser things\\nAre mortiz'd and adioyn'd: which when it falles,\\nEach small annexment, pettie consequence\\nAttends the boystrous Ruine. Neuer alone\\nDid the King sighe, but with a generall grone\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='King. Arme you, I pray you to this speedie Voyage;\\nFor we will Fetters put vpon this feare,\\nWhich now goes too free-footed\\n\\n   Both. We will haste vs.\\n\\nExeunt. Gent.\\n\\nEnter Polonius.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Pol. My Lord, he's going to his Mothers Closset:\\nBehinde the Arras Ile conuey my selfe\\nTo heare the Processe. Ile warrant shee'l tax him home,\\nAnd as you said, and wisely was it said,\\n'Tis meete that some more audience then a Mother,\\nSince Nature makes them partiall, should o're-heare\\nThe speech of vantage. Fare you well my Liege,\\nIle call vpon you ere you go to bed,\\nAnd tell you what I know\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"King. Thankes deere my Lord.\\nOh my offence is ranke, it smels to heauen,\\nIt hath the primall eldest curse vpon't,\\nA Brothers murther. Pray can I not,\\nThough inclination be as sharpe as will:\\nMy stronger guilt, defeats my strong intent,\\nAnd like a man to double businesse bound,\\nI stand in pause where I shall first begin,\\nAnd both neglect; what if this cursed hand\\nWere thicker then it selfe with Brothers blood,\\nIs there not Raine enough in the sweet Heauens\\nTo wash it white as Snow? Whereto serues mercy,\\nBut to confront the visage of Offence?\\nAnd what's in Prayer, but this two-fold force,\\nTo be fore-stalled ere we come to fall,\\nOr pardon'd being downe? Then Ile looke vp,\\nMy fault is past. But oh, what forme of Prayer\\nCan serue my turne? Forgiue me my foule Murther:\\nThat cannot be, since I am still possest\\nOf those effects for which I did the Murther.\\nMy Crowne, mine owne Ambition, and my Queene:\\nMay one be pardon'd, and retaine th' offence?\\nIn the corrupted currants of this world,\\nOffences gilded hand may shoue by Iustice,\\nAnd oft 'tis seene, the wicked prize it selfe\\nBuyes out the Law; but 'tis not so aboue,\\nThere is no shuffling, there the Action lyes\\nIn his true Nature, and we our selues compell'd\\nEuen to the teeth and forehead of our faults,\\nTo giue in euidence. What then? What rests?\\nTry what Repentance can. What can it not?\\nYet what can it, when one cannot repent?\\nOh wretched state! Oh bosome, blacke as death!\\nOh limed soule, that strugling to be free,\\nArt more ingag'd: Helpe Angels, make assay:\\nBow stubborne knees, and heart with strings of Steele,\\nBe soft as sinewes of the new-borne Babe,\\nAll may be well.\\nEnter Hamlet.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Now might I do it pat, now he is praying,\\nAnd now Ile doo't, and so he goes to Heauen,\\nAnd so am I reueng'd: that would be scann'd,\\nA Villaine killes my Father, and for that\\nI his foule Sonne, do this same Villaine send\\nTo heauen. Oh this is hyre and Sallery, not Reuenge.\\nHe tooke my Father grossely, full of bread,\\nWith all his Crimes broad blowne, as fresh as May,\\nAnd how his Audit stands, who knowes, saue Heauen:\\nBut in our circumstance and course of thought\\n'Tis heauie with him: and am I then reueng'd,\\nTo take him in the purging of his Soule,\\nWhen he is fit and season'd for his passage? No.\\nVp Sword, and know thou a more horrid hent\\nWhen he is drunke asleepe: or in his Rage,\\nOr in th' incestuous pleasure of his bed,\\nAt gaming, swearing, or about some acte\\nThat ha's no rellish of Saluation in't,\\nThen trip him, that his heeles may kicke at Heauen,\\nAnd that his Soule may be as damn'd and blacke\\nAs Hell, whereto it goes. My Mother stayes,\\nThis Physicke but prolongs thy sickly dayes.\\nEnter.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='King. My words flye vp, my thoughts remain below,\\nWords without thoughts, neuer to Heauen go.\\nEnter.\\n\\nEnter Queene and Polonius.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Pol. He will come straight:\\nLooke you lay home to him,\\nTell him his prankes haue been too broad to beare with,\\nAnd that your Grace hath screen'd, and stoode betweene\\nMuch heate, and him. Ile silence me e'ene heere:\\nPray you be round with him\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. within. Mother, mother, mother\\n\\n   Qu. Ile warrant you, feare me not.\\nWithdraw, I heare him coming.\\nEnter Hamlet.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Now Mother, what's the matter?\\n  Qu. Hamlet, thou hast thy Father much offended\\n\\n\\n   Ham. Mother, you haue my Father much offended\\n\\n   Qu. Come, come, you answer with an idle tongue\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. Go, go, you question with an idle tongue'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Qu. Why how now Hamlet?\\n  Ham. Whats the matter now?\\n  Qu. Haue you forgot me?\\n  Ham. No by the Rood, not so:\\nYou are the Queene, your Husbands Brothers wife,\\nBut would you were not so. You are my Mother'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Qu. Nay, then Ile set those to you that can speake'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. Come, come, and sit you downe, you shall not\\nboudge:\\nYou go not till I set you vp a glasse,\\nWhere you may see the inmost part of you?\\n  Qu. What wilt thou do? thou wilt not murther me?\\nHelpe, helpe, hoa'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Pol. What hoa, helpe, helpe, helpe\\n\\n   Ham. How now, a Rat? dead for a Ducate, dead\\n\\n   Pol. Oh I am slaine.\\n\\nKilles Polonius'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Qu. Oh me, what hast thou done?\\n  Ham. Nay I know not, is it the King?\\n  Qu. Oh what a rash, and bloody deed is this?\\n  Ham. A bloody deed, almost as bad good Mother,\\nAs kill a King, and marrie with his Brother'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Qu. As kill a King?\\n  Ham. I Lady, 'twas my word.\\nThou wretched, rash, intruding foole farewell,\\nI tooke thee for thy Betters, take thy Fortune,\\nThou find'st to be too busie, is some danger.\\nLeaue wringing of your hands, peace, sit you downe,\\nAnd let me wring your heart, for so I shall\\nIf it be made of penetrable stuffe;\\nIf damned Custome haue not braz'd it so,\\nThat it is proofe and bulwarke against Sense\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Qu. What haue I done, that thou dar'st wag thy tong,\\nIn noise so rude against me?\\n  Ham. Such an Act\\nThat blurres the grace and blush of Modestie,\\nCals Vertue Hypocrite, takes off the Rose\\nFrom the faire forehead of an innocent loue,\\nAnd makes a blister there. Makes marriage vowes\\nAs false as Dicers Oathes. Oh such a deed,\\nAs from the body of Contraction pluckes\\nThe very soule, and sweete Religion makes\\nA rapsidie of words. Heauens face doth glow,\\nYea this solidity and compound masse,\\nWith tristfull visage as against the doome,\\nIs thought-sicke at the act\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Qu. Aye me; what act, that roares so lowd, & thunders\\nin the Index'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Looke heere vpon this Picture, and on this,\\nThe counterfet presentment of two Brothers:\\nSee what a grace was seated on his Brow,\\nHyperions curles, the front of Ioue himselfe,\\nAn eye like Mars, to threaten or command\\nA Station, like the Herald Mercurie\\nNew lighted on a heauen-kissing hill:\\nA Combination, and a forme indeed,\\nWhere euery God did seeme to set his Seale,\\nTo giue the world assurance of a man.\\nThis was your Husband. Looke you now what followes.\\nHeere is your Husband, like a Mildew'd eare\\nBlasting his wholsom breath. Haue you eyes?\\nCould you on this faire Mountaine leaue to feed,\\nAnd batten on this Moore? Ha? Haue you eyes?\\nYou cannot call it Loue: For at your age,\\nThe hey-day in the blood is tame, it's humble,\\nAnd waites vpon the Iudgement: and what Iudgement\\nWould step from this, to this? What diuell was't,\\nThat thus hath cousend you at hoodman-blinde?\\nO Shame! where is thy Blush? Rebellious Hell,\\nIf thou canst mutine in a Matrons bones,\\nTo flaming youth, let Vertue be as waxe.\\nAnd melt in her owne fire. Proclaime no shame,\\nWhen the compulsiue Ardure giues the charge,\\nSince Frost it selfe, as actiuely doth burne,\\nAs Reason panders Will\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Qu. O Hamlet, speake no more.\\nThou turn'st mine eyes into my very soule,\\nAnd there I see such blacke and grained spots,\\nAs will not leaue their Tinct\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Nay, but to liue\\nIn the ranke sweat of an enseamed bed,\\nStew'd in Corruption; honying and making loue\\nOuer the nasty Stye\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Qu. Oh speake to me, no more,\\nThese words like Daggers enter in mine eares.\\nNo more sweet Hamlet'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. A Murderer, and a Villaine:\\nA Slaue, that is not twentieth part the tythe\\nOf your precedent Lord. A vice of Kings,\\nA Cutpurse of the Empire and the Rule.\\nThat from a shelfe, the precious Diadem stole,\\nAnd put it in his Pocket'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Qu. No more.\\nEnter Ghost.\\n\\n  Ham. A King of shreds and patches.\\nSaue me; and houer o're me with your wings\\nYou heauenly Guards. What would your gracious figure?\\n  Qu. Alas he's mad\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Do you not come your tardy Sonne to chide,\\nThat laps't in Time and Passion, lets go by\\nTh' important acting of your dread command? Oh say\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ghost. Do not forget: this Visitation\\nIs but to whet thy almost blunted purpose.\\nBut looke, Amazement on thy Mother sits;\\nO step betweene her, and her fighting Soule,\\nConceit in weakest bodies, strongest workes.\\nSpeake to her Hamlet'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. How is it with you Lady?\\n  Qu. Alas, how is't with you?\\nThat you bend your eye on vacancie,\\nAnd with their corporall ayre do hold discourse.\\nForth at your eyes, your spirits wildely peepe,\\nAnd as the sleeping Soldiours in th' Alarme,\\nYour bedded haire, like life in excrements,\\nStart vp, and stand an end. Oh gentle Sonne,\\nVpon the heate and flame of thy distemper\\nSprinkle coole patience. Whereon do you looke?\\n  Ham. On him, on him: look you how pale he glares,\\nHis forme and cause conioyn'd, preaching to stones,\\nWould make them capeable. Do not looke vpon me,\\nLeast with this pitteous action you conuert\\nMy sterne effects: then what I haue to do,\\nWill want true colour; teares perchance for blood\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Qu. To who do you speake this?\\n  Ham. Do you see nothing there?\\n  Qu. Nothing at all, yet all that is I see\\n\\n   Ham. Nor did you nothing heare?\\n  Qu. No, nothing but our selues'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. Why look you there: looke how it steals away:\\nMy Father in his habite, as he liued,\\nLooke where he goes euen now out at the Portall.\\nEnter.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Qu. This is the very coynage of your Braine,\\nThis bodilesse Creation extasie is very cunning in'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Extasie?\\nMy Pulse as yours doth temperately keepe time,\\nAnd makes as healthfull Musicke. It is not madnesse\\nThat I haue vttered; bring me to the Test\\nAnd I the matter will re-word: which madnesse\\nWould gamboll from. Mother, for loue of Grace,\\nLay not a flattering Vnction to your soule,\\nThat not your trespasse, but my madnesse speakes:\\nIt will but skin and filme the Vlcerous place,\\nWhil'st ranke Corruption mining all within,\\nInfects vnseene. Confesse your selfe to Heauen,\\nRepent what's past, auoyd what is to come,\\nAnd do not spred the Compost on the Weedes,\\nTo make them ranke. Forgiue me this my Vertue,\\nFor in the fatnesse of this pursie times,\\nVertue it selfe, of Vice must pardon begge,\\nYea courb, and woe, for leaue to do him good\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Qu. Oh Hamlet,\\nThou hast cleft my heart in twaine'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. O throw away the worser part of it,\\nAnd liue the purer with the other halfe.\\nGood night, but go not to mine Vnkles bed,\\nAssume a Vertue, if you haue it not, refraine to night,\\nAnd that shall lend a kinde of easinesse\\nTo the next abstinence. Once more goodnight,\\nAnd when you are desirous to be blest,\\nIle blessing begge of you. For this same Lord,\\nI do repent: but heauen hath pleas'd it so,\\nTo punish me with this, and this with me,\\nThat I must be their Scourge and Minister.\\nI will bestow him, and will answer well\\nThe death I gaue him: so againe, good night.\\nI must be cruell, onely to be kinde;\\nThus bad begins and worse remaines behinde\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Qu. What shall I do?\\n  Ham. Not this by no meanes that I bid you do:\\nLet the blunt King tempt you againe to bed,\\nPinch Wanton on your cheeke, call you his Mouse,\\nAnd let him for a paire of reechie kisses,\\nOr padling in your necke with his damn'd Fingers,\\nMake you to rauell all this matter out,\\nThat I essentially am not in madnesse,\\nBut made in craft. 'Twere good you let him know,\\nFor who that's but a Queene, faire, sober, wise,\\nWould from a Paddocke, from a Bat, a Gibbe,\\nSuch deere concernings hide, Who would do so,\\nNo in despight of Sense and Secrecie,\\nVnpegge the Basket on the houses top:\\nLet the Birds flye, and like the famous Ape\\nTo try Conclusions in the Basket, creepe\\nAnd breake your owne necke downe\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Qu. Be thou assur'd, if words be made of breath,\\nAnd breath of life: I haue no life to breath\\nWhat thou hast saide to me\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. I must to England, you know that?\\n  Qu. Alacke I had forgot: 'Tis so concluded on\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. This man shall set me packing:\\nIle lugge the Guts into the Neighbor roome,\\nMother goodnight. Indeede this Counsellor\\nIs now most still, most secret, and most graue,\\nWho was in life, a foolish prating Knaue.\\nCome sir, to draw toward an end with you.\\nGood night Mother.\\nExit Hamlet tugging in Polonius.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Enter King.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"King. There's matters in these sighes.\\nThese profound heaues\\nYou must translate; Tis fit we vnderstand them.\\nWhere is your Sonne?\\n  Qu. Ah my good Lord, what haue I seene to night?\\n  King. What Gertrude? How do's Hamlet?\\n  Qu. Mad as the Seas, and winde, when both contend\\nWhich is the Mightier, in his lawlesse fit\\nBehinde the Arras, hearing something stirre,\\nHe whips his Rapier out, and cries a Rat, a Rat,\\nAnd in his brainish apprehension killes\\nThe vnseene good old man\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"King. Oh heauy deed:\\nIt had bin so with vs had we beene there:\\nHis Liberty is full of threats to all,\\nTo you your selfe, to vs, to euery one.\\nAlas, how shall this bloody deede be answered?\\nIt will be laide to vs, whose prouidence\\nShould haue kept short, restrain'd, and out of haunt,\\nThis mad yong man. But so much was our loue,\\nWe would not vnderstand what was most fit,\\nBut like the Owner of a foule disease,\\nTo keepe it from divulging, let's it feede\\nEuen on the pith of life. Where is he gone?\\n  Qu. To draw apart the body he hath kild,\\nO're whom his very madnesse like some Oare\\nAmong a Minerall of Mettels base\\nShewes it selfe pure. He weepes for what is done\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='King. Oh Gertrude, come away:\\nThe Sun no sooner shall the Mountaines touch,\\nBut we will ship him hence, and this vilde deed,\\nWe must with all our Maiesty and Skill\\nBoth countenance, and excuse.\\nEnter Ros. & Guild.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ho Guildenstern:\\nFriends both go ioyne you with some further ayde:\\nHamlet in madnesse hath Polonius slaine,\\nAnd from his Mother Clossets hath he drag'd him.\\nGo seeke him out, speake faire, and bring the body\\nInto the Chappell. I pray you hast in this.\\nExit Gent.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Come Gertrude, wee'l call vp our wisest friends,\\nTo let them know both what we meane to do,\\nAnd what's vntimely done. Oh come away,\\nMy soule is full of discord and dismay.\\n\\nExeunt.\\n\\nEnter Hamlet.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Enter Hamlet.\\n\\n  Ham. Safely stowed\\n\\n   Gentlemen within. Hamlet, Lord Hamlet\\n\\n   Ham. What noise? Who cals on Hamlet?\\nOh heere they come.\\nEnter Ros. and Guildensterne.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ro. What haue you done my Lord with the dead body?\\n  Ham. Compounded it with dust, whereto 'tis Kinne\\n\\n   Rosin. Tell vs where 'tis, that we may take it thence,\\nAnd beare it to the Chappell\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. Do not beleeue it'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Rosin. Beleeue what?\\n  Ham. That I can keepe your counsell, and not mine\\nowne. Besides, to be demanded of a Spundge, what replication\\nshould be made by the Sonne of a King'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Rosin. Take you me for a Spundge, my Lord?\\n  Ham. I sir, that sokes vp the Kings Countenance, his\\nRewards, his Authorities (but such Officers do the King\\nbest seruice in the end. He keepes them like an Ape in\\nthe corner of his iaw, first mouth'd to be last swallowed,\\nwhen he needes what you haue glean'd, it is but squeezing\\nyou, and Spundge you shall be dry againe\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Rosin. I vnderstand you not my Lord\\n\\n   Ham. I am glad of it: a knauish speech sleepes in a\\nfoolish eare\\n\\n   Rosin. My Lord, you must tell vs where the body is,\\nand go with vs to the King'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. The body is with the King, but the King is not\\nwith the body. The King, is a thing-\\n  Guild. A thing my Lord?\\n  Ham. Of nothing: bring me to him, hide Fox, and all\\nafter.\\n\\nExeunt.\\n\\nEnter King.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"King. I haue sent to seeke him, and to find the bodie:\\nHow dangerous is it that this man goes loose:\\nYet must not we put the strong Law on him:\\nHee's loued of the distracted multitude,\\nWho like not in their iudgement, but their eyes:\\nAnd where 'tis so, th' Offenders scourge is weigh'd\\nBut neerer the offence: to beare all smooth, and euen,\\nThis sodaine sending him away, must seeme\\nDeliberate pause, diseases desperate growne,\\nBy desperate appliance are releeued,\\nOr not at all.\\nEnter Rosincrane.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"How now? What hath befalne?\\n  Rosin. Where the dead body is bestow'd my Lord,\\nWe cannot get from him\\n\\n   King. But where is he?\\n  Rosin. Without my Lord, guarded to know your\\npleasure\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"King. Bring him before vs\\n\\n   Rosin. Hoa, Guildensterne? Bring in my Lord.\\nEnter Hamlet and Guildensterne.\\n\\n  King. Now Hamlet, where's Polonius?\\n  Ham. At Supper\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"King. At Supper? Where?\\n  Ham. Not where he eats, but where he is eaten, a certaine\\nconuocation of wormes are e'ne at him. Your worm\\nis your onely Emperor for diet. We fat all creatures else\\nto fat vs, and we fat our selfe for Magots. Your fat King,\\nand your leane Begger is but variable seruice to dishes,\\nbut to one Table that's the end\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='King. What dost thou meane by this?\\n  Ham. Nothing but to shew you how a King may go\\na Progresse through the guts of a Begger\\n\\n   King. Where is Polonius'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. In heauen, send thither to see. If your Messenger\\nfinde him not there, seeke him i'th other place your\\nselfe: but indeed, if you finde him not this moneth, you\\nshall nose him as you go vp the staires into the Lobby\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='King. Go seeke him there\\n\\n   Ham. He will stay till ye come'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"K. Hamlet, this deed of thine, for thine especial safety\\nWhich we do tender, as we deerely greeue\\nFor that which thou hast done, must send thee hence\\nWith fierie Quicknesse. Therefore prepare thy selfe,\\nThe Barke is readie, and the winde at helpe,\\nTh' Associates tend, and euery thing at bent\\nFor England\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. For England?\\n  King. I Hamlet\\n\\n   Ham. Good\\n\\n   King. So is it, if thou knew'st our purposes\\n\\n   Ham. I see a Cherube that see's him: but come, for\\nEngland. Farewell deere Mother\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='King. Thy louing Father Hamlet\\n\\n   Hamlet. My Mother: Father and Mother is man and\\nwife: man & wife is one flesh, and so my mother. Come,\\nfor England.\\n\\nExit'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"King. Follow him at foote,\\nTempt him with speed aboord:\\nDelay it not, Ile haue him hence to night.\\nAway, for euery thing is Seal'd and done\\nThat else leanes on th' Affaire, pray you make hast.\\nAnd England, if my loue thou holdst at ought,\\nAs my great power thereof may giue thee sense,\\nSince yet thy Cicatrice lookes raw and red\\nAfter the Danish Sword, and thy free awe\\nPayes homage to vs; thou maist not coldly set\\nOur Soueraigne Processe, which imports at full\\nBy Letters coniuring to that effect\\nThe present death of Hamlet. Do it England,\\nFor like the Hecticke in my blood he rages,\\nAnd thou must cure me: Till I know 'tis done,\\nHow ere my happes, my ioyes were ne're begun.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Exit\\n\\nEnter Fortinbras with an Armie.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"For. Go Captaine, from me greet the Danish King,\\nTell him that by his license, Fortinbras\\nClaimes the conueyance of a promis'd March\\nOuer his Kingdome. You know the Rendeuous:\\nIf that his Maiesty would ought with vs,\\nWe shall expresse our dutie in his eye,\\nAnd let him know so\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Cap. I will doo't, my Lord\\n\\n   For. Go safely on.\\nEnter.\\n\\nEnter Queene and Horatio.\\n\\n  Qu. I will not speake with her\\n\\n   Hor. She is importunate, indeed distract, her moode\\nwill needs be pittied\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Qu. What would she haue?\\n  Hor. She speakes much of her Father; saies she heares\\nThere's trickes i'th' world, and hems, and beats her heart,\\nSpurnes enuiously at Strawes, speakes things in doubt,\\nThat carry but halfe sense: Her speech is nothing,\\nYet the vnshaped vse of it doth moue\\nThe hearers to Collection; they ayme at it,\\nAnd botch the words vp fit to their owne thoughts,\\nWhich as her winkes, and nods, and gestures yeeld them,\\nIndeed would make one thinke there would be thought,\\nThough nothing sure, yet much vnhappily\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Qu. 'Twere good she were spoken with,\\nFor she may strew dangerous coniectures\\nIn ill breeding minds. Let her come in.\\nTo my sicke soule (as sinnes true Nature is)\\nEach toy seemes Prologue, to some great amisse,\\nSo full of Artlesse iealousie is guilt,\\nIt spill's it selfe, in fearing to be spilt.\\nEnter Ophelia distracted.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ophe. Where is the beauteous Maiesty of Denmark\\n\\n   Qu. How now Ophelia?\\n  Ophe. How should I your true loue know from another one?\\nBy his Cockle hat and staffe, and his Sandal shoone'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Qu. Alas sweet Lady: what imports this Song?\\n  Ophe. Say you? Nay pray you marke.\\nHe is dead and gone Lady, he is dead and gone,\\nAt his head a grasse-greene Turfe, at his heeles a stone.\\nEnter King.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Qu. Nay but Ophelia\\n\\n   Ophe. Pray you marke.\\nWhite his Shrow'd as the Mountaine Snow\\n\\n   Qu. Alas, looke heere my Lord\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ophe. Larded with sweet Flowers:\\nWhich bewept to the graue did not go,\\nWith true-loue showres'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"King. How do ye, pretty Lady?\\n  Ophe. Well, God dil'd you. They say the Owle was\\na Bakers daughter. Lord, wee know what we are, but\\nknow not what we may be. God be at your Table\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='King. Conceit vpon her Father'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ophe. Pray you let's haue no words of this: but when\\nthey aske you what it meanes, say you this:\\nTo morrow is S[aint]. Valentines day, all in the morning betime,\\nAnd I a Maid at your Window, to be your Valentine.\\nThen vp he rose, & don'd his clothes, & dupt the chamber dore,\\nLet in the Maid, that out a Maid, neuer departed more\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='King. Pretty Ophelia'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ophe. Indeed la? without an oath Ile make an end ont.\\nBy gis, and by S[aint]. Charity,\\nAlacke, and fie for shame:\\nYong men wil doo't, if they come too't,\\nBy Cocke they are too blame.\\nQuoth she before you tumbled me,\\nYou promis'd me to Wed:\\nSo would I ha done by yonder Sunne,\\nAnd thou hadst not come to my bed\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"King. How long hath she bin thus?\\n  Ophe. I hope all will be well. We must bee patient,\\nbut I cannot choose but weepe, to thinke they should\\nlay him i'th' cold ground: My brother shall knowe of it,\\nand so I thanke you for your good counsell. Come, my\\nCoach: Goodnight Ladies: Goodnight sweet Ladies:\\nGoodnight, goodnight.\\nEnter.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='King. Follow her close,\\nGiue her good watch I pray you:\\nOh this is the poyson of deepe greefe, it springs\\nAll from her Fathers death. Oh Gertrude, Gertrude,\\nWhen sorrowes comes, they come not single spies,\\nBut in Battalians. First, her Father slaine,\\nNext your Sonne gone, and he most violent Author\\nOf his owne iust remoue: the people muddied,\\nThicke and vnwholsome in their thoughts, and whispers\\nFor good Polonius death; and we haue done but greenly\\nIn hugger mugger to interre him. Poore Ophelia\\nDiuided from her selfe, and her faire Iudgement,\\nWithout the which we are Pictures, or meere Beasts.\\nLast, and as much containing as all these,\\nHer Brother is in secret come from France,\\nKeepes on his wonder, keepes himselfe in clouds,\\nAnd wants not Buzzers to infect his eare\\nWith pestilent Speeches of his Fathers death,\\nWhere in necessitie of matter Beggard,\\nWill nothing sticke our persons to Arraigne\\nIn eare and eare. O my deere Gertrude, this,\\nLike to a murdering Peece in many places,\\nGiues me superfluous death.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='A Noise within.\\n\\nEnter a Messenger.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Qu. Alacke, what noyse is this?\\n  King. Where are my Switzers?\\nLet them guard the doore. What is the matter?\\n  Mes. Saue your selfe, my Lord.\\nThe Ocean (ouer-peering of his List)\\nEates not the Flats with more impittious haste\\nThen young Laertes, in a Riotous head,\\nOre-beares your Officers, the rabble call him Lord,\\nAnd as the world were now but to begin,\\nAntiquity forgot, Custome not knowne,\\nThe Ratifiers and props of euery word,\\nThey cry choose we? Laertes shall be King,\\nCaps, hands, and tongues, applaud it to the clouds,\\nLaertes shall be King, Laertes King'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Qu. How cheerefully on the false Traile they cry,\\nOh this is Counter you false Danish Dogges.\\n\\nNoise within. Enter Laertes.\\n\\n  King. The doores are broke'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Laer. Where is the King, sirs? Stand you all without\\n\\n   All. No, let's come in\\n\\n   Laer. I pray you giue me leaue\\n\\n   Al. We will, we will\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Laer. I thanke you: Keepe the doore.\\nOh thou vilde King, giue me my Father\\n\\n   Qu. Calmely good Laertes'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Laer. That drop of blood, that calmes\\nProclaimes me Bastard:\\nCries Cuckold to my Father, brands the Harlot\\nEuen heere betweene the chaste vnsmirched brow\\nOf my true Mother'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"King. What is the cause Laertes,\\nThat thy Rebellion lookes so Gyant-like?\\nLet him go Gertrude: Do not feare our person:\\nThere's such Diuinity doth hedge a King,\\nThat Treason can but peepe to what it would,\\nActs little of his will. Tell me Laertes,\\nWhy thou art thus Incenst? Let him go Gertrude.\\nSpeake man\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Laer. Where's my Father?\\n  King. Dead\\n\\n   Qu. But not by him\\n\\n   King. Let him demand his fill\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Laer. How came he dead? Ile not be Iuggel'd with.\\nTo hell Allegeance: Vowes, to the blackest diuell.\\nConscience and Grace, to the profoundest Pit.\\nI dare Damnation: to this point I stand,\\nThat both the worlds I giue to negligence,\\nLet come what comes: onely Ile be reueng'd\\nMost throughly for my Father\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='King. Who shall stay you?\\n  Laer. My Will, not all the world,\\nAnd for my meanes, Ile husband them so well,\\nThey shall go farre with little'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='King. Good Laertes:\\nIf you desire to know the certaintie\\nOf your deere Fathers death, if writ in your reuenge,\\nThat Soop-stake you will draw both Friend and Foe,\\nWinner and Looser'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Laer. None but his Enemies\\n\\n   King. Will you know them then\\n\\n   La. To his good Friends, thus wide Ile ope my Armes:\\nAnd like the kinde Life-rend'ring Politician,\\nRepast them with my blood\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"King. Why now you speake\\nLike a good Childe, and a true Gentleman.\\nThat I am guiltlesse of your Fathers death,\\nAnd am most sensible in greefe for it,\\nIt shall as leuell to your Iudgement pierce\\nAs day do's to your eye.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='A noise within. Let her come in.\\n\\nEnter Ophelia.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Laer. How now? what noise is that?\\nOh heate drie vp my Braines, teares seuen times salt,\\nBurne out the Sence and Vertue of mine eye.\\nBy Heauen, thy madnesse shall be payed by waight,\\nTill our Scale turnes the beame. Oh Rose of May,\\nDeere Maid, kinde Sister, sweet Ophelia:\\nOh Heauens, is't possible, a yong Maids wits,\\nShould be as mortall as an old mans life?\\nNature is fine in Loue, and where 'tis fine,\\nIt sends some precious instance of it selfe\\nAfter the thing it loues\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ophe. They bore him bare fac'd on the Beer,\\nHey non nony, nony, hey nony:\\nAnd on his graue raines many a teare,\\nFare you well my Doue\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Laer. Had'st thou thy wits, and did'st perswade Reuenge,\\nit could not moue thus\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ophe. You must sing downe a-downe, and you call\\nhim a-downe-a. Oh, how the wheele becomes it? It is\\nthe false Steward that stole his masters daughter\\n\\n   Laer. This nothings more then matter'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ophe. There's Rosemary, that's for Remembraunce.\\nPray loue remember: and there is Paconcies, that's for\\nThoughts\\n\\n   Laer. A document in madnesse, thoughts & remembrance\\nfitted\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ophe. There's Fennell for you, and Columbines: ther's\\nRew for you, and heere's some for me. Wee may call it\\nHerbe-Grace a Sundaies: Oh you must weare your Rew\\nwith a difference. There's a Daysie, I would giue you\\nsome Violets, but they wither'd all when my Father dyed:\\nThey say, he made a good end;\\nFor bonny sweet Robin is all my ioy\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Laer. Thought, and Affliction, Passion, Hell it selfe:\\nShe turnes to Fauour, and to prettinesse'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ophe. And will he not come againe,\\nAnd will he not come againe:\\nNo, no, he is dead, go to thy Death-bed,\\nHe neuer wil come againe.\\nHis Beard as white as Snow,\\nAll Flaxen was his Pole:\\nHe is gone, he is gone, and we cast away mone,\\nGramercy on his Soule.\\nAnd of all Christian Soules, I pray God.\\nGod buy ye.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Exeunt. Ophelia'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Laer. Do you see this, you Gods?\\n  King. Laertes, I must common with your greefe,\\nOr you deny me right: go but apart,\\nMake choice of whom your wisest Friends you will,\\nAnd they shall heare and iudge 'twixt you and me;\\nIf by direct or by Colaterall hand\\nThey finde vs touch'd, we will our Kingdome giue,\\nOur Crowne, our Life, and all that we call Ours\\nTo you in satisfaction. But if not,\\nBe you content to lend your patience to vs,\\nAnd we shall ioyntly labour with your soule\\nTo giue it due content\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Laer. Let this be so:\\nHis meanes of death, his obscure buriall;\\nNo Trophee, Sword, nor Hatchment o're his bones,\\nNo Noble rite, nor formall ostentation,\\nCry to be heard, as 'twere from Heauen to Earth,\\nThat I must call in question\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"King. So you shall:\\nAnd where th' offence is, let the great Axe fall.\\nI pray you go with me.\\n\\nExeunt.\\n\\nEnter Horatio, with an Attendant.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Hora. What are they that would speake with me?\\n  Ser. Saylors sir, they say they haue Letters for you'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Hor. Let them come in,\\nI do not know from what part of the world\\nI should be greeted, if not from Lord Hamlet.\\nEnter Saylor.\\n\\n  Say. God blesse you Sir\\n\\n   Hor. Let him blesse thee too'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Say. Hee shall Sir, and't please him. There's a Letter\\nfor you Sir: It comes from th' Ambassadours that was\\nbound for England, if your name be Horatio, as I am let\\nto know it is.\\n\\nReads the Letter.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Horatio, When thou shalt haue ouerlook'd this, giue these\\nFellowes some meanes to the King: They haue Letters\\nfor him. Ere we were two dayes old at Sea, a Pyrate of very\\nWarlicke appointment gaue vs Chace. Finding our selues too\\nslow of Saile, we put on a compelled Valour. In the Grapple, I\\nboorded them: On the instant they got cleare of our Shippe, so\\nI alone became their Prisoner. They haue dealt with mee, like\\nTheeues of Mercy, but they knew what they did. I am to doe\\na good turne for them. Let the King haue the Letters I haue\\nsent, and repaire thou to me with as much hast as thou wouldest\\nflye death. I haue words to speake in your eare, will make thee\\ndumbe, yet are they much too light for the bore of the Matter.\\nThese good Fellowes will bring thee where I am. Rosincrance\\nand Guildensterne, hold their course for England. Of them\\nI haue much to tell thee, Farewell.\\nHe that thou knowest thine,\\nHamlet.\\nCome, I will giue you way for these your Letters,\\nAnd do't the speedier, that you may direct me\\nTo him from whom you brought them.\\nEnter.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Enter King and Laertes.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='King. Now must your conscience my acquittance seal,\\nAnd you must put me in your heart for Friend,\\nSith you haue heard, and with a knowing eare,\\nThat he which hath your Noble Father slaine,\\nPursued my life'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Laer. It well appeares. But tell me,\\nWhy you proceeded not against these feates,\\nSo crimefull, and so Capitall in Nature,\\nAs by your Safety, Wisedome, all things else,\\nYou mainly were stirr'd vp?\\n  King. O for two speciall Reasons,\\nWhich may to you (perhaps) seeme much vnsinnowed,\\nAnd yet to me they are strong. The Queen his Mother,\\nLiues almost by his lookes: and for my selfe,\\nMy Vertue or my Plague, be it either which,\\nShe's so coniunctiue to my life, and soule;\\nThat as the Starre moues not but in his Sphere,\\nI could not but by her. The other Motiue,\\nWhy to a publike count I might not go,\\nIs the great loue the generall gender beare him,\\nWho dipping all his Faults in their affection,\\nWould like the Spring that turneth Wood to Stone,\\nConuert his Gyues to Graces. So that my Arrowes\\nToo slightly timbred for so loud a Winde,\\nWould haue reuerted to my Bow againe,\\nAnd not where I had arm'd them\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Laer. And so haue I a Noble Father lost,\\nA Sister driuen into desperate tearmes,\\nWho was (if praises may go backe againe)\\nStood Challenger on mount of all the Age\\nFor her perfections. But my reuenge will come'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"King. Breake not your sleepes for that,\\nYou must not thinke\\nThat we are made of stuffe, so flat, and dull,\\nThat we can let our Beard be shooke with danger,\\nAnd thinke it pastime. You shortly shall heare more,\\nI lou'd your Father, and we loue our Selfe,\\nAnd that I hope will teach you to imagine-\\nEnter a Messenger.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='How now? What Newes?\\n  Mes. Letters my Lord from Hamlet, This to your\\nMaiesty: this to the Queene'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"King. From Hamlet? Who brought them?\\n  Mes. Saylors my Lord they say, I saw them not:\\nThey were giuen me by Claudio, he receiu'd them\\n\\n   King. Laertes you shall heare them:\\nLeaue vs.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Exit Messenger'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"High and Mighty, you shall know I am set naked on your\\nKingdome. To morrow shall I begge leaue to see your Kingly\\nEyes. When I shall (first asking your Pardon thereunto) recount\\nth' Occasions of my sodaine, and more strange returne.\\nHamlet.\\nWhat should this meane? Are all the rest come backe?\\nOr is it some abuse? Or no such thing?\\n  Laer. Know you the hand?\\n  Kin. 'Tis Hamlets Character, naked and in a Postscript\\nhere he sayes alone: Can you aduise me?\\n  Laer. I'm lost in it my Lord; but let him come,\\nIt warmes the very sicknesse in my heart,\\nThat I shall liue and tell him to his teeth;\\nThus diddest thou\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Kin. If it be so Laertes, as how should it be so:\\nHow otherwise will you be rul'd by me?\\n  Laer. If so you'l not o'rerule me to a peace\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Kin. To thine owne peace: if he be now return'd,\\nAs checking at his Voyage, and that he meanes\\nNo more to vndertake it; I will worke him\\nTo an exployt now ripe in my Deuice,\\nVnder the which he shall not choose but fall;\\nAnd for his death no winde of blame shall breath,\\nBut euen his Mother shall vncharge the practice,\\nAnd call it accident: Some two Monthes hence\\nHere was a Gentleman of Normandy,\\nI'ue seene my selfe, and seru'd against the French,\\nAnd they ran well on Horsebacke; but this Gallant\\nHad witchcraft in't; he grew into his Seat,\\nAnd to such wondrous doing brought his Horse,\\nAs had he beene encorps't and demy-Natur'd\\nWith the braue Beast, so farre he past my thought,\\nThat I in forgery of shapes and trickes,\\nCome short of what he did\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Laer. A Norman was't?\\n  Kin. A Norman\\n\\n   Laer. Vpon my life Lamound\\n\\n   Kin. The very same\\n\\n   Laer. I know him well, he is the Brooch indeed,\\nAnd Iemme of all our Nation\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Kin. Hee mad confession of you,\\nAnd gaue you such a Masterly report,\\nFor Art and exercise in your defence;\\nAnd for your Rapier most especiall,\\nThat he cryed out, t'would be a sight indeed,\\nIf one could match you Sir. This report of his\\nDid Hamlet so envenom with his Enuy,\\nThat he could nothing doe but wish and begge,\\nYour sodaine comming ore to play with him;\\nNow out of this\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Laer. Why out of this, my Lord?\\n  Kin. Laertes was your Father deare to you?\\nOr are you like the painting of a sorrow,\\nA face without a heart?\\n  Laer. Why aske you this?\\n  Kin. Not that I thinke you did not loue your Father,\\nBut that I know Loue is begun by Time:\\nAnd that I see in passages of proofe,\\nTime qualifies the sparke and fire of it:\\nHamlet comes backe: what would you vndertake,\\nTo show your selfe your Fathers sonne indeed,\\nMore then in words?\\n  Laer. To cut his throat i'th' Church\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Kin. No place indeed should murder Sancturize;\\nReuenge should haue no bounds: but good Laertes\\nWill you doe this, keepe close within your Chamber,\\nHamlet return'd, shall know you are come home:\\nWee'l put on those shall praise your excellence,\\nAnd set a double varnish on the fame\\nThe Frenchman gaue you, bring you in fine together,\\nAnd wager on your heads, he being remisse,\\nMost generous, and free from all contriuing,\\nWill not peruse the Foiles? So that with ease,\\nOr with a little shuffling, you may choose\\nA Sword vnbaited, and in a passe of practice,\\nRequit him for your Father\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Laer. I will doo't.\\nAnd for that purpose Ile annoint my Sword:\\nI bought an Vnction of a Mountebanke\\nSo mortall, I but dipt a knife in it,\\nWhere it drawes blood, no Cataplasme so rare,\\nCollected from all Simples that haue Vertue\\nVnder the Moone, can saue the thing from death,\\nThat is but scratcht withall: Ile touch my point,\\nWith this contagion, that if I gall him slightly,\\nIt may be death\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Kin. Let's further thinke of this,\\nWeigh what conuenience both of time and meanes\\nMay fit vs to our shape, if this should faile;\\nAnd that our drift looke through our bad performance,\\n'Twere better not assaid; therefore this Proiect\\nShould haue a backe or second, that might hold,\\nIf this should blast in proofe: Soft, let me see\\nWee'l make a solemne wager on your commings,\\nI ha't: when in your motion you are hot and dry,\\nAs make your bowts more violent to the end,\\nAnd that he cals for drinke; Ile haue prepar'd him\\nA Challice for the nonce; whereon but sipping,\\nIf he by chance escape your venom'd stuck,\\nOur purpose may hold there; how sweet Queene.\\nEnter Queene.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Queen. One woe doth tread vpon anothers heele,\\nSo fast they'l follow: your Sister's drown'd Laertes\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Laer. Drown'd! O where?\\n  Queen. There is a Willow growes aslant a Brooke,\\nThat shewes his hore leaues in the glassie streame:\\nThere with fantasticke Garlands did she come,\\nOf Crow-flowers, Nettles, Daysies, and long Purples,\\nThat liberall Shepheards giue a grosser name;\\nBut our cold Maids doe Dead Mens Fingers call them:\\nThere on the pendant boughes, her Coronet weeds\\nClambring to hang; an enuious sliuer broke,\\nWhen downe the weedy Trophies, and her selfe,\\nFell in the weeping Brooke, her cloathes spred wide,\\nAnd Mermaid-like, a while they bore her vp,\\nWhich time she chaunted snatches of old tunes,\\nAs one incapable of her owne distresse,\\nOr like a creature Natiue, and indued\\nVnto that Element: but long it could not be,\\nTill that her garments, heauy with her drinke,\\nPul'd the poore wretch from her melodious buy,\\nTo muddy death\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Laer. Alas then, is she drown'd?\\n  Queen. Drown'd, drown'd\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Laer. Too much of water hast thou poore Ophelia,\\nAnd therefore I forbid my teares: but yet\\nIt is our tricke, Nature her custome holds,\\nLet shame say what it will; when these are gone\\nThe woman will be out: Adue my Lord,\\nI haue a speech of fire, that faine would blaze,\\nBut that this folly doubts it.\\nEnter.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Kin. Let's follow, Gertrude:\\nHow much I had to doe to calme his rage?\\nNow feare I this will giue it start againe;\\nTherefore let's follow.\\n\\nExeunt.\\n\\nEnter two Clownes.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Clown. Is she to bee buried in Christian buriall, that\\nwilfully seekes her owne saluation?\\n  Other. I tell thee she is, and therefore make her Graue\\nstraight, the Crowner hath sate on her, and finds it Christian\\nburiall'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Clo. How can that be, vnlesse she drowned her selfe in\\nher owne defence?\\n  Other. Why 'tis found so\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Clo. It must be Se offendendo, it cannot bee else: for\\nheere lies the point; If I drowne my selfe wittingly, it argues\\nan Act: and an Act hath three branches. It is an\\nAct to doe and to performe; argall she drown'd her selfe\\nwittingly\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Other. Nay but heare you Goodman Deluer'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Clown. Giue me leaue; heere lies the water; good:\\nheere stands the man; good: If the man goe to this water\\nand drowne himselfe; it is will he nill he, he goes;\\nmarke you that? But if the water come to him & drowne\\nhim; hee drownes not himselfe. Argall, hee that is not\\nguilty of his owne death, shortens not his owne life'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Other. But is this law?\\n  Clo. I marry is't, Crowners Quest Law\\n\\n   Other. Will you ha the truth on't: if this had not\\nbeene a Gentlewoman, shee should haue beene buried\\nout of Christian Buriall\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Clo. Why there thou say'st. And the more pitty that\\ngreat folke should haue countenance in this world to\\ndrowne or hang themselues, more then their euen Christian.\\nCome, my Spade; there is no ancient Gentlemen,\\nbut Gardiners, Ditchers and Graue-makers; they hold vp\\nAdams Profession\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Other. Was he a Gentleman?\\n  Clo. He was the first that euer bore Armes\\n\\n   Other. Why he had none'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Clo. What, ar't a Heathen? how doth thou vnderstand\\nthe Scripture? the Scripture sayes Adam dig'd;\\ncould hee digge without Armes? Ile put another question\\nto thee; if thou answerest me not to the purpose, confesse\\nthy selfe-\\n  Other. Go too\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Clo. What is he that builds stronger then either the\\nMason, the Shipwright, or the Carpenter?\\n  Other. The Gallowes maker; for that Frame outliues a\\nthousand Tenants'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Clo. I like thy wit well in good faith, the Gallowes\\ndoes well; but how does it well? it does well to those\\nthat doe ill: now, thou dost ill to say the Gallowes is\\nbuilt stronger then the Church: Argall, the Gallowes\\nmay doe well to thee. Too't againe, Come\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Other. Who builds stronger then a Mason, a Shipwright,\\nor a Carpenter?\\n  Clo. I, tell me that, and vnyoake\\n\\n   Other. Marry, now I can tell\\n\\n   Clo. Too't\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Clo. Too't\\n\\n   Other. Masse, I cannot tell.\\nEnter Hamlet and Horatio a farre off.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Clo. Cudgell thy braines no more about it; for your\\ndull Asse will not mend his pace with beating; and when\\nyou are ask't this question next, say a Graue-maker: the\\nHouses that he makes, lasts till Doomesday: go, get thee\\nto Yaughan, fetch me a stoupe of Liquor.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Sings.\\n\\nIn youth when I did loue, did loue,\\nme thought it was very sweete:\\nTo contract O the time for a my behoue,\\nO me thought there was nothing meete'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Ha's this fellow no feeling of his businesse, that\\nhe sings at Graue-making?\\n  Hor. Custome hath made it in him a property of easinesse\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. 'Tis ee'n so; the hand of little Imployment hath\\nthe daintier sense\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Clowne sings. But Age with his stealing steps\\nhath caught me in his clutch:\\nAnd hath shipped me intill the Land,\\nas if I had neuer beene such'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. That Scull had a tongue in it, and could sing\\nonce: how the knaue iowles it to th' grownd, as if it\\nwere Caines Iaw-bone, that did the first murther: It\\nmight be the Pate of a Polititian which this Asse o're Offices:\\none that could circumuent God, might it not?\\n  Hor. It might, my Lord\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Or of a Courtier, which could say, Good Morrow\\nsweet Lord: how dost thou, good Lord? this\\nmight be my Lord such a one, that prais'd my Lord such\\na ones Horse, when he meant to begge it; might it not?\\n  Hor. I, my Lord\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Why ee'n so: and now my Lady Wormes,\\nChaplesse, and knockt about the Mazard with a Sextons\\nSpade; heere's fine Reuolution, if wee had the tricke to\\nsee't. Did these bones cost no more the breeding, but\\nto play at Loggets with 'em? mine ake to thinke\\non't\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Clowne sings. A Pickhaxe and a Spade, a Spade,\\nfor and a shrowding-Sheete:\\nO a Pit of Clay for to be made,\\nfor such a Guest is meete'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. There's another: why might not that bee the\\nScull of a Lawyer? where be his Quiddits now? his\\nQuillets? his Cases? his Tenures, and his Tricks? why\\ndoe's he suffer this rude knaue now to knocke him about\\nthe Sconce with a dirty Shouell, and will not tell him of\\nhis Action of Battery? hum. This fellow might be in's\\ntime a great buyer of Land, with his Statutes, his Recognizances,\\nhis Fines, his double Vouchers, his Recoueries:\\nIs this the fine of his Fines, and the recouery of his Recoueries,\\nto haue his fine Pate full of fine Dirt? will his\\nVouchers vouch him no more of his Purchases, and double\\nones too, then the length and breadth of a paire of\\nIndentures? the very Conueyances of his Lands will\\nhardly lye in this Boxe; and must the Inheritor himselfe\\nhaue no more? ha?\\n  Hor. Not a iot more, my Lord\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. Is not Parchment made of Sheep-skinnes?\\n  Hor. I my Lord, and of Calue-skinnes too'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. They are Sheepe and Calues that seek out assurance\\nin that. I will speake to this fellow: whose Graue's\\nthis Sir?\\n  Clo. Mine Sir:\\nO a Pit of Clay for to be made,\\nfor such a Guest is meete\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. I thinke it be thine indeed: for thou liest in't\\n\\n   Clo. You lye out on't Sir, and therefore it is not yours:\\nfor my part, I doe not lye in't; and yet it is mine\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Thou dost lye in't, to be in't and say 'tis thine:\\n'tis for the dead, not for the quicke, therefore thou\\nlyest\\n\\n   Clo. 'Tis a quicke lye Sir, 'twill away againe from me\\nto you\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. What man dost thou digge it for?\\n  Clo. For no man Sir\\n\\n   Ham. What woman then?\\n  Clo. For none neither'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Who is to be buried in't?\\n  Clo. One that was a woman Sir; but rest her Soule,\\nshee's dead\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. How absolute the knaue is? wee must speake\\nby the Carde, or equiuocation will vndoe vs: by the\\nLord Horatio, these three yeares I haue taken note of it,\\nthe Age is growne so picked, that the toe of the Pesant\\ncomes so neere the heeles of our Courtier, hee galls his\\nKibe. How long hast thou been a Graue-maker?\\n  Clo. Of all the dayes i'th' yeare, I came too't that day\\nthat our last King Hamlet o'recame Fortinbras\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. How long is that since?\\n  Clo. Cannot you tell that? euery foole can tell that:\\nIt was the very day, that young Hamlet was borne, hee\\nthat was mad, and sent into England'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. I marry, why was he sent into England?\\n  Clo. Why, because he was mad; hee shall recouer his\\nwits there; or if he do not, it's no great matter there\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Why?\\n  Clo. 'Twill not be seene in him, there the men are as\\nmad as he\\n\\n   Ham. How came he mad?\\n  Clo. Very strangely they say\\n\\n   Ham. How strangely?\\n  Clo. Faith e'ene with loosing his wits\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. Vpon what ground?\\n  Clo. Why heere in Denmarke: I haue bin sixeteene\\nheere, man and Boy thirty yeares'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. How long will a man lie i'th' earth ere he rot?\\n  Clo. Ifaith, if he be not rotten before he die (as we haue\\nmany pocky Coarses now adaies, that will scarce hold\\nthe laying in) he will last you some eight yeare, or nine\\nyeare. A Tanner will last you nine yeare\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Why he, more then another?\\n  Clo. Why sir, his hide is so tan'd with his Trade, that\\nhe will keepe out water a great while. And your water,\\nis a sore Decayer of your horson dead body. Heres a Scull\\nnow: this Scul, has laine in the earth three & twenty years\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. Whose was it?\\n  Clo. A whoreson mad Fellowes it was;\\nWhose doe you thinke it was?\\n  Ham. Nay, I know not'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Clo. A pestilence on him for a mad Rogue, a pour'd a\\nFlaggon of Renish on my head once. This same Scull\\nSir, this same Scull sir, was Yoricks Scull, the Kings Iester\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. This?\\n  Clo. E'ene that\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. Let me see. Alas poore Yorick, I knew him Horatio,\\na fellow of infinite Iest; of most excellent fancy, he\\nhath borne me on his backe a thousand times: And how\\nabhorred my Imagination is, my gorge rises at it. Heere\\nhung those lipps, that I haue kist I know not how oft.\\nWhere be your Iibes now? Your Gambals? Your\\nSongs? Your flashes of Merriment that were wont to\\nset the Table on a Rore? No one now to mock your own\\nIeering? Quite chopfalne? Now get you to my Ladies\\nChamber, and tell her, let her paint an inch thicke, to this\\nfauour she must come. Make her laugh at that: prythee\\nHoratio tell me one thing'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Hor. What's that my Lord?\\n  Ham. Dost thou thinke Alexander lookt o'this fashion\\ni'th' earth?\\n  Hor. E'ene so\\n\\n   Ham. And smelt so? Puh\\n\\n   Hor. E'ene so, my Lord\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. To what base vses we may returne Horatio.\\nWhy may not Imagination trace the Noble dust of Alexander,\\ntill he find it stopping a bunghole'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Hor. 'Twere to consider: to curiously to consider so\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. No faith, not a iot. But to follow him thether\\nwith modestie enough, & likeliehood to lead it; as thus.\\nAlexander died: Alexander was buried: Alexander returneth\\ninto dust; the dust is earth; of earth we make\\nLome, and why of that Lome (whereto he was conuerted)\\nmight they not stopp a Beere-barrell?\\nImperiall Caesar, dead and turn'd to clay,\\nMight stop a hole to keepe the winde away.\\nOh, that that earth, which kept the world in awe,\\nShould patch a Wall, t' expell the winters flaw.\\nBut soft, but soft, aside; heere comes the King.\\nEnter King, Queene, Laertes, and a Coffin, with Lords attendant.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"The Queene, the Courtiers. Who is that they follow,\\nAnd with such maimed rites? This doth betoken,\\nThe Coarse they follow, did with disperate hand,\\nFore do it owne life; 'twas some Estate.\\nCouch we a while, and mark\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Laer. What Cerimony else?\\n  Ham. That is Laertes, a very Noble youth: Marke'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Laer. What Cerimony else?\\n  Priest. Her Obsequies haue bin as farre inlarg'd.\\nAs we haue warrantie, her death was doubtfull,\\nAnd but that great Command, o're-swaies the order,\\nShe should in ground vnsanctified haue lodg'd,\\nTill the last Trumpet. For charitable praier,\\nShardes, Flints, and Peebles, should be throwne on her:\\nYet heere she is allowed her Virgin Rites,\\nHer Maiden strewments, and the bringing home\\nOf Bell and Buriall\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Laer. Must there no more be done ?\\n  Priest. No more be done:\\nWe should prophane the seruice of the dead,\\nTo sing sage Requiem, and such rest to her\\nAs to peace-parted Soules'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Laer. Lay her i'th' earth,\\nAnd from her faire and vnpolluted flesh,\\nMay Violets spring. I tell thee (churlish Priest)\\nA Ministring Angell shall my Sister be,\\nWhen thou liest howling?\\n  Ham. What, the faire Ophelia?\\n  Queene. Sweets, to the sweet farewell.\\nI hop'd thou should'st haue bin my Hamlets wife:\\nI thought thy Bride-bed to haue deckt (sweet Maid)\\nAnd not t'haue strew'd thy Graue\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Laer. Oh terrible woer,\\nFall ten times trebble, on that cursed head\\nWhose wicked deed, thy most Ingenious sence\\nDepriu'd thee of. Hold off the earth a while,\\nTill I haue caught her once more in mine armes:\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Leaps in the graue.\\n\\nNow pile your dust, vpon the quicke, and dead,\\nTill of this flat a Mountaine you haue made,\\nTo o're top old Pelion, or the skyish head\\nOf blew Olympus\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. What is he, whose griefes\\nBeares such an Emphasis? whose phrase of Sorrow\\nConiure the wandring Starres, and makes them stand\\nLike wonder-wounded hearers? This is I,\\nHamlet the Dane'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Laer. The deuill take thy soule'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Thou prai'st not well,\\nI prythee take thy fingers from my throat;\\nSir though I am not Spleenatiue, and rash,\\nYet haue I something in me dangerous,\\nWhich let thy wisenesse feare. Away thy hand\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='King. Pluck them asunder\\n\\n   Qu. Hamlet, Hamlet\\n\\n   Gen. Good my Lord be quiet\\n\\n   Ham. Why I will fight with him vppon this Theme.\\nVntill my eielids will no longer wag'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Qu. Oh my Sonne, what Theame?\\n  Ham. I lou'd Ophelia; fortie thousand Brothers\\nCould not (with all there quantitie of Loue)\\nMake vp my summe. What wilt thou do for her?\\n  King. Oh he is mad Laertes,\\n  Qu. For loue of God forbeare him\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Come show me what thou'lt doe.\\nWoo't weepe? Woo't fight? Woo't teare thy selfe?\\nWoo't drinke vp Esile, eate a Crocodile?\\nIle doo't. Dost thou come heere to whine;\\nTo outface me with leaping in her Graue?\\nBe buried quicke with her, and so will I.\\nAnd if thou prate of Mountaines; let them throw\\nMillions of Akers on vs; till our ground\\nSindging his pate against the burning Zone,\\nMake Ossa like a wart. Nay, and thou'lt mouth,\\nIle rant as well as thou\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Kin. This is meere Madnesse:\\nAnd thus awhile the fit will worke on him:\\nAnon as patient as the female Doue,\\nWhen that her Golden Cuplet are disclos'd;\\nHis silence will sit drooping\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Heare you Sir:\\nWhat is the reason that you vse me thus?\\nI lou'd you euer; but it is no matter:\\nLet Hercules himselfe doe what he may,\\nThe Cat will Mew, and Dogge will haue his day.\\nEnter.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Kin. I pray you good Horatio wait vpon him,\\nStrengthen your patience in our last nights speech,\\nWee'l put the matter to the present push:\\nGood Gertrude set some watch ouer your Sonne,\\nThis Graue shall haue a liuing Monument:\\nAn houre of quiet shortly shall we see;\\nTill then, in patience our proceeding be.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Exeunt.\\n\\nEnter Hamlet and Horatio\\n\\n   Ham. So much for this Sir; now let me see the other,\\nYou doe remember all the Circumstance'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Hor. Remember it my Lord?\\n  Ham. Sir, in my heart there was a kinde of fighting,\\nThat would not let me sleepe; me thought I lay\\nWorse then the mutines in the Bilboes, rashly,\\n(And praise be rashnesse for it) let vs know,\\nOur indiscretion sometimes serues vs well,\\nWhen our deare plots do paule, and that should teach vs,\\nThere's a Diuinity that shapes our ends,\\nRough-hew them how we will\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Hor. That is most certaine'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Vp from my Cabin\\nMy sea-gowne scarft about me in the darke,\\nGrop'd I to finde out them; had my desire,\\nFinger'd their Packet, and in fine, withdrew\\nTo mine owne roome againe, making so bold,\\n(My feares forgetting manners) to vnseale\\nTheir grand Commission, where I found Horatio,\\nOh royall knauery: An exact command,\\nLarded with many seuerall sorts of reason;\\nImporting Denmarks health, and Englands too,\\nWith hoo, such Bugges and Goblins in my life,\\nThat on the superuize no leasure bated,\\nNo not to stay the grinding of the Axe,\\nMy head should be struck off\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Hor. Ist possible?\\n  Ham. Here's the Commission, read it at more leysure:\\nBut wilt thou heare me how I did proceed?\\n  Hor. I beseech you\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Being thus benetted round with Villaines,\\nEre I could make a Prologue to my braines,\\nThey had begun the Play. I sate me downe,\\nDeuis'd a new Commission, wrote it faire,\\nI once did hold it as our Statists doe,\\nA basenesse to write faire; and laboured much\\nHow to forget that learning: but Sir now,\\nIt did me Yeomans seriuce: wilt thou know\\nThe effects of what I wrote?\\n  Hor. I, good my Lord\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. An earnest Coniuration from the King,\\nAs England was his faithfull Tributary,\\nAs loue betweene them, as the Palme should flourish,\\nAs Peace should still her wheaten Garland weare,\\nAnd stand a Comma 'tweene their amities,\\nAnd many such like Assis of great charge,\\nThat on the view and know of these Contents,\\nWithout debatement further, more or lesse,\\nHe should the bearers put to sodaine death,\\nNot shriuing time allowed\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Hor. How was this seal'd?\\n  Ham. Why, euen in that was Heauen ordinate;\\nI had my fathers Signet in my Purse,\\nWhich was the Modell of that Danish Seale:\\nFolded the Writ vp in forme of the other,\\nSubscrib'd it, gau't th' impression, plac't it safely,\\nThe changeling neuer knowne: Now, the next day\\nWas our Sea Fight, and what to this was sement,\\nThou know'st already\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Hor. So Guildensterne and Rosincrance, go too't\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Why man, they did make loue to this imployment\\nThey are not neere my Conscience; their debate\\nDoth by their owne insinuation grow:\\n'Tis dangerous, when the baser nature comes\\nBetweene the passe, and fell incensed points\\nOf mighty opposites\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Hor. Why, what a King is this?\\n  Ham. Does it not, thinkst thee, stand me now vpon\\nHe that hath kil'd my King, and whor'd my Mother,\\nPopt in betweene th' election and my hopes,\\nThrowne out his Angle for my proper life,\\nAnd with such coozenage; is't not perfect conscience,\\nTo quit him with this arme? And is't not to be damn'd\\nTo let this Canker of our nature come\\nIn further euill\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Hor. It must be shortly knowne to him from England\\nWhat is the issue of the businesse there'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. It will be short,\\nThe interim's mine, and a mans life's no more\\nThen to say one: but I am very sorry good Horatio,\\nThat to Laertes I forgot my selfe;\\nFor by the image of my Cause, I see\\nThe Portraiture of his; Ile count his fauours:\\nBut sure the brauery of his griefe did put me\\nInto a Towring passion\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Hor. Peace, who comes heere?\\nEnter young Osricke.\\n\\n  Osr. Your Lordship is right welcome back to Denmarke\\n\\n   Ham. I humbly thank you Sir, dost know this waterflie?\\n  Hor. No my good Lord'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Thy state is the more gracious; for 'tis a vice to\\nknow him: he hath much Land, and fertile; let a Beast\\nbe Lord of Beasts, and his Crib shall stand at the Kings\\nMesse; 'tis a Chowgh; but as I saw spacious in the possession\\nof dirt\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Osr. Sweet Lord, if your friendship were at leysure,\\nI should impart a thing to you from his Maiesty'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. I will receiue it with all diligence of spirit; put\\nyour Bonet to his right vse, 'tis for the head\\n\\n   Osr. I thanke your Lordship, 'tis very hot\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. No, beleeue mee 'tis very cold, the winde is\\nNortherly\\n\\n   Osr. It is indifferent cold my Lord indeed\\n\\n   Ham. Mee thinkes it is very soultry, and hot for my\\nComplexion\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Osr. Exceedingly, my Lord, it is very soultry, as 'twere\\nI cannot tell how: but my Lord, his Maiesty bad me signifie\\nto you, that he ha's laid a great wager on your head:\\nSir, this is the matter\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. I beseech you remember\\n\\n   Osr. Nay, in good faith, for mine ease in good faith:\\nSir, you are not ignorant of what excellence Laertes is at\\nhis weapon'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. What's his weapon?\\n  Osr. Rapier and dagger\\n\\n   Ham. That's two of his weapons; but well\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Osr. The sir King ha's wag'd with him six Barbary horses,\\nagainst the which he impon'd as I take it, sixe French\\nRapiers and Poniards, with their assignes, as Girdle,\\nHangers or so: three of the Carriages infaith are very\\ndeare to fancy, very responsiue to the hilts, most delicate\\ncarriages, and of very liberall conceit\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. What call you the Carriages?\\n  Osr. The Carriages Sir, are the hangers'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. The phrase would bee more Germaine to the\\nmatter: If we could carry Cannon by our sides; I would\\nit might be Hangers till then; but on sixe Barbary Horses\\nagainst sixe French Swords: their Assignes, and three\\nliberall conceited Carriages, that's the French but against\\nthe Danish; why is this impon'd as you call it?\\n  Osr. The King Sir, hath laid that in a dozen passes betweene\\nyou and him, hee shall not exceed you three hits;\\nHe hath one twelue for mine, and that would come to\\nimediate tryall, if your Lordship would vouchsafe the\\nAnswere\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. How if I answere no?\\n  Osr. I meane my Lord, the opposition of your person\\nin tryall'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Sir, I will walke heere in the Hall; if it please\\nhis Maiestie, 'tis the breathing time of day with me; let\\nthe Foyles bee brought, the Gentleman willing, and the\\nKing hold his purpose; I will win for him if I can: if\\nnot, Ile gaine nothing but my shame, and the odde hits\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Osr. Shall I redeliuer you ee'n so?\\n  Ham. To this effect Sir, after what flourish your nature\\nwill\\n\\n   Osr. I commend my duty to your Lordship\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Yours, yours; hee does well to commend it\\nhimselfe, there are no tongues else for's tongue\\n\\n   Hor. This Lapwing runs away with the shell on his\\nhead\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. He did Complie with his Dugge before hee\\nsuck't it: thus had he and mine more of the same Beauty\\nthat I know the drossie age dotes on; only got the tune of\\nthe time, and outward habite of encounter, a kinde of\\nyesty collection, which carries them through & through\\nthe most fond and winnowed opinions; and doe but blow\\nthem to their tryalls: the Bubbles are out\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Hor. You will lose this wager, my Lord'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. I doe not thinke so, since he went into France,\\nI haue beene in continuall practice; I shall winne at the\\noddes: but thou wouldest not thinke how all heere about\\nmy heart: but it is no matter'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Hor. Nay, good my Lord\\n\\n   Ham. It is but foolery; but it is such a kinde of\\ngain-giuing as would perhaps trouble a woman'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Hor. If your minde dislike any thing, obey. I will forestall\\ntheir repaire hither, and say you are not fit'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Not a whit, we defie Augury; there's a speciall\\nProuidence in the fall of a sparrow. If it be now, 'tis not\\nto come: if it bee not to come, it will bee now: if it\\nbe not now; yet it will come; the readinesse is all, since no\\nman ha's ought of what he leaues. What is't to leaue betimes?\\nEnter King, Queene, Laertes and Lords, with other Attendants with\\nFoyles,\\nand Gauntlets, a Table and Flagons of Wine on it.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Kin. Come Hamlet, come, and take this hand from me'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Giue me your pardon Sir, I'ue done you wrong,\\nBut pardon't as you are a Gentleman.\\nThis presence knowes,\\nAnd you must needs haue heard how I am punisht\\nWith sore distraction? What I haue done\\nThat might your nature honour, and exception\\nRoughly awake, I heere proclaime was madnesse:\\nWas't Hamlet wrong'd Laertes? Neuer Hamlet.\\nIf Hamlet from himselfe be tane away:\\nAnd when he's not himselfe, do's wrong Laertes,\\nThen Hamlet does it not, Hamlet denies it:\\nWho does it then? His Madnesse? If't be so,\\nHamlet is of the Faction that is wrong'd,\\nHis madnesse is poore Hamlets Enemy.\\nSir, in this Audience,\\nLet my disclaiming from a purpos'd euill,\\nFree me so farre in your most generous thoughts,\\nThat I haue shot mine Arrow o're the house,\\nAnd hurt my Mother\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Laer. I am satisfied in Nature,\\nWhose motiue in this case should stirre me most\\nTo my Reuenge. But in my termes of Honor\\nI stand aloofe, and will no reconcilement,\\nTill by some elder Masters of knowne Honor,\\nI haue a voyce, and president of peace\\nTo keepe my name vngorg'd. But till that time,\\nI do receiue your offer'd loue like loue,\\nAnd wil not wrong it\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. I do embrace it freely,\\nAnd will this Brothers wager frankely play.\\nGiue vs the Foyles: Come on\\n\\n   Laer. Come one for me'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Ile be your foile Laertes, in mine ignorance,\\nYour Skill shall like a Starre i'th' darkest night,\\nSticke fiery off indeede\\n\\n   Laer. You mocke me Sir\\n\\n   Ham. No by this hand\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"King. Giue them the Foyles yong Osricke,\\nCousen Hamlet, you know the wager\\n\\n   Ham. Verie well my Lord,\\nYour Grace hath laide the oddes a'th' weaker side\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"King. I do not feare it,\\nI haue seene you both:\\nBut since he is better'd, we haue therefore oddes\\n\\n   Laer. This is too heauy,\\nLet me see another\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. This likes me well,\\nThese Foyles haue all a length.\\n\\nPrepare to play.\\n\\n  Osricke. I my good Lord'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='King. Set me the Stopes of wine vpon that Table:\\nIf Hamlet giue the first, or second hit,\\nOr quit in answer of the third exchange,\\nLet all the Battlements their Ordinance fire,\\nThe King shal drinke to Hamlets better breath,\\nAnd in the Cup an vnion shal he throw\\nRicher then that, which foure successiue Kings\\nIn Denmarkes Crowne haue worne.\\nGiue me the Cups,\\nAnd let the Kettle to the Trumpets speake,\\nThe Trumpet to the Cannoneer without,\\nThe Cannons to the Heauens, the Heauen to Earth,\\nNow the King drinkes to Hamlet. Come, begin,\\nAnd you the Iudges beare a wary eye'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. Come on sir\\n\\n   Laer. Come on sir.\\n\\nThey play.\\n\\n  Ham. One\\n\\n   Laer. No\\n\\n   Ham. Iudgement\\n\\n   Osr. A hit, a very palpable hit\\n\\n   Laer. Well: againe'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"King. Stay, giue me drinke.\\nHamlet, this Pearle is thine,\\nHere's to thy health. Giue him the cup,\\n\\nTrumpets sound, and shot goes off.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. Ile play this bout first, set by a-while.\\nCome: Another hit; what say you?\\n  Laer. A touch, a touch, I do confesse\\n\\n   King. Our Sonne shall win'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Qu. He's fat, and scant of breath.\\nHeere's a Napkin, rub thy browes,\\nThe Queene Carowses to thy fortune, Hamlet\\n\\n   Ham. Good Madam\\n\\n   King. Gertrude, do not drinke\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Qu. I will my Lord;\\nI pray you pardon me\\n\\n   King. It is the poyson'd Cup, it is too late\\n\\n   Ham. I dare not drinke yet Madam,\\nBy and by\\n\\n   Qu. Come, let me wipe thy face\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Laer. My Lord, Ile hit him now\\n\\n   King. I do not thinke't\\n\\n   Laer. And yet 'tis almost 'gainst my conscience\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Come for the third.\\nLaertes, you but dally,\\nI pray you passe with your best violence,\\nI am affear'd you make a wanton of me\\n\\n   Laer. Say you so? Come on.\\n\\nPlay.\\n\\n  Osr. Nothing neither way\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Laer. Haue at you now.\\n\\nIn scuffling they change Rapiers.\\n\\n  King. Part them, they are incens'd\\n\\n   Ham. Nay come, againe\\n\\n   Osr. Looke to the Queene there hoa\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Hor. They bleed on both sides. How is't my Lord?\\n  Osr. How is't Laertes?\\n  Laer. Why as a Woodcocke\\nTo mine Sprindge, Osricke,\\nI am iustly kill'd with mine owne Treacherie\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. How does the Queene?\\n  King. She sounds to see them bleede\\n\\n   Qu. No, no, the drinke, the drinke.\\nOh my deere Hamlet, the drinke, the drinke,\\nI am poyson'd\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Oh Villany! How? Let the doore be lock'd.\\nTreacherie, seeke it out\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Laer. It is heere Hamlet.\\nHamlet, thou art slaine,\\nNo Medicine in the world can do thee good.\\nIn thee, there is not halfe an houre of life;\\nThe Treacherous Instrument is in thy hand,\\nVnbated and envenom'd: the foule practise\\nHath turn'd it selfe on me. Loe, heere I lye,\\nNeuer to rise againe: Thy Mothers poyson'd:\\nI can no more, the King, the King's too blame\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. The point envenom'd too,\\nThen venome to thy worke.\\n\\nHurts the King.\\n\\n  All. Treason, Treason\\n\\n   King. O yet defend me Friends, I am but hurt\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Ham. Heere thou incestuous, murdrous,\\nDamned Dane,\\nDrinke off this Potion: Is thy Vnion heere?\\nFollow my Mother.\\n\\nKing Dyes.'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"King Dyes.\\n\\n  Laer. He is iustly seru'd.\\nIt is a poyson temp'red by himselfe:\\nExchange forgiuenesse with me, Noble Hamlet;\\nMine and my Fathers death come not vpon thee,\\nNor thine on me.\\n\\nDyes.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. Heauen make thee free of it, I follow thee.\\nI am dead Horatio, wretched Queene adiew,\\nYou that looke pale, and tremble at this chance,\\nThat are but Mutes or audience to this acte:\\nHad I but time (as this fell Sergeant death\\nIs strick'd in his Arrest) oh I could tell you.\\nBut let it be: Horatio, I am dead,\\nThou liu'st, report me and my causes right\\nTo the vnsatisfied\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Hor. Neuer beleeue it.\\nI am more an Antike Roman then a Dane:\\nHeere's yet some Liquor left\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. As th'art a man, giue me the Cup.\\nLet go, by Heauen Ile haue't.\\nOh good Horatio, what a wounded name,\\n(Things standing thus vnknowne) shall liue behind me.\\nIf thou did'st euer hold me in thy heart,\\nAbsent thee from felicitie awhile,\\nAnd in this harsh world draw thy breath in paine,\\nTo tell my Storie.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"March afarre off, and shout within.\\n\\nWhat warlike noyse is this?\\nEnter Osricke.\\n\\n  Osr. Yong Fortinbras, with conquest come fro[m] Poland\\nTo th' Ambassadors of England giues this warlike volly\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Ham. O I dye Horatio:\\nThe potent poyson quite ore-crowes my spirit,\\nI cannot liue to heare the Newes from England,\\nBut I do prophesie th' election lights\\nOn Fortinbras, he ha's my dying voyce,\\nSo tell him with the occurrents more and lesse,\\nWhich haue solicited. The rest is silence. O, o, o, o.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Dyes'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Hora. Now cracke a Noble heart:\\nGoodnight sweet Prince,\\nAnd flights of Angels sing thee to thy rest,\\nWhy do's the Drumme come hither?\\nEnter Fortinbras and English Ambassador, with Drumme, Colours,\\nand\\nAttendants.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Fortin. Where is this sight?\\n  Hor. What is it ye would see;\\nIf ought of woe, or wonder, cease your search'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='For. His quarry cries on hauocke. Oh proud death,\\nWhat feast is toward in thine eternall Cell.\\nThat thou so many Princes, at a shoote,\\nSo bloodily hast strooke'),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"Amb. The sight is dismall,\\nAnd our affaires from England come too late,\\nThe eares are senselesse that should giue vs hearing,\\nTo tell him his command'ment is fulfill'd,\\nThat Rosincrance and Guildensterne are dead:\\nWhere should we haue our thankes?\\n  Hor. Not from his mouth,\\nHad it th' abilitie of life to thanke you:\\nHe neuer gaue command'ment for their death.\\nBut since so iumpe vpon this bloodie question,\\nYou from the Polake warres, and you from England\\nAre heere arriued. Giue order that these bodies\\nHigh on a stage be placed to the view,\\nAnd let me speake to th' yet vnknowing world,\\nHow these things came about. So shall you heare\\nOf carnall, bloudie, and vnnaturall acts,\\nOf accidentall iudgements, casuall slaughters\\nOf death's put on by cunning, and forc'd cause,\\nAnd in this vpshot, purposes mistooke,\\nFalne on the Inuentors head. All this can I\\nTruly deliuer\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"For. Let vs hast to heare it,\\nAnd call the Noblest to the Audience.\\nFor me, with sorrow, I embrace my Fortune,\\nI haue some Rites of memory in this Kingdome,\\nWhich are to claime, my vantage doth\\nInuite me,\\n  Hor. Of that I shall haue alwayes cause to speake,\\nAnd from his mouth\\nWhose voyce will draw on more:\\nBut let this same be presently perform'd,\\nEuen whiles mens mindes are wilde,\\nLest more mischance\\nOn plots, and errors happen\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content=\"For. Let foure Captaines\\nBeare Hamlet like a Soldier to the Stage,\\nFor he was likely, had he beene put on\\nTo haue prou'd most royally:\\nAnd for his passage,\\nThe Souldiours Musicke, and the rites of Warre\\nSpeake lowdly for him.\\nTake vp the body; Such a sight as this\\nBecomes the Field, but heere shewes much amis.\\nGo, bid the Souldiers shoote.\"),\n",
       " Document(metadata={'source': 'D:\\\\btch 16\\\\NPl\\\\hamlet.txt'}, page_content='Exeunt. Marching: after the which, a Peale of Ordenance are shot\\noff.\\n\\n\\nFINIS. The tragedie of HAMLET, Prince of Denmarke.')]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "text_split=CharacterTextSplitter(separator=\"\\n\\n\",\n",
    "                                 chunk_size=200,\n",
    "                                 chunk_overlap=20)\n",
    "text_doc=text_split.split_documents(fin_t)\n",
    "text_doc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb44b74",
   "metadata": {},
   "source": [
    "# HTML tages splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "eb674f40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Main Topic': 'Heading 1 (h1)'}, page_content='Heading 1 (h1)'),\n",
       " Document(metadata={'Main Topic': 'Heading 1 (h1)', 'SubTopic': 'Heading 2 (h2)'}, page_content='Heading 2 (h2)'),\n",
       " Document(metadata={'Main Topic': 'Heading 1 (h1)', 'SubTopic': 'Heading 2 (h2)'}, page_content='Heading 3 (h3)  \\nHeading 4 (h4)  \\nHeading 5 (h5)  \\nHeading 6 (h6)')]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import HTMLHeaderTextSplitter\n",
    "\n",
    "tag=\"\"\"<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" \n",
    "          content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>HTML</title>\n",
    "</head>\n",
    "<body>\n",
    "      <h1>Heading 1 (h1)</h1>\n",
    "      <h2>Heading 2 (h2)</h2>\n",
    "      <h3>Heading 3 (h3)</h3>\n",
    "      <h4>Heading 4 (h4)</h4>\n",
    "      <h5>Heading 5 (h5)</h5>\n",
    "      <h6>Heading 6 (h6)</h6>\n",
    "</body>\n",
    "</html>\"\"\"\n",
    "header_to_split_on=[\n",
    "    (\"h1\",\"Main Topic\"),\n",
    "    (\"h2\",'SubTopic')\n",
    "]\n",
    "\n",
    "Splliter=HTMLHeaderTextSplitter(headers_to_split_on=header_to_split_on)\n",
    "document=Splliter.split_text(tag)\n",
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "150fe25b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='End container NOTE: Script required for drop-down button to work (mirrors).  \\nEnd header wrapper End content End footer  \\nEnd header  \\nEnd navigation End search  \\nStanford Encyclopedia of Philosophy  \\nMenu  \\nBrowse  \\nTable of Contents  \\nWhat\\'s New  \\nRandom Entry  \\nChronological  \\nArchives  \\nAbout  \\nEditorial Information  \\nAbout the SEP  \\nEditorial Board  \\nHow to Cite the SEP  \\nSpecial Characters  \\nAdvanced Tools  \\nContact  \\nSupport SEP  \\nSupport the SEP  \\nPDFs for SEP Friends  \\nMake a Donation  \\nSEPIA for Libraries  \\nBegin article sidebar End article sidebar NOTE: Article content must have two wrapper divs: id=\"article\" and id=\"article-content\" End article NOTE: article banner is outside of the id=\"article\" div. End article-banner  \\nEntry Navigation  \\nEntry Contents  \\nBibliography  \\nAcademic Tools  \\nFriends PDF Preview  \\nAuthor and Citation Info  \\nBack to Top  \\nEnd article-content  \\nBEGIN ARTICLE HTML #aueditable DO NOT MODIFY THIS LINE AND BELOW END ARTICLE HTML  \\nDO NOT MODIFY THIS LINE AND ABOVE'),\n",
       " Document(metadata={'Main Topic': 'Plato'}, page_content='Plato'),\n",
       " Document(metadata={'Main Topic': 'Plato'}, page_content='First published Sat Mar 20, 2004; substantive revision Sat Feb 12, 2022  \\nPlato (429?–347 B.C.E.) is, by any reckoning, one of the most\\ndazzling writers in the Western literary tradition and one of the most\\npenetrating, wide-ranging, and influential authors in the history of\\nphilosophy. An Athenian citizen of high status, he displays in his\\nworks his absorption in the political events and intellectual\\nmovements of his time, but the questions he raises are so profound and\\nthe strategies he uses for tackling them so richly suggestive and\\nprovocative that educated readers of nearly every period have in some\\nway been influenced by him, and in practically every age there have\\nbeen philosophers who count themselves Platonists in some important\\nrespects. He was not the first thinker or writer to whom the word\\n“philosopher” should be applied. But he was so\\nself-conscious about how philosophy should be conceived, and what its\\nscope and ambitions properly are, and he so transformed the\\nintellectual currents with which he grappled, that the subject of\\nphilosophy, as it is often conceived—a rigorous and systematic\\nexamination of ethical, political, metaphysical, and epistemological\\nissues, armed with a distinctive method—can be called his\\ninvention. Few other authors in the history of Western philosophy\\napproximate him in depth and range: perhaps only Aristotle (who\\nstudied with him), Aquinas, and Kant would be generally agreed to be\\nof the same rank.  \\nEntry Contents Entry Contents  \\n1. Plato’s central doctrines  \\n2. Plato’s puzzles  \\n3. Dialogue, setting, character  \\n4. Socrates  \\n5. Plato’s indirectness  \\n6. Can we know Plato’s mind?  \\n7. Socrates as the dominant speaker  \\n8. Links between the dialogues  \\n9. Does Plato change his mind about forms?  \\n10. Does Plato change his mind about politics?  \\n11. The historical Socrates: early, middle, and late dialogues  \\n12. Why dialogues?  \\nBibliography  \\nPrimary Literature  \\nSecondary Literature  \\nAcademic Tools  \\nOther Internet Resources  \\nRelated Entries  \\n1. Plato’s central doctrines  \\nMany people associate Plato with a few central doctrines that are\\nadvocated in his writings: The world that appears to our senses is in\\nsome way defective and filled with error, but there is a more real and\\nperfect realm, populated by entities (called “forms” or\\n“ideas”) that are eternal, changeless, and in some sense\\nparadigmatic for the structure and character of the world presented to\\nour senses. Among the most important of these abstract objects (as\\nthey are now called, because they are not located in space or time)\\nare goodness, beauty, equality, bigness, likeness, unity, being,\\nsameness, difference, change, and changelessness. (These\\nterms—“goodness”, “beauty”, and so\\non—are often capitalized by those who write about Plato, in\\norder to call attention to their exalted status; similarly for\\n“Forms” and “Ideas.”) The most fundamental\\ndistinction in Plato’s philosophy is between the many observable\\nobjects that appear beautiful (good, just, unified, equal, big) and\\nthe one object that is what beauty (goodness, justice, unity) really\\nis, from which those many beautiful (good, just, unified, equal, big)\\nthings receive their names and their corresponding characteristics.\\nNearly every major work of Plato is, in some way, devoted to or\\ndependent on this distinction. Many of them explore the ethical and\\npractical consequences of conceiving of reality in this bifurcated\\nway. We are urged to transform our values by taking to heart the\\ngreater reality of the forms and the defectiveness of the corporeal\\nworld. We must recognize that the soul is a different sort of object\\nfrom the body—so much so that it does not depend on the\\nexistence of the body for its functioning, and can in fact grasp the\\nnature of the forms far more easily when it is not encumbered by its\\nattachment to anything corporeal. In a few of Plato’s works, we\\nare told that the soul always retains the ability to recollect what it\\nonce grasped of the forms, when it was disembodied prior to its\\npossessor’s birth (see especially ), and that the\\nlives we lead are to some extent a punishment or reward for choices we\\nmade in a previous existence (see especially the final pages of ). But in many of Plato’s writings, it is\\nasserted or assumed that true philosophers—those who recognize\\nhow important it is to distinguish the one (the one thing that\\ngoodness is, or virtue is, or courage is) from the many (the many\\nthings that are called good or virtuous or courageous )—are in a\\nposition to become ethically superior to unenlightened human beings,\\nbecause of the greater degree of insight they can acquire. To\\nunderstand which things are good and why they are good (and if we are\\nnot interested in such questions, how can we become good?), we must\\ninvestigate the form of good.  \\nMeno  \\nRepublic  \\n2. Plato’s puzzles  \\nAlthough these propositions are often identified by Plato’s\\nreaders as forming a large part of the core of his philosophy, many of\\nhis greatest admirers and most careful students point out that few, if\\nany, of his writings can accurately be described as mere advocacy of a\\ncut-and-dried group of propositions. Often Plato’s works exhibit\\na certain degree of dissatisfaction and puzzlement with even those\\ndoctrines that are being recommended for our consideration. For\\nexample, the forms are sometimes described as hypotheses (see for\\nexample ). The form of good in particular is described\\nas something of a mystery whose real nature is elusive and as yet\\nunknown to anyone at all ( ). Puzzles are\\nraised—and not overtly answered—about how of\\nthe forms can be known and how we are to talk about them without\\nfalling into contradiction ( ), or about what it is\\nto know anything ( ) or to name anything\\n( ). When one compares Plato with some of the other\\nphilosophers who are often ranked with him—Aristotle, Aquinas,\\nand Kant, for example—he can be recognized to be far more\\nexploratory, incompletely systematic, elusive, and playful than they.\\nThat, along with his gifts as a writer and as a creator of vivid\\ncharacter and dramatic setting, is one of the reasons why he is often\\nthought to be the ideal author from whom one should receive\\none’s introduction to philosophy. His readers are not presented\\nwith an elaborate system of doctrines held to be so fully worked out\\nthat they are in no need of further exploration or development;\\ninstead, what we often receive from Plato is a few key ideas together\\nwith a series of suggestions and problems about how those ideas are to\\nbe interrogated and deployed. Readers of a Platonic dialogue are drawn\\ninto thinking for themselves about the issues raised, if they are to\\nlearn what the dialogue itself might be thought to say about them.\\nMany of his works therefore give their readers a strong sense of\\nphilosophy as a living and unfinished subject (perhaps one that can\\nnever be completed) to which they themselves will have to contribute.\\nAll of Plato’s works are in some way meant to leave further work\\nfor their readers, but among the ones that most conspicuously fall\\ninto this category are: , , , , , and .  \\nPhaedo  \\nRepublic  \\nany  \\nParmenides  \\nTheaetetus  \\nCratylus  \\nEuthyphro  \\nLaches  \\nCharmides  \\nEuthydemus  \\nTheaetetus  \\nParmenides  \\n3. Dialogue, setting, character  \\nThere is another feature of Plato’s writings that makes him\\ndistinctive among the great philosophers and colors our experience of\\nhim as an author. Nearly everything he wrote takes the form of a\\ndialogue. (There is one striking exception: his ,\\nwhich purports to be the speech that Socrates gave in his\\ndefense—the Greek word means\\n“defense”—when, in 399, he was legally charged and\\nconvicted of the crime of impiety. However, even there, Socrates is\\npresented at one point addressing questions of a philosophical\\ncharacter to his accuser, Meletus, and responding to them. In\\naddition, since antiquity, a collection of 13 letters has been\\nincluded among his collected works, but their authenticity as\\ncompositions of Plato is not universally accepted among scholars, and\\nmany or most of them are almost certainly not his (see Burnyeat and\\nFrede 2015). Most of them purport to be the outcome of his involvement\\nin the politics of Syracuse, a heavily populated Greek city located in\\nSicily and ruled by tyrants.)  \\nApology  \\napologia  \\nWe are of course familiar with the dialogue form through our\\nacquaintance with the literary genre of drama. But Plato’s\\ndialogues do not try to create a fictional world for the purposes of\\ntelling a story, as many literary dramas do; nor do they invoke an\\nearlier mythical realm, like the creations of the great Greek\\ntragedians Aeschylus, Sophocles, and Euripides. Nor are they all\\npresented in the form of a drama: in many of them, a single speaker\\nnarrates events in which he participated. They are philosophical\\ndiscussions—“debates” would, in some cases, also be\\nan appropriate word—among a small number of interlocutors, many\\nof whom can be identified as real historical figures (see Nails 2002);\\nand often they begin with a depiction of the setting of the\\ndiscussion—a visit to a prison, a wealthy man’s house, a\\ncelebration over drinks, a religious festival, a visit to the\\ngymnasium, a stroll outside the city’s wall, a long walk on a\\nhot day. As a group, they form vivid portraits of a social world, and\\nare not purely intellectual exchanges between characterless and\\nsocially unmarked speakers. (At any rate, that is true of a large\\nnumber of Plato’s interlocutors. However, it must be added that\\nin some of his works the speakers display little or no character. See,\\nfor example, and —dialogues\\nin which a visitor from the town of Elea in Southern Italy leads the\\ndiscussion; and , a discussion between an unnamed\\nAthenian and two named fictional characters, one from Crete and the\\nother from Sparta.) In of his dialogues (though not\\nall), Plato is not only attempting to draw his readers into a\\ndiscussion, but is also commenting on the social milieu that he is\\ndepicting, and criticizing the character and ways of life of his\\ninterlocutors (see Blondell 2002). Some of the dialogues that most\\nevidently fall into this category are , , , , and .  \\nSophist  \\nStatesman  \\nLaws  \\nmany  \\nProtagoras  \\nGorgias  \\nHippias Major  \\nEuthydemus  \\nSymposium  \\n4. Socrates  \\nThere is one interlocutor who speaks in nearly all of Plato’s\\ndialogues, being completely absent only in , which\\nancient testimony tells us was one of his latest works: that figure is\\nSocrates. Like nearly everyone else who appears in Plato’s\\nworks, he is not an invention of Plato: there really was a Socrates\\njust as there really was a Crito, a Gorgias, a Thrasymachus, and a\\nLaches. Plato was not the only author whose personal experience of\\nSocrates led to the depiction of him as a character in one or more\\ndramatic works. Socrates is one of the principal characters of\\nAristophanes’ comedy, ; and Xenophon, a historian\\nand military leader, wrote, like Plato, both an of\\nSocrates (an account of Socrates’ trial) and other works in\\nwhich Socrates appears as a principal speaker. Furthermore, we have\\nsome fragmentary remains of dialogues written by other contemporaries\\nof Socrates besides Plato and Xenophon (Aeschines, Antisthenes,\\nEucleides, Phaedo), and these purport to describe conversations he\\nconducted with others (see Boys-Stone and Rowe 2013). So, when Plato\\nwrote dialogues that feature Socrates as a principal speaker, he was\\nboth contributing to a genre that was inspired by the life of Socrates\\nand participating in a lively literary debate about the kind of person\\nSocrates was and the value of the intellectual conversations in which\\nhe was involved. Aristophanes’ comic portrayal of Socrates is at\\nthe same time a bitter critique of him and other leading intellectual\\nfigures of the day (the 420s B.C.), but from Plato, Xenophon, and the\\nother composers (in the 390’s and later) of “Socratic\\ndiscourses” (as Aristotle calls this body of writings) we\\nreceive a far more favorable impression.  \\nLaws  \\nClouds  \\nApology  \\nEvidently, the historical Socrates was the sort of person who provoked\\nin those who knew him, or knew of him, a profound response, and he\\ninspired many of those who came under his influence to write about\\nhim. But the portraits composed by Aristophanes, Xenophon, and Plato\\nare the ones that have survived intact, and they are therefore the\\nones that must play the greatest role in shaping our conception of\\nwhat Socrates was like. Of these, has the least value\\nas an indication of what was distinctive of Socrates’ mode of\\nphilosophizing: after all, it is not intended as a philosophical work,\\nand although it may contain a few lines that are characterizations of\\nfeatures unique to Socrates, for the most part it is an attack on a\\nphilosophical type—the long-haired, unwashed, amoral\\ninvestigator into abstruse phenomena—rather than a depiction of\\nSocrates himself. Xenophon’s depiction of Socrates, whatever its\\nvalue as historical testimony (which may be considerable), is\\ngenerally thought to lack the philosophical subtlety and depth of\\nPlato’s. At any rate, no one (certainly not Xenophon himself)\\ntakes Xenophon to be a major philosopher in his own right; when we\\nread his Socratic works, we are not encountering a great philosophical\\nmind. But that is what we experience when we read Plato. We may read\\nPlato’s Socratic dialogues because we are (as Plato evidently\\nwanted us to be) interested in who Socrates was and what he stood for,\\nbut even if we have little or no desire to learn about the historical\\nSocrates, we will want to read Plato because in doing so we are\\nencountering an author of the greatest philosophical significance. No\\ndoubt he in some way borrowed in important ways from Socrates, though\\nit is not easy to say where to draw the line between him and his\\nteacher (more about this below in section 12). But it is widely agreed\\namong scholars that Plato is not a mere transcriber of the words of\\nSocrates (any more than Xenophon or the other authors of Socratic\\ndiscourses). His use of a figure called “Socrates” in so\\nmany of his dialogues should not be taken to mean that Plato is merely\\npreserving for a reading public the lessons he learned from his\\nteacher.  \\nClouds  \\n5. Plato’s indirectness  \\nSocrates, it should be kept in mind, does not appear in all of\\nPlato’s works. He makes no appearance in , and\\nthere are several dialogues ( , , ) in which his role is small and peripheral, while\\nsome other figure dominates the conversation or even, as in the and , presents a long and elaborate,\\ncontinuous discourse of their own. Plato’s dialogues are not a\\nstatic literary form; not only do his topics vary, not only do his\\nspeakers vary, but the role played by questions and answers is never\\nthe same from one dialogue to another. ( , for\\nexample, is a series of speeches, and there are also lengthy speeches\\nin , , , , , , and ; in fact, one might reasonably question whether these\\nworks are properly called dialogues). But even though Plato constantly\\nadapted “the dialogue form” (a commonly used term, and\\nconvenient enough, so long as we do not think of it as an unvarying\\nunity) to suit his purposes, it is striking that throughout his career\\nas a writer he never engaged in a form of composition that was widely\\nused in his time and was soon to become the standard mode of\\nphilosophical address: Plato never became a writer of philosophical\\ntreatises, even though the writing of treatises (for example, on\\nrhetoric, medicine, and geometry) was a common practice among his\\npredecessors and contemporaries. (The closest we come to an exception\\nto this generalization is the seventh letter, which contains a brief\\nsection in which the author, Plato or someone pretending to be him,\\ncommits himself to several philosophical points—while insisting,\\nat the same time, that no philosopher will write about the deepest\\nmatters, but will communicate his thoughts only in private discussion\\nwith selected individuals. As noted above, the authenticity of\\nPlato’s letters is a matter of great controversy; and in any\\ncase, the author of the seventh letter declares his opposition to the\\nwriting of philosophical books. Whether Plato wrote it or not, it\\ncannot be regarded as a philosophical treatise, and its author did not\\nwish it to be so regarded.) In all of his writings—except in the\\nletters, if any of them are genuine—Plato never speaks to his\\naudience directly (see Frede 1992) and in his own voice. Strictly\\nspeaking, he does not himself affirm anything in his dialogues;\\nrather, it is the interlocutors in his dialogues who are made by Plato\\nto do all of the affirming, doubting, questioning, arguing, and so on.\\nWhatever he wishes to communicate to us is conveyed indirectly.  \\nLaws  \\nSophist  \\nStatesman  \\nTimaeus  \\nTimaeus  \\nCritias  \\nSymposium  \\nApology  \\nMenexenus  \\nProtagoras  \\nCrito  \\nPhaedrus  \\nTimaeus  \\nCritias  \\n6. Can we know Plato’s mind?  \\nThis feature of Plato’s works raises important questions about\\nhow they are to be read, and has led to considerable controversy among\\nthose who study his writings. Since he does not himself affirm\\nanything in any of his dialogues, can we ever be on secure ground in\\nattributing a philosophical doctrine to him (as opposed to one of his\\ncharacters)? Did he himself have philosophical convictions, and can we\\ndiscover what they were? Are we justified in speaking of “the\\nphilosophy of Plato”? Or, if we attribute some view to Plato\\nhimself, are we being unfaithful to the spirit in which he intended\\nthe dialogues to be read? Is his whole point, in refraining from\\nwriting treatises, to discourage the readers of his works from asking\\nwhat their author believes and to encourage them instead simply to\\nconsider the plausibility or implausibility of what his characters are\\nsaying? Is that why Plato wrote dialogues? If not for this reason,\\nthen what his purpose in refraining from addressing his\\naudience in a more direct way (see Griswold 1988, Klagge and Smith\\n1992, Press 2002)? There are other important questions about the\\nparticular shape his dialogues take: for example, why does Socrates\\nplay such a prominent role in so many of them, and why, in some of\\nthese works, does Socrates play a smaller role, or none at all?  \\nwas  \\nOnce these questions are raised and their difficulty acknowledged, it\\nis tempting, in reading Plato’s works and reflecting upon them,\\nto adopt a strategy of extreme caution. Rather than commit oneself to\\nany hypothesis about what he is trying to communicate to his readers,\\none might adopt a stance of neutrality about his intentions, and\\nconfine oneself to talking only about what is said by his . One cannot be faulted, for\\nexample, if one notes that, in Plato’s ,\\nSocrates argues that justice in the soul consists in each part of the\\nsoul doing its own. It is equally correct to point out that other\\nprincipal speakers in that work, Glaucon and Adeimantus, accept the\\narguments that Socrates gives for that definition of justice. Perhaps\\nthere is no need for us to say more—to say, for example, that\\nPlato himself agrees that this is how justice should be defined, or\\nthat Plato himself accepts the arguments that Socrates gives in\\nsupport of this definition. And we might adopt this same\\n“minimalist” approach to of Plato’s\\nworks. After all, is it of any importance to discover what went on\\ninside his head as he wrote—to find out whether he himself\\nendorsed the ideas he put in the mouths of his characters, whether\\nthey constitute “the philosophy of Plato”? Should we not\\nread his works for their intrinsic philosophical value, and not as\\ntools to be used for entering into the mind of their author? We know\\nwhat Plato’s characters say—and isn’t that all that\\nwe need, for the purpose of engaging with his works\\nphilosophically?  \\ndramatis  \\npersonae  \\nRepublic  \\nall  \\nBut the fact that we know what Plato’s characters does not show that by refusing to entertain any hypotheses about what\\nthe author of these works is trying to communicate to his readers we\\ncan understand what those characters by what they say.\\nWe should not lose sight of this obvious fact: it is Plato, not any of\\nhis , who is reaching out to a readership\\nand trying to influence their beliefs and actions by means of his\\nliterary actions. When we ask whether an argument put forward by a\\ncharacter in Plato’s works should be read as an effort to\\npersuade us of its conclusion, or is better read as a revelation of\\nhow foolish that speaker is, we are asking about what as author (not that character) is trying to lead us to believe,\\nthrough the writing that he is presenting to our attention. We need to\\ninterpret the work itself to find out what it, or Plato the author, is\\nsaying. Similarly, when we ask how a word that has several different\\nsenses is best understood, we are asking what Plato means to\\ncommunicate to us through the speaker who uses that word. We should\\nnot suppose that we can derive much philosophical value from\\nPlato’s writings if we refuse to entertain any thoughts about\\nwhat use he intends us to make of the things his speakers say.\\nPenetrating the mind of Plato and comprehending what his interlocutors\\nmean by what they say are not two separate tasks but one, and if we do\\nnot ask what his interlocutors mean by what they say, and what the\\ndialogue itself indicates we should think about what they mean, we\\nwill not profit from reading his dialogues.  \\nsay  \\nmean  \\ndramatis personae  \\nPlato  \\nFurthermore, the dialogues have certain characteristics that are most\\neasily explained by supposing that Plato is using them as vehicles for\\ninducing his readers to become convinced (or more convinced than they\\nalready are) of certain propositions—for example, that there are\\nforms, that the soul is not corporeal, that knowledge can be acquired\\nonly by means of a study of the forms, and so on. Why, after all, did\\nPlato write so many works (for example: , , , , , , , , , ) in which one\\ncharacter dominates the conversation (often, but not always, Socrates)\\nand convinces the other speakers (at times, after encountering initial\\nresistance) that they should accept or reject certain conclusions, on\\nthe basis of the arguments presented? The only plausible way of\\nanswering that question is to say that these dialogues were intended\\nby Plato to be devices by which he might induce the audience for which\\nthey are intended to reflect on and accept the arguments and\\nconclusions offered by his principal interlocutor. (It is noteworthy\\nthat in , the principal speaker—an unnamed visitor\\nfrom Athens—proposes that laws should be accompanied by\\n“preludes” in which their philosophical basis is given as\\nfull an explanation as possible. The educative value of written texts\\nis thus explicitly acknowledged by Plato’s dominant speaker. If\\npreludes can educate a whole citizenry that is prepared to learn from\\nthem, then surely Plato thinks that other sorts of written\\ntexts—for example, his own dialogues—can also serve an\\neducative function.)  \\nPhaedo  \\nSymposium  \\nRepublic  \\nPhaedrus  \\nTheaetetus  \\nSophist  \\nStatesman  \\nTimaeus  \\nPhilebus  \\nLaws  \\nLaws  \\nThis does not mean that Plato thinks that his readers can become wise\\nsimply by reading and studying his works. On the contrary, it is\\nhighly likely that he wanted all of his writings to be supplementary\\naids to philosophical conversation: in one of his works, he has\\nSocrates warn his readers against relying solely on books, or taking\\nthem to be authoritative. They are, Socrates says, best used as\\ndevices that stimulate the readers’ memory of discussions they\\nhave had ( 274e-276d). In those face-to-face\\nconversations with a knowledgeable leader, positions are taken,\\narguments are given, and conclusions are drawn. Plato’s\\nwritings, he implies in this passage from , will work\\nbest when conversational seeds have already been sown for the\\narguments they contain.  \\nPhaedrus  \\nPhaedrus  \\n7. Socrates as the dominant speaker  \\nIf we take Plato to be trying to persuade us, in many of his works, to\\naccept the conclusions arrived at by his principal interlocutors (or\\nto persuade us of the refutations of their opponents), we can easily\\nexplain why he so often chooses Socrates as the dominant speaker in\\nhis dialogues. Presumably the contemporary audience for whom Plato was\\nwriting included many of Socrates’ admirers. They would be\\npredisposed to think that a character called “Socrates”\\nwould have all of the intellectual brilliance and moral passion of the\\nhistorical person after whom he is named (especially since Plato often\\nmakes special efforts to give his “Socrates” a life-like\\nreality, and has him refer to his trial or to the characteristics by\\nwhich he was best known); and the aura surrounding the character\\ncalled “Socrates” would give the words he speaks in the\\ndialogue considerable persuasive power. Furthermore, if Plato felt\\nstrongly indebted to Socrates for many of his philosophical techniques\\nand ideas, that would give him further reason for assigning a dominant\\nrole to him in many of his works. (More about this in section 12.)  \\nOf course, there are other more speculative possible ways of\\nexplaining why Plato so often makes Socrates his principal speaker.\\nFor example, we could say that Plato was trying to undermine the\\nreputation of the historical Socrates by writing a series of works in\\nwhich a figure called “Socrates” manages to persuade a\\ngroup of naïve and sycophantic interlocutors to accept absurd\\nconclusions on the basis of sophistries. But anyone who has read some\\nof Plato’s works will quickly recognize the utter implausibility\\nof that alternative way of reading them. Plato could have written into\\nhis works clear signals to the reader that the arguments of Socrates\\ndo not work, and that his interlocutors are foolish to accept them.\\nBut there are many signs in such works as , , , and that point\\nin the opposite direction. (And the great admiration Plato feels for\\nSocrates is also evident from his .) The reader is\\ngiven every encouragement to believe that the reason why Socrates is\\nsuccessful in persuading his interlocutors (on those occasions when he\\ndoes succeed) is that his arguments are powerful ones. The reader, in\\nother words, is being encouraged by the author to accept those\\narguments, if not as definitive then at least as highly arresting and\\ndeserving of careful and full positive consideration. When we\\ninterpret the dialogues in this way, we cannot escape the fact that we\\nare entering into the mind of Plato, and attributing to him, their\\nauthor, a positive evaluation of the arguments that his speakers\\npresent to each other.  \\nMeno  \\nPhaedo  \\nRepublic  \\nPhaedrus  \\nApology  \\n8. Links between the dialogues  \\nThere is a further reason for entertaining hypotheses about what Plato\\nintended and believed, and not merely confining ourselves to\\nobservations about what sorts of people his characters are and what\\nthey say to each other. When we undertake a serious study of Plato,\\nand go beyond reading just one of his works, we are inevitably\\nconfronted with the question of how we are to link the work we are\\ncurrently reading with the many others that Plato composed.\\nAdmittedly, many of his dialogues make a fresh start in their setting\\nand their interlocutors: typically, Socrates encounters a group of\\npeople many of whom do not appear in any other work of Plato, and so,\\nas an author, he needs to give his readers some indication of their\\ncharacter and social circumstances. But often Plato’s characters\\nmake statements that would be difficult for readers to understand\\nunless they had already read one or more of his other works. For\\nexample, in (73a-b), Socrates says that one argument\\nfor the immortality of the soul derives from the fact that when people\\nare asked certain kinds of questions, and are aided with diagrams,\\nthey answer in a way that shows that they are not learning afresh from\\nthe diagrams or from information provided in the questions, but are\\ndrawing their knowledge of the answers from within themselves. That\\nremark would be of little worth for an audience that had not already\\nread . Several pages later, Socrates tells his\\ninterlocutors that his argument about our prior knowledge of equality\\nitself (the form of equality) applies no less to other forms—to\\nthe beautiful, good, just, pious and to all the other things that are\\ninvolved in their asking and answering of questions (75d). This\\nreference to asking and answering questions would not be well\\nunderstood by a reader who had not yet encountered a series of\\ndialogues in which Socrates asks his interlocutors questions of the\\nform, “What is X?” ( : what is piety? : what is courage? : What is\\nmoderation? : what is beauty? see Dancy 2004).\\nEvidently, Plato is assuming that readers of have\\nalready read several of his other works, and will bring to bear on the\\ncurrent argument all of the lessons that they have learned from them.\\nIn some of his writings, Plato’s characters refer ahead to the\\ncontinuation of their conversations on another day, or refer back to\\nconversations they had recently: thus Plato signals to us that we\\nshould read , , and sequentially; and similarly, since the opening of refers us back to , Plato is\\nindicating to his readers that they must seek some connection between\\nthese two works.  \\nPhaedo  \\nMeno  \\nEuthyphro  \\nLaches  \\nCharmides  \\nHippias Major  \\nPhaedo  \\nTheaetetus  \\nSophist  \\nStatesman  \\nTimaeus  \\nRepublic  \\nThese features of the dialogues show Plato’s awareness that he\\ncannot entirely start from scratch in every work that he writes. He\\nwill introduce new ideas and raise fresh difficulties, but he will\\nalso expect his readers to have already familiarized themselves with\\nthe conversations held by the interlocutors of other\\ndialogues—even when there is some alteration among those\\ninterlocutors. (Meno does not re-appear in ; Timaeus\\nwas not among the interlocutors of .) Why does Plato\\nhave his dominant characters (Socrates, the Eleatic visitor) reaffirm\\nsome of the same points from one dialogue to another, and build on\\nideas that were made in earlier works? If the dialogues were merely\\nmeant as provocations to thought—mere exercises for the\\nmind—there would be no need for Plato to identify his leading\\ncharacters with a consistent and ever-developing doctrine. For\\nexample, Socrates continues to maintain, over a large number of\\ndialogues, that there are such things as forms—and there is no\\nbetter explanation for this continuity than to suppose that Plato is\\nrecommending that doctrine to his readers. Furthermore, when Socrates\\nis replaced as the principal investigator by the visitor from Elea (in and ), the existence of forms\\ncontinues to be taken for granted, and the visitor criticizes any\\nconception of reality that excludes such incorporeal objects as souls\\nand forms. The Eleatic visitor, in other words, upholds a metaphysics\\nthat is, in many respects, like the one that Socrates is made to\\ndefend. Again, the best explanation for this continuity is that Plato\\nis using both characters—Socrates and the Eleatic\\nvisitor—as devices for the presentation and defense of a\\ndoctrine that he embraces and wants his readers to embrace as\\nwell.  \\nPhaedo  \\nRepublic  \\nSophist  \\nStatesman  \\n9. Does Plato change his mind about forms?  \\nThis way of reading Plato’s dialogues does not presuppose that\\nhe never changes his mind about anything—that whatever any of\\nhis main interlocutors uphold in one dialogue will continue to be\\npresupposed or affirmed elsewhere without alteration. It is, in fact,\\na difficult and delicate matter to determine, on the basis of our\\nreading of the dialogues, whether Plato means to modify or reject in\\none dialogue what he has his main interlocutor affirm in some other.\\nOne of the most intriguing and controversial questions about his\\ntreatment of the forms, for example, is whether he concedes that his\\nconception of those abstract entities is vulnerable to criticism; and,\\nif so, whether he revises some of the assumptions he had been making\\nabout them, or develops a more elaborate picture of them that allows\\nhim to respond to that criticism (see Meinwald 2016). In , the principal interlocutor (not Socrates—he\\nis here portrayed as a promising, young philosopher in need of further\\ntraining—but rather the pre-Socratic from Elea who gives the\\ndialogue its name: Parmenides) subjects the forms to withering\\ncriticism, and then consents to conduct an inquiry into the nature of\\noneness that has no overt connection to his critique of the forms.\\nDoes the discussion of oneness (a baffling series of\\ncontradictions—or at any rate, propositions that seem, on the\\nsurface, to be contradictions) in some way help address the problems\\nraised about forms? That is one way of reading the dialogue. And if we\\ndo read it in this way, does that show that Plato has changed his mind\\nabout some of the ideas about forms he inserted into earlier\\ndialogues? Can we find dialogues in which we encounter a “new\\ntheory of forms”—that is, a way of thinking of forms that\\ncarefully steers clear of the assumptions about forms that led to\\nParmenides’ critique? It is not easy to say. But we cannot even\\nraise this as an issue worth pondering unless we presuppose that\\nbehind the dialogues there stands a single mind that is using these\\nwritings as a way of hitting upon the truth, and of bringing that\\ntruth to the attention of others. If we find Timaeus (the principal\\ninterlocutor of the dialogue named after him) and the Eleatic visitor\\nof the and talking about forms in\\na way that is entirely consistent with the way Socrates talks about\\nforms in and , then there is only one\\nreasonable explanation for that consistency: Plato believes that their\\nway of talking about forms is correct, or is at least strongly\\nsupported by powerful considerations. If, on the other hand, we find\\nthat Timaeus or the Eleatic visitor talks about forms in a way that\\ndoes not harmonize with the way Socrates conceives of those abstract\\nobjects, in the dialogues that assign him a central role as director\\nof the conversation, then the most plausible explanation for these\\ndiscrepancies is that Plato has changed his mind about the nature of\\nthese entities. It would be implausible to suppose that Plato himself\\nhad no convictions about forms, and merely wants to give his readers\\nmental exercise by composing dialogues in which different leading\\ncharacters talk about these objects in discordant ways.  \\nParmenides  \\nSophist  \\nStatesman  \\nPhaedo  \\nRepublic  \\n10. Does Plato change his mind about politics?  \\nThe same point—that we must view the dialogues as the product of\\na single mind, a single philosopher, though perhaps one who changes\\nhis mind—can be made in connection with the politics of\\nPlato’s works (see Bobonich 2002).  \\nIt is noteworthy, to begin with, that Plato is, among other things, a philosopher. For he gives expression, in several of\\nhis writings (particular ), to a yearning to escape\\nfrom the tawdriness of ordinary human relations. (Similarly, he\\nevinces a sense of the ugliness of the sensible world, whose beauty\\npales in comparison with that of the forms.) Because of this, it would\\nhave been all too easy for Plato to turn his back entirely on\\npractical reality, and to confine his speculations to theoretical\\nquestions. Some of his works— is a stellar\\nexample—do confine themselves to exploring questions that seem\\nto have no bearing whatsoever on practical life. But it is remarkable\\nhow few of his works fall into this category. Even the highly abstract\\nquestions raised in about the nature of being and\\nnot-being are, after all, embedded in a search for the definition of\\nsophistry; and thus they call to mind the question whether Socrates\\nshould be classified as a sophist—whether, in other words,\\nsophists are to be despised and avoided. In any case, despite the\\ngreat sympathy Plato expresses for the desire to shed one’s body\\nand live in an incorporeal world, he devotes an enormous amount of\\nenergy to the task of understanding the world we live in, appreciating\\nits limited beauty, and improving it.  \\npolitical  \\nPhaedo  \\nParmenides  \\nSophist  \\nHis tribute to the mixed beauty of the sensible world, in , consists in his depiction of it as the outcome of\\ndivine efforts to mold reality in the image of the forms, using simple\\ngeometrical patterns and harmonious arithmetic relations as building\\nblocks. The desire to transform human relations is given expression in\\na far larger number of works. Socrates presents himself, in\\nPlato’s , as a man who does not have his head in\\nthe clouds (that is part of Aristophanes’ charge against him in ). He does not want to escape from the everyday world\\nbut to make it better (see Allen 2010). He presents himself, in , as the only Athenian who has tried his hand at the\\ntrue art of politics.  \\nTimaeus  \\nApology  \\nClouds  \\nGorgias  \\nSimilarly, the Socrates of devotes a considerable\\npart of his discussion to the critique of ordinary social\\ninstitutions—the family, private property, and rule by the many.\\nThe motivation that lies behind the writing of this dialogue is the\\ndesire to transform (or, at any rate, to improve) political life, not\\nto escape from it (although it is acknowledged that the desire to\\nescape is an honorable one: the best sort of rulers greatly prefer the\\ncontemplation of divine reality to the governance of the city). And if\\nwe have any further doubts that Plato does take an interest in the\\npractical realm, we need only turn to . A work of such\\ngreat detail and length about voting procedures, punishments,\\neducation, legislation, and the oversight of public officials can only\\nhave been produced by someone who wants to contribute something to the\\nimprovement of the lives we lead in this sensible and imperfect realm.\\nFurther evidence of Plato’s interest in practical matters can be\\ndrawn from his letters, if they are genuine. In most of them, he\\npresents himself as having a deep interest in educating (with the help\\nof his friend, Dion) the ruler of Syracuse, Dionysius II, and thus\\nreforming that city’s politics.  \\nRepublic  \\nLaws  \\nJust as any attempt to understand Plato’s views about forms must\\nconfront the question whether his thoughts about them developed or\\naltered over time, so too our reading of him as a political\\nphilosopher must be shaped by a willingness to consider the\\npossibility that he changed his mind. For example, on any plausible\\nreading of , Plato evinces a deep antipathy to rule\\nby the many. Socrates tells his interlocutors that the only politics\\nthat should engage them are those of the anti-democratic regime he\\ndepicts as the paradigm of a good constitution. And yet in , the Athenian visitor proposes a detailed legislative\\nframework for a city in which non-philosophers (people who have never\\nheard of the forms, and have not been trained to understand them) are\\ngiven considerable powers as rulers. Plato would not have invested so\\nmuch time in the creation of this comprehensive and lengthy work, had\\nhe not believed that the creation of a political community ruled by\\nthose who are philosophically unenlightened is a project that deserves\\nthe support of his readers. Has Plato changed his mind, then? Has he\\nre-evaluated the highly negative opinion he once held of those who are\\ninnocent of philosophy? Did he at first think that the reform of\\nexisting Greek cities, with all of their imperfections, is a waste of\\ntime—but then decide that it is an endeavor of great value? (And\\nif so, what led him to change his mind?) Answers to these questions\\ncan be justified only by careful attention to what he has his\\ninterlocutors say. But it would be utterly implausible to suppose that\\nthese developmental questions need not be raised, on the grounds that and each has its own cast of\\ncharacters, and that the two works therefore cannot come into\\ncontradiction with each other. According to this hypothesis (one that\\nmust be rejected), because it is Socrates (not Plato) who is critical\\nof democracy in , and because it is the Athenian\\nvisitor (not Plato) who recognizes the merits of rule by the many in , there is no possibility that the two dialogues are in\\ntension with each other. Against this hypothesis, we should say: Since\\nboth and are works in which Plato is\\ntrying to move his readers towards certain conclusions, by having them\\nreflect on certain arguments—these dialogues are not barred from\\nhaving this feature by their use of interlocutors—it would be an\\nevasion of our responsibility as readers and students of Plato not to\\nask whether what one of them advocates is compatible with what the\\nother advocates. If we answer that question negatively, we have some\\nexplaining to do: what led to this change? Alternatively, if we\\nconclude that the two works are compatible, we must say why the\\nappearance of conflict is illusory.  \\nRepublic  \\nLaws  \\nRepublic  \\nLaws  \\nRepublic  \\nLaws  \\nRepublic  \\nLaws  \\n11. The historical Socrates: early, middle, and late dialogues  \\nMany contemporary scholars find it plausible that when Plato embarked\\non his career as a philosophical writer, he composed, in addition to\\nhis of Socrates, a number of short ethical dialogues\\nthat contain little or nothing in the way of positive philosophical\\ndoctrine, but are mainly devoted to portraying the way in which\\nSocrates punctured the pretensions of his interlocutors and forced\\nthem to realize that they are unable to offer satisfactory definitions\\nof the ethical terms they used, or satisfactory arguments for their\\nmoral beliefs. According to this way of placing the dialogues into a\\nrough chronological order—associated especially with Gregory\\nVlastos’s name (see especially his , chapters 2 and 3)—Plato, at this point\\nof his career, was content to use his writings primarily for the\\npurpose of preserving the memory of Socrates and making plain the\\nsuperiority of his hero, in intellectual skill and moral seriousness,\\nto all of his contemporaries—particularly those among them who\\nclaimed to be experts on religious, political, or moral matters. Into\\nthis category of early dialogues (they are also sometimes called\\n“Socratic” dialogues, possibly without any intended\\nchronological connotation) are placed: , , , , , , , , , , and , (Some scholars hold that we can tell which of\\nthese come later during Plato’s early period. For example, it is\\nsometimes said that and are\\nlater, because of their greater length and philosophical complexity.\\nOther dialogues—for example, and —are thought not to be among Plato’s\\nearliest within this early group, because in them Socrates appears to\\nbe playing a more active role in shaping the progress of the dialogue:\\nthat is, he has more ideas of his own.) In comparison with many of\\nPlato’s other dialogues, these “Socratic” works\\ncontain little in the way of metaphysical, epistemological, or\\nmethodological speculation, and they therefore fit well with the way\\nSocrates characterizes himself in Plato’s : as a\\nman who leaves investigations of high falutin’ matters (which\\nare “in the sky and below the earth”) to wiser heads, and\\nconfines all of his investigations to the question how one should live\\none’s life. Aristotle describes Socrates as someone whose\\ninterests were restricted to only one branch of philosophy—the\\nrealm of the ethical; and he also says that he was in the habit of\\nasking definitional questions to which he himself lacked answers\\n( 987b1, 183b7).\\nThat testimony gives added weight to the widely accepted hypothesis\\nthat there is a group of dialogues—the ones mentioned above as\\nhis early works, whether or not they were all written early in\\nPlato’s writing career—in which Plato used the dialogue\\nform as a way of portraying the philosophical activities of the\\nhistorical Socrates (although, of course, he might also have used them\\nin other ways as well—for example to suggest and begin to\\nexplore philosophical difficulties raised by them, see Santas 1979,\\nBrickhouse and Smith 1994).  \\nApology  \\nSocrates Ironist and\\nMoral Philosopher  \\nCharmides  \\nCrito  \\nEuthydemus  \\nEuthyphro  \\nGorgias  \\nHippias  \\nMajor  \\nHippias  \\nMinor  \\nIon  \\nLaches  \\nLysis  \\nProtagoras  \\nProtagoras  \\nGorgias  \\nCharmides  \\nLysis  \\nApology  \\nMetaphysics  \\nSophistical Refutations  \\nBut at a certain point—so says this hypothesis about the\\nchronology of the dialogues—Plato began to use his works to\\nadvance ideas that were his own creations rather than those of\\nSocrates, although he continued to use the name “Socrates”\\nfor the interlocutor who presented and argued for these new ideas. The\\nspeaker called “Socrates” now begins to move beyond and\\ndepart from the historical Socrates: he has views about the\\nmethodology that should be used by philosophers (a methodology\\nborrowed from mathematics), and he argues for the immortality of the\\nsoul and the existence and importance of the forms of beauty, justice,\\ngoodness, and the like. (By contrast, in Socrates\\nsays that no one knows what becomes of us after we die.) is often said to be the dialogue in which Plato first\\ncomes into his own as a philosopher who is moving far beyond the ideas\\nof his teacher (though it is also commonly said that we see a new\\nmethodological sophistication and a greater interest in mathematical\\nknowledge in ). Having completed all of the dialogues\\nthat, according to this hypothesis, we characterize as early, Plato\\nwidened the range of topics to be explored in his writings (no longer\\nconfining himself to ethics), and placed the theory of forms (and\\nrelated ideas about language, knowledge, and love) at the center of\\nhis thinking. In these works of his “middle”\\nperiod—for example, in , , , , and —there is both a change of emphasis and of\\ndoctrine. The focus is no longer on ridding ourselves of false ideas\\nand self-deceit; rather, we are asked to accept (however tentatively)\\na radical new conception of ourselves (now divided into three parts),\\nour world—or rather, our two worlds—and our need to\\nnegotiate between them. Definitions of the most important virtue terms\\nare finally proposed in (the search for them in some\\nof the early dialogues having been unsuccessful): Book I of this\\ndialogue is a portrait of how the historical Socrates might have\\nhandled the search for a definition of justice, and the rest of the\\ndialogue shows how the new ideas and tools discovered by Plato can\\ncomplete the project that his teacher was unable to finish. Plato\\ncontinues to use a figure called “Socrates” as his\\nprincipal interlocutor, and in this way he creates a sense of\\ncontinuity between the methods, insights, and ideals of the historical\\nSocrates and the new Socrates who has now become a vehicle for the\\narticulation of his own new philosophical outlook. In doing so, he\\nacknowledges his intellectual debt to his teacher and appropriates for\\nhis own purposes the extraordinary prestige of the man who was the\\nwisest of his time.  \\nApology  \\nPhaedo  \\nMeno  \\nPhaedo  \\nCratylus  \\nSymposium  \\nRepublic  \\nPhaedrus  \\nRepublic  \\nThis hypothesis about the chronology of Plato’s writings has a\\nthird component: it does not place his works into either of only two\\ncategories—the early or “Socratic” dialogues, and\\nall the rest—but works instead with a threefold division of\\nearly, middle, and late. That is because, following ancient testimony,\\nit has become a widely accepted assumption that is one\\nof Plato’s last works, and further that this dialogue shares a\\ngreat many stylistic affinities with a small group of others: , , , , and . These five dialogues together\\nwith are generally agreed to be his late works, because\\nthey have much more in common with each other, when one counts certain\\nstylistic features apparent only to readers of Plato’s Greek,\\nthan with any of Plato’s other works. (Computer counts have\\naided these stylometric studies, but the isolation of a group of six\\ndialogues by means of their stylistic commonalities was recognized in\\nthe nineteenth century. See Brandwood 1990, Young 1994.)  \\nLaws  \\nSophist  \\nStatesman  \\nTimaeus  \\nCritias  \\nPhilebus  \\nLaws  \\nIt is not at all clear whether there are one or more affinities among this group of six\\ndialogues—that is, whether the philosophy they contain is\\nsharply different from that of all of the other dialogues. Plato does\\nnothing to encourage the reader to view these works as a distinctive\\nand separate component of his thinking. On the contrary, he links with (the conversations they\\npresent have a largely overlapping cast of characters, and take place\\non successive days) no less than and . contains, in its opening pages, a\\nreference to the conversation of —and perhaps\\nPlato is thus signaling to his readers that they should bring to bear\\non the lessons that are to be drawn from . Similarly, opens with a reminder\\nof some of the principal ethical and political doctrines of . It could be argued, of course, that when one looks\\nbeyond these stage-setting devices, one finds significant\\nphilosophical changes in the six late dialogues, setting this group\\noff from all that preceded them. But there is no consensus that they\\nshould be read in this way. Resolving this issue requires intensive\\nstudy of the content of Plato’s works. So, although it is widely\\naccepted that the six dialogues mentioned above belong to\\nPlato’s latest period, there is, as yet, no agreement among\\nstudents of Plato that these six form a distinctive stage in his\\nphilosophical development.  \\nphilosophical  \\nSophist  \\nTheaetetus  \\nSophist  \\nStatesman  \\nSophist  \\nParmenides  \\nSophist  \\nParmenides  \\nTimaeus  \\nRepublic  \\nIn fact, it remains a matter of dispute whether the division of\\nPlato’s works into three periods—early, middle,\\nlate—does correctly indicate the order of composition, and\\nwhether it is a useful tool for the understanding of his thought (See\\nCooper 1997, vii–xxvii). Of course, it would be wildly\\nimplausible to suppose that Plato’s writing career began with\\nsuch complex works as , , , or . In light of widely accepted\\nassumptions about how most philosophical minds develop, it is likely\\nthat when Plato started writing philosophical works some of the\\nshorter and simpler dialogues were the ones he composed: , or , or (for example).\\n(Similarly, does not advance a complex philosophical\\nagenda or presuppose an earlier body of work; so that too is likely to\\nhave been composed near the beginning of Plato’s writing\\ncareer.) Even so, there is no good reason to eliminate the hypothesis\\nthat throughout much of his life Plato devoted himself to writing two\\nsorts of dialogues at the same time, moving back and forth between\\nthem as he aged: on the one hand, introductory works whose primary\\npurpose is to show readers the difficulty of apparently simple\\nphilosophical problems, and thereby to rid them of their pretensions\\nand false beliefs; and on the other hand, works filled with more\\nsubstantive philosophical theories supported by elaborate\\nargumentation. Moreover, one could point to features of many of the\\n“Socratic” dialogues that would justify putting them in\\nthe latter category, even though the argumentation does not concern\\nmetaphysics or methodology or invoke\\nmathematics— , , , , among\\nthem.  \\nLaws  \\nParmenides  \\nPhaedrus  \\nRepublic  \\nLaches  \\nCrito  \\nIon  \\nApology  \\nGorgias  \\nProtagoras  \\nLysis  \\nEuthydemus  \\nHippias Major  \\nPlato makes it clear that both of these processes, one preceding the\\nother, must be part of one’s philosophical education. One of his\\ndeepest methodological convictions (affirmed in , , and ) is that in order to make\\nintellectual progress we must recognize that knowledge cannot be\\nacquired by passively receiving it from others: rather, we must work\\nour way through problems and assess the merits of competing theories\\nwith an independent mind. Accordingly, some of his dialogues are\\nprimarily devices for breaking down the reader’s complacency,\\nand that is why it is essential that they come to no positive\\nconclusions; others are contributions to theory-construction, and are\\ntherefore best absorbed by those who have already passed through the\\nfirst stage of philosophical development. We should not assume that\\nPlato could have written the preparatory dialogues only at the\\nearliest stage of his career. Although he may well have begun his\\nwriting career by taking up that sort of project, he may have\\ncontinued writing these “negative” works at later stages,\\nat the same time that he was composing his theory-constructing\\ndialogues. For example although both and are widely assumed to be early dialogues, they\\nmight have been written around the same time as and , which are generally assumed to be compositions of\\nhis middle period—or even later.  \\nMeno  \\nTheaetetus  \\nSophist  \\nEuthydemus  \\nCharmides  \\nSymposium  \\nRepublic  \\nNo doubt, some of the works widely considered to be early really are\\nsuch. But it is an open question which and how many of them are. At\\nany rate, it is clear that Plato continued to write in a\\n“Socratic” and “negative” vein even after he\\nwas well beyond the earliest stages of his career: features a Socrates who is even more insistent upon his ignorance than\\nare the dramatic representations of Socrates in briefer and\\nphilosophically less complex works that are reasonably assumed to be\\nearly; and like many of those early works, seeks\\nbut does not find the answer to the “what is it?” question\\nthat it relentlessly pursues—“What is knowledge?”\\nSimilarly, , though certainly not an early\\ndialogue, is a work whose principal aim is to puzzle the reader by the\\npresentation of arguments for apparently contradictory conclusions;\\nsince it does not tell us how it is possible to accept all of those\\nconclusions, its principal effect on the reader is similar to that of\\ndialogues (many of them no doubt early) that reach only negative\\nconclusions. Plato uses this educational device—provoking the\\nreader through the presentation of opposed arguments, and leaving the\\ncontradiction unresolved—in (often\\nconsidered an early dialogue) as well. So it is clear that even after\\nhe was well beyond the earliest stages of his thinking, he continued\\nto assign himself the project of writing works whose principal aim is\\nthe presentation of unresolved difficulties. (And, just as we should\\nrecognize that puzzling the reader continues to be his aim even in\\nlater works, so too we should not overlook the fact that there is some\\nsubstantive theory-construction in the ethical works that are simple\\nenough to have been early compositions: , for example,\\naffirms a theory of poetic inspiration; and sets out\\nthe conditions under which a citizen acquires an obligation to obey\\ncivic commands. Neither ends in failure.)  \\nTheaetetus  \\nTheaetetus  \\nParmenides  \\nProtagoras  \\nIon  \\nCrito  \\nIf we are justified in taking Socrates’ speech in Plato’s to constitute reliable evidence about what the\\nhistorical Socrates was like, then whatever we find in Plato’s\\nother works that is of a piece with that speech can also be safely\\nattributed to Socrates. So understood, Socrates was a moralist but\\n(unlike Plato) not a metaphysician or epistemologist or cosmologist.\\nThat fits with Aristotle’s testimony, and Plato’s way of\\nchoosing the dominant speaker of his dialogues gives further support\\nto this way of distinguishing between him and Socrates. The number of\\ndialogues that are dominated by a Socrates who is spinning out\\nelaborate philosophical doctrines is remarkably small: , , , and . All of them are dominated by ethical issues:\\nwhether to fear death, whether to be just, whom to love, the place of\\npleasure. Evidently, Plato thinks that it is appropriate to make\\nSocrates the major speaker in a dialogue that is filled with positive\\ncontent only when the topics explored in that work primarily have to\\ndo with the ethical life of the individual. (The political aspects of are explicitly said to serve the larger question\\nwhether any individual, no matter what his circumstances, should be\\njust.) When the doctrines he wishes to present systematically become\\nprimarily metaphysical, he turns to a visitor from Elea\\n( , ); when they become cosmological,\\nhe turns to Timaeus; when they become constitutional, he turns, in , to a visitor from Athens (and he then eliminates\\nSocrates entirely). In effect, Plato is showing us: although he owes a\\ngreat deal to the ethical insights of Socrates, as well as to his\\nmethod of puncturing the intellectual pretensions of his interlocutors\\nby leading them into contradiction, he thinks he should not put into\\nthe mouth of his teacher too elaborate an exploration of ontological,\\nor cosmological, or political themes, because Socrates refrained from\\nentering these domains. This may be part of the explanation why he has\\nSocrates put into the mouth of the personified Laws of Athens the\\ntheory advanced in , which reaches the conclusion that\\nit would be unjust for him to escape from prison. Perhaps Plato is\\nindicating, at the point where these speakers enter the dialogue, that\\nnone of what is said here is in any way derived from or inspired by\\nthe conversation of Socrates.  \\nApology  \\nPhaedo  \\nRepublic  \\nPhaedrus  \\nPhilebus  \\nRepublic  \\nSophist  \\nStatesman  \\nLaws  \\nCrito  \\nJust as we should reject the idea that Plato must have made a\\ndecision, at a fairly early point in his career, no longer to write\\none kind of dialogue (negative, destructive, preparatory) and to write\\nonly works of elaborate theory-construction; so we should also\\nquestion whether he went through an early stage during which he\\nrefrained from introducing into his works any of his own ideas (if he\\nhad any), but was content to play the role of a faithful portraitist,\\nrepresenting to his readers the life and thought of Socrates. It is\\nunrealistic to suppose that someone as original and creative as Plato,\\nwho probably began to write dialogues somewhere in his thirties (he\\nwas around 28 when Socrates was killed), would have started his\\ncompositions with no ideas of his own, or, having such ideas, would\\nhave decided to suppress them, for some period of time, allowing\\nhimself to think for himself only later. (What would have led to such\\na decision?) We should instead treat the moves made in the dialogues,\\neven those that are likely to be early, as Platonic\\ninventions—derived, no doubt, by Plato’s reflections on\\nand transformations of the key themes of Socrates that he attributes\\nto Socrates in . That speech indicates, for example,\\nthat the kind of religiosity exhibited by Socrates was unorthodox and\\nlikely to give offense or lead to misunderstanding. It would be\\nimplausible to suppose that Plato simply concocted the idea that\\nSocrates followed a divine sign, especially because Xenophon too\\nattributes this to his Socrates. But what of the various philosophical\\nmoves rehearsed in —the dialogue in which\\nSocrates searches, unsuccessfully, for an understanding of what piety\\nis? We have no good reason to think that in writing this work Plato\\nadopted the role of a mere recording device, or something close to it\\n(changing a word here and there, but for the most part simply\\nrecalling what he heard Socrates say, as he made his way to court). It\\nis more likely that Plato, having been inspired by the unorthodoxy of\\nSocrates’ conception of piety, developed, on his own, a series\\nof questions and answers designed to show his readers how difficult it\\nis to reach an understanding of the central concept that\\nSocrates’ fellow citizens relied upon when they condemned him to\\ndeath. The idea that it is important to search for definitions may\\nhave been Socratic in origin. (After all, Aristotle attributes this\\nmuch to Socrates.) But the twists and turns of the arguments in and other dialogues that search for definitions are\\nmore likely to be the products of Plato’s mind than the content\\nof any conversations that really took place.  \\nApology  \\nEuthyphro  \\nEuthyphro  \\n12. Why dialogues?  \\nIt is equally unrealistic to suppose that when Plato embarked on his\\ncareer as a writer, he made a conscious decision to put all of the\\ncompositions that he would henceforth compose for a general reading\\npublic (with the exception of ) in the form of a\\ndialogue. If the question, “why did Plato write\\ndialogues?”, which many of his readers are tempted to ask,\\npre-supposes that there must have been some such once-and-for-all\\ndecision, then it is poorly posed. It makes better sense to break that\\nquestion apart into many little ones: better to ask, “Why did\\nPlato write (for example: , or , or , or ) in the form of a dialogue—and one , say) mostly in the form of a long and\\nrhetorically elaborate single speech?” than to ask why he\\ndecided to adopt the dialogue form.  \\nApology  \\nthis particular work  \\nProtagoras  \\nRepublic  \\nSymposium  \\nLaws  \\nthat  \\n(  \\nTimaeus  \\nThe best way to form a reasonable conjecture about why Plato wrote any\\ngiven work in the form of a dialogue is to ask: what would be lost,\\nwere one to attempt to re-write this work in a way that eliminated the\\ngive-and-take of interchange, stripped the characters of their\\npersonality and social markers, and transformed the result into\\nsomething that comes straight from the mouth of its author? This is\\noften a question that will be easy to answer, but the answer might\\nvary greatly from one dialogue to another. In pursuing this strategy,\\nwe must not rule out the possibility that some of Plato’s\\nreasons for writing this or that work in the form of a dialogue will\\nalso be his reason for doing so in other cases—perhaps some of\\nhis reasons, so far as we can guess at them, will be present in all\\nother cases. For example, the use of character and conversation allows\\nan author to enliven his work, to awaken the interest of his\\nreadership, and therefore to reach a wider audience. The enormous\\nappeal of Plato’s writings is in part a result of their dramatic\\ncomposition. Even treatise-like compositions— and , for example—improve in readability because of\\ntheir conversational frame. Furthermore, the dialogue form allows\\nPlato’s evident interest in pedagogical questions (how is it\\npossible to learn? what is the best way to learn? from what sort of\\nperson can we learn? what sort of person is in a position to learn?)\\nto be pursued not only in the content of his compositions but also in\\ntheir form. Even in such questions are not far from\\nPlato’s mind, as he demonstrates, through the dialogue form, how\\nit is possible for the citizens of Athens, Sparta, and Crete to learn\\nfrom each other by adapting and improving upon each other’s\\nsocial and political institutions.  \\nTimaeus  \\nLaws  \\nLaws  \\nIn some of his works, it is evident that one of Plato’s goals is\\nto create a sense of puzzlement among his readers, and that the\\ndialogue form is being used for this purpose. The is perhaps the clearest example of such a work, because here Plato\\nrelentlessly rubs his readers’ faces in a baffling series of\\nunresolved puzzles and apparent contradictions. But several of his\\nother works also have this character, though to a smaller degree: for\\nexample, (can virtue be taught?), (is voluntary wrongdoing better than involuntary\\nwrongdoing?), and portions of (are some people virtuous\\nbecause of divine inspiration?). Just as someone who encounters\\nSocrates in conversation should sometimes be puzzled about whether he\\nmeans what he says (or whether he is instead speaking ironically), so\\nPlato sometimes uses the dialogue form to create in his readers a\\nsimilar sense of discomfort about what he means and what we ought to\\ninfer from the arguments that have been presented to us. But Socrates\\ndoes not speak ironically, and similarly Plato’s\\ndialogues do not aim at creating a sense of bafflement\\nabout what we are to think about the subject under discussion. There\\nis no mechanical rule for discovering how best to read a dialogue, no\\ninterpretive strategy that applies equally well to all of his works.\\nWe will best understand Plato’s works and profit most from our\\nreading of them if we recognize their great diversity of styles and\\nadapt our way of reading accordingly. Rather than impose on our\\nreading of Plato a uniform expectation of what he must be doing\\n(because he has done such a thing elsewhere), we should bring to each\\ndialogue a receptivity to what is unique to it. That would be the most\\nfitting reaction to the artistry in his philosophy.  \\nParmenides  \\nProtagoras  \\nHippias\\nMinor  \\nMeno  \\nalways  \\nalways  \\nBibliography  \\nThe bibliography below is meant as a highly selective and limited\\nguide for readers who want to learn more about the issues covered\\nabove. Further discussion of these and other issues regarding\\nPlato’s philosophy, and far more bibliographical information, is\\navailable in the other entries on Plato.  \\nPrimary Literature  \\nCooper, John M. (ed.), 1997, ,\\nIndianapolis: Hackett. (Contains translations of all the works handed\\ndown from antiquity with attribution to Plato, some of which are\\nuniversally agreed to be spurious, with explanatory footnotes and both\\na general Introduction to the study of the dialogues and individual\\nIntroductory Notes to each work translated.)  \\nPlato: Complete Works  \\nBurnyeat, Myles and Michael Frede, 2015, , Dominic Scott (ed.), Oxford: Oxford University\\nPress.  \\nThe Pseudo-Platonic\\nSeventh Letter  \\nSecondary Literature  \\nAhbel-Rappe, Sara, and Rachana Kamtekar (eds.), 2006, , Oxford: Blackwell.  \\nA\\nCompanion to Socrates  \\nAllen, Danielle, S., 2010, , Malden, MA:\\nWiley-Blackwell.  \\nWhy Plato Wrote  \\nAnnas, Julia, 2003, ,\\nOxford: Oxford University Press.  \\nPlato: A Very Short Introduction  \\nBenson, Hugh (ed.), 2006, , Oxford:\\nBlackwell.  \\nA Companion to Plato  \\nBlondell, Ruby, 2002, , Cambridge: Cambridge University Press.  \\nThe Play of Character in Plato’s\\nDialogues  \\nBobonich, Christopher, 2002, , Oxford: Oxford University Press.  \\nPlato’s Utopia Recast: His\\nLater Ethics and Politics  \\nBoys-Stone George, and Christopher Rowe (eds.), 2013, ,\\nIndianapolis: Hackett.  \\nThe\\nCircle of Socrates: Readings in the First-Generation Socratics  \\nBrandwood, Leonard, 1990, , Cambridge: Cambridge University Press.  \\nThe Chronology of Plato’s\\nDialogues  \\nBrickhouse, Thomas C. & Nicholas D. Smith, 1994, , Oxford: Oxford University Press.  \\nPlato’s Socrates  \\nDancy, Russell, 2004, , Cambridge: Cambridge University Press.  \\nPlato’s Introduction of\\nForms  \\nEbrey, David and Richard Kraut (eds.), 2022, , Cambridge: Cambridge University Press.  \\nThe Cambridge\\nCompanion to Plato  \\nFine, Gail (ed.), 1999, , Oxford: Oxford University Press.  \\nPlato 1: Metaphysics and\\nEpistemology  \\n––– (ed.), 1999, , Oxford: Oxford University Press.  \\nPlato 2: Ethics, Politics,\\nReligion, and the Soul  \\n––– (ed.), 2008, , Oxford: Oxford University Press. (Essays by many scholars\\non a wide range of topics, including several studies of individual\\ndialogues.)  \\nThe Oxford Handbook of\\nPlato  \\n––– (ed.), 2019, , Oxford: Oxford University Press.  \\nThe Oxford Handbook of\\nPlato  \\nFrede, Michael, 1992, “Plato’s Arguments and the\\nDialogue Form,” in , Supplementary Volume 1992, Oxford: Oxford University\\nPress, pp. 201–220.  \\nOxford Studies in Ancient\\nPhilosophy  \\nGriswold, Charles L. (ed.), 1988, , London: Routledge.  \\nPlatonic Writings, Platonic\\nReadings  \\nGuthrie, W.K.C., 1971, , Cambridge: Cambridge\\nUniversity Press.  \\nSocrates  \\n–––, 1975, ,\\nVolume 4, Cambridge: Cambridge University Press.  \\nA History of Greek Philosophy  \\n–––, 1978, , Volume 5, Cambridge: Cambridge University Press.  \\nA History of Greek\\nPhilosophy  \\nIrwin, Terence, 1995, , Oxford:\\nOxford University Press.  \\nPlato’s Ethics  \\nKahn, Charles H., 1996, , Cambridge: Cambridge\\nUniversity Press.  \\nPlato and the Socratic Dialogue: The\\nPhilosophical Use of a Literary Form  \\n–––, 2003, “On Platonic Chronology,”\\nin Julia Annas and Christopher Rowe (eds.), , Cambridge, MA: Harvard University\\nPress, chapter 4.  \\nNew Perspectives on\\nPlato: Modern and Ancient  \\nKlagge, James C. and Nicholas D. Smith (eds.), 1992, , Oxford Studies in Ancient\\nPhilosophy, Supplementary Volume 1992, Oxford: Clarendon Press.  \\nMethods\\nof Interpreting Plato and His Dialogue  \\nKraut, Richard (ed.), 1992, , Cambridge: Cambridge University Press.  \\nThe Cambridge Companion to\\nPlato  \\n–––, 2008, , London:\\nGranta.  \\nHow to Read Plato  \\nLedger, Gerald R., 1989, , Oxford: Oxford University\\nPress.  \\nRe-Counting Plato: A Computer\\nAnalysis of Plato’s Style  \\nMcCabe, Mary Margaret, 1994, ,\\nPrinceton: Princeton University Press.  \\nPlato’s Individuals  \\n–––, 2000, , Cambridge: Cambridge University\\nPress.  \\nPlato and His Predecessors: The\\nDramatisation of Reason  \\nMeinwald, Constance, 2016, , London: Routledge.  \\nPlato  \\nMorrison, Donald R., 2012, , Cambridge: Cambridge University Press.  \\nThe Cambridge Companion to\\nSocrates  \\nNails, Debra, 1995, , Dordrecht: Kluwer Academic Publishers.  \\nAgora, Academy, and the Conduct of\\nPhilosophy  \\n–––, 2002, , Indianapolis:\\nHackett. (An encyclopedia of information about the characters in all\\nof the dialogues.)  \\nThe People of Plato: A\\nProsopography of Plato and Other Socratics  \\nNightingale, Andrea, 1993, , Cambridge: Cambridge University\\nPress.  \\nGenres in Dialogue: Plato and the\\nConstruction of Philosophy  \\nPeterson, Sandra, 2011, , Cambridge: Cambridge University Press.  \\nSocrates and Philosophy in the\\nDialogues of Plato  \\nPress, Gerald A. (ed.), 2000, , Lanham, MD: Rowman & Littlefield.  \\nWho Speaks for Plato? Studies in\\nPlatonic Anonymity  \\nPrior, William J., 2019, , Cambridge: Polity\\nPress.  \\nSocrates  \\nRowe, C.J., 2007, , Cambridge: Cambridge University Press.  \\nPlato and the Art of Philosophical\\nWriting  \\nRowe, Christopher, & Malcolm Schofield (eds.), 2000, , Cambridge: Cambridge University\\nPress. (Contains 7 introductory essays by 7 hands on Socratic and\\nPlatonic political thought.)  \\nGreek\\nand Roman Political Thought  \\nRudebusch, George, 2009, , Malden, MA:\\nWiley-Blackwell.  \\nSocrates  \\nRussell, Daniel C., 2005, , Oxford: Clarendon Press.  \\nPlato on Pleasure and the Good\\nLife  \\nRutherford, R.B., 1995, , Cambridge, MA: Harvard University\\nPress.  \\nThe Art of Plato: Ten Essays in\\nPlatonic Interpretation  \\nSantas, Gerasimos, 1979, , London: Routledge & Kegan Paul.  \\nSocrates: Philosophy in Plato’s\\nEarly Dialogues  \\nSayre, Kenneth, 1995, ,\\nNotre Dame: University of Notre Dame Press.  \\nPlato’s Literary Garden  \\nSchofield, Malcolm, 2006, ,\\nOxford: Oxford University Press.  \\nPlato: Political Philosophy  \\nSilverman, Allan, 2002, , Princeton: Princeton University\\nPress.  \\nThe Dialectic of Essence: A Study of\\nPlato’s Metaphysics  \\nSmith, Nicholas D. and Thomas C. Brickhouse, 1994, , Oxford: Oxford University Press  \\nPlato’s Socrates  \\n–––and John Bussanich (eds.), 2015, , London: Bloomsbury.  \\nThe\\nBloomsbury Companion to Socrates  \\nTaylor, C.C.W., 1998, , Oxford: Oxford University\\nPress.  \\nSocrates  \\nThesleff, Holger, 1982, ,\\nCommentationes Humanarum Litterarum 70, Helsinki: Societas Scientiarum\\nFennica.  \\nStudies in Platonic Chronology  \\nVander Waerdt, Paul. A. (ed.), 1994, , Ithaca: Cornell University Press.  \\nThe Socratic\\nMovement  \\nVasiliou, Iakovos, 2008, ,\\nCambridge: Cambridge University Press.  \\nAiming at Virtue in Plato  \\nVlastos, Gregory, 1991, , Cambridge: Cambridge University Press.  \\nSocrates: Ironist and Moral\\nPhilosopher  \\n–––, 1995, (Volume 2: Socrates, Plato, and Their Tradition), Daniel W. Graham\\n(ed.), Princeton: Princeton University Press.  \\nStudies in Greek Philosophy  \\nWhite, Nicholas P., 1976, ,\\nIndianapolis: Hackett.  \\nPlato on Knowledge and Reality  \\nYoung, Charles M., 1994, “Plato and Computer Dating,” , 12: 227–250.  \\nOxford Studies in Ancient Philosophy  \\nZuckert, Catherine H., 2009, , Chicago: University of Chicago\\nPress.  \\nPlato’s Philosophers: The\\nCoherence of the Dialogues'),\n",
       " Document(metadata={'Main Topic': 'Plato', 'SubTopic': 'Academic Tools'}, page_content='Academic Tools'),\n",
       " Document(metadata={}, page_content='.  \\nHow to cite this entry  \\nat the .  \\nPreview the PDF version of this entry  \\nFriends of the SEP Society  \\nat the Internet Philosophy Ontology Project (InPhO).  \\nLook up topics and thinkers related to this entry  \\nat , with links to its database.  \\nEnhanced bibliography for this entry  \\nPhilPapers  \\nOther Internet Resources  \\n(maintained by Bernard Suzanne)  \\nLinks to Original texts of Plato’s Dialogues  \\n,\\n a short podcast by Peter Adamson (Philosophy, Kings College\\nLondon).  \\nIn Dialogue: the Life and Works of Plato  \\nRelated Entries  \\n| | | | | | | | <a href=\"../socratic-dialogues/\"> Socratic Dialogues </a>  \\nabstract objects  \\nAristotle  \\neducation, philosophy of  \\nepistemology  \\nmetaphysics  \\nPlato: ethics and politics in  \\nThe Republic  \\nreligion: and morality in western philosophy  \\nSocrates  \\nby < >  \\nCopyright © 2022  \\nRichard Kraut  \\nrkraut1 northwestern edu  \\n@  \\n.  \\nOpen access to the SEP is made possible by a world-wide funding initiative. The Encyclopedia Now Needs Your Support Please Read How You Can Help Keep the Encyclopedia Free  \\nEnd footer menu End mirrors End site credits  \\nBrowse  \\nTable of Contents  \\nWhat\\'s New  \\nRandom Entry  \\nChronological  \\nArchives  \\nAbout  \\nEditorial Information  \\nAbout the SEP  \\nEditorial Board  \\nHow to Cite the SEP  \\nSpecial Characters  \\nAdvanced Tools  \\nAccessibility  \\nContact  \\nSupport SEP  \\nSupport the SEP  \\nPDFs for SEP Friends  \\nMake a Donation  \\nSEPIA for Libraries  \\nMirror Sites  \\nView this site from another server:  \\nUSA (Main Site)  \\nPhilosophy, Stanford University  \\nInfo about mirror sites  \\nThe Stanford Encyclopedia of Philosophy is by , Department of Philosophy, Stanford University  \\ncopyright © 2023  \\nThe Metaphysics Research Lab  \\nLibrary of Congress Catalog Data: ISSN 1095-5054  \\n$(\\'.dropdown-toggle\\').dropdown();')]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url=\"https://plato.stanford.edu/entries/plato/\"\n",
    "header_to_split_on=[\n",
    "    (\"h1\",\"Main Topic\"),\n",
    "    (\"h2\",'SubTopic')\n",
    "]\n",
    "\n",
    "text_sp=HTMLHeaderTextSplitter(headers_to_split_on=header_to_split_on)\n",
    "html_spliter=text_sp.split_text_from_url(url)\n",
    "html_spliter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600e040d",
   "metadata": {},
   "source": [
    "# JSON  TEXT Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6f936e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveJsonSplitter\n",
    "import json\n",
    "\n",
    "json_data=[\n",
    "  {\n",
    "    \"id\": 1,\n",
    "    \"name\": \"Vasudev\",\n",
    "    \"role\": \"Data Scientist\",\n",
    "    \"skills\": [\"Python\", \"Machine Learning\", \"SQL\"],\n",
    "    \"experience\": { \"years\": 0, \"level\": \"Fresher\" },\n",
    "    \"available\": True\n",
    "  },\n",
    "  {\n",
    "    \"id\": 2,\n",
    "    \"name\": \"Anita\",\n",
    "    \"role\": \"Data Analyst\",\n",
    "    \"skills\": [\"Excel\", \"Power BI\", \"SQL\"],\n",
    "    \"experience\": { \"years\": 2, \"level\": \"Junior\" },\n",
    "    \"available\": False\n",
    "  },\n",
    "  {\n",
    "    \"id\": 3,\n",
    "    \"name\": \"Rohit\",\n",
    "    \"role\": \"Software Engineer\",\n",
    "    \"skills\": [\"Java\", \"Spring Boot\", \"MySQL\"],\n",
    "    \"experience\": { \"years\": 3, \"level\": \"Mid\" },\n",
    "    \"available\": True\n",
    "  },\n",
    "  {\n",
    "    \"id\": 4,\n",
    "    \"name\": \"Priya\",\n",
    "    \"role\": \"AI Engineer\",\n",
    "    \"skills\": [\"Python\", \"Deep Learning\", \"TensorFlow\"],\n",
    "    \"experience\": { \"years\": 5, \"level\": \"Senior\" },\n",
    "    \"available\": True\n",
    "  },\n",
    "  {\n",
    "    \"id\": 5,\n",
    "    \"name\": \"Karan\",\n",
    "    \"role\": \"Frontend Developer\",\n",
    "    \"skills\": [\"HTML\", \"CSS\", \"React\"],\n",
    "    \"experience\": { \"years\": 1, \"level\": \"Junior\" },\n",
    "    \"available\": True\n",
    "  },\n",
    "  {\n",
    "    \"id\": 6,\n",
    "    \"name\": \"Meera\",\n",
    "    \"role\": \"Backend Developer\",\n",
    "    \"skills\": [\"Node.js\", \"Express\", \"MongoDB\"],\n",
    "    \"experience\": { \"years\": 4, \"level\": \"Mid\" },\n",
    "    \"available\": False\n",
    "  },\n",
    "  {\n",
    "    \"id\": 7,\n",
    "    \"name\": \"Amit\",\n",
    "    \"role\": \"DevOps Engineer\",\n",
    "    \"skills\": [\"AWS\", \"Docker\", \"Kubernetes\"],\n",
    "    \"experience\": { \"years\": 6, \"level\": \"Senior\" },\n",
    "    \"available\": True\n",
    "  },\n",
    "  {\n",
    "    \"id\": 8,\n",
    "    \"name\": \"Sneha\",\n",
    "    \"role\": \"UI/UX Designer\",\n",
    "    \"skills\": [\"Figma\", \"Sketch\", \"Adobe XD\"],\n",
    "    \"experience\": { \"years\": 2, \"level\": \"Junior\" },\n",
    "    \"available\": True\n",
    "  },\n",
    "  {\n",
    "    \"id\": 9,\n",
    "    \"name\": \"Arjun\",\n",
    "    \"role\": \"Mobile App Developer\",\n",
    "    \"skills\": [\"Flutter\", \"Dart\", \"Firebase\"],\n",
    "    \"experience\": { \"years\": 3, \"level\": \"Mid\" },\n",
    "    \"available\": False\n",
    "  },\n",
    "  {\n",
    "    \"id\": 10,\n",
    "    \"name\": \"Neha\",\n",
    "    \"role\": \"Project Manager\",\n",
    "    \"skills\": [\"Agile\", \"Scrum\", \"Leadership\"],\n",
    "    \"experience\": { \"years\": 7, \"level\": \"Senior\" },\n",
    "    \"available\": True\n",
    "  }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcfc551",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_text_splitters import RecursiveJsonSplitter\n",
    "\n",
    "splitter = RecursiveJsonSplitter(max_chunk_size=50)\n",
    "docs = splitter.create_documents(json_data)  # Pass the list directly\n",
    "\n",
    "for doc in docs:\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "228e8b7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='{\"id\": 1, \"name\": \"Vasudev\", \"role\": \"Data Scientist\"}'),\n",
       " Document(metadata={}, page_content='{\"skills\": [\"Python\", \"Machine Learning\", \"SQL\"], \"experience\": {\"years\": 0, \"level\": \"Fresher\"}}'),\n",
       " Document(metadata={}, page_content='{\"available\": true}'),\n",
       " Document(metadata={}, page_content='{\"id\": 2, \"name\": \"Anita\", \"role\": \"Data Analyst\", \"skills\": [\"Excel\", \"Power BI\", \"SQL\"]}'),\n",
       " Document(metadata={}, page_content='{\"experience\": {\"years\": 2, \"level\": \"Junior\"}, \"available\": false}'),\n",
       " Document(metadata={}, page_content='{\"id\": 3, \"name\": \"Rohit\", \"role\": \"Software Engineer\", \"skills\": [\"Java\", \"Spring Boot\", \"MySQL\"]}'),\n",
       " Document(metadata={}, page_content='{\"experience\": {\"years\": 3, \"level\": \"Mid\"}, \"available\": true}'),\n",
       " Document(metadata={}, page_content='{\"id\": 4, \"name\": \"Priya\", \"role\": \"AI Engineer\", \"skills\": [\"Python\", \"Deep Learning\", \"TensorFlow\"]}'),\n",
       " Document(metadata={}, page_content='{\"experience\": {\"years\": 5, \"level\": \"Senior\"}, \"available\": true}'),\n",
       " Document(metadata={}, page_content='{\"id\": 5, \"name\": \"Karan\", \"role\": \"Frontend Developer\", \"skills\": [\"HTML\", \"CSS\", \"React\"]}'),\n",
       " Document(metadata={}, page_content='{\"experience\": {\"years\": 1, \"level\": \"Junior\"}, \"available\": true}'),\n",
       " Document(metadata={}, page_content='{\"id\": 6, \"name\": \"Meera\", \"role\": \"Backend Developer\"}'),\n",
       " Document(metadata={}, page_content='{\"skills\": [\"Node.js\", \"Express\", \"MongoDB\"], \"experience\": {\"years\": 4, \"level\": \"Mid\"}}'),\n",
       " Document(metadata={}, page_content='{\"available\": false}'),\n",
       " Document(metadata={}, page_content='{\"id\": 7, \"name\": \"Amit\", \"role\": \"DevOps Engineer\", \"skills\": [\"AWS\", \"Docker\", \"Kubernetes\"]}'),\n",
       " Document(metadata={}, page_content='{\"experience\": {\"years\": 6, \"level\": \"Senior\"}, \"available\": true}'),\n",
       " Document(metadata={}, page_content='{\"id\": 8, \"name\": \"Sneha\", \"role\": \"UI/UX Designer\", \"skills\": [\"Figma\", \"Sketch\", \"Adobe XD\"]}'),\n",
       " Document(metadata={}, page_content='{\"experience\": {\"years\": 2, \"level\": \"Junior\"}, \"available\": true}'),\n",
       " Document(metadata={}, page_content='{\"id\": 9, \"name\": \"Arjun\", \"role\": \"Mobile App Developer\"}'),\n",
       " Document(metadata={}, page_content='{\"skills\": [\"Flutter\", \"Dart\", \"Firebase\"], \"experience\": {\"years\": 3, \"level\": \"Mid\"}}'),\n",
       " Document(metadata={}, page_content='{\"available\": false}'),\n",
       " Document(metadata={}, page_content='{\"id\": 10, \"name\": \"Neha\", \"role\": \"Project Manager\", \"skills\": [\"Agile\", \"Scrum\", \"Leadership\"]}'),\n",
       " Document(metadata={}, page_content='{\"experience\": {\"years\": 7, \"level\": \"Senior\"}, \"available\": true}')]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitter=RecursiveJsonSplitter(max_chunk_size=100)\n",
    "docs = splitter.create_documents(json_data)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e0e58d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
